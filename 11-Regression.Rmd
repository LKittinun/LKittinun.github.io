# Regression model

Regression model คือการสร้างสมการถดถอยของความสัมพันธ์ระหว่าง 2+ ตัวแปร โดยประกอบไปด้วย

-   ตัวแปรต้น (independent variable, $x$) คือ ตัวแปรที่เป็น**ต้นเหตุ**

-   ตัวแปรตาม (dependent variable, $y$) คือ ตัวแปรที่เป็น**ปลายเหตุ** ซึ่งเป็นผลมาจากการเปลี่ยนแปลงของตัวแปรต้น

ความสัมพันธ์ของตัวแปรต้นและตัวแปรตามจะเขียนในรูปแบบ $f(x) \sim x$

## Linear regression

### Model summary

คือ การสร้างความสัมพันธ์ของตัวแปรแบบเชิงเส้น ใช้กับข้อมูลแบบต่อเนื่อง (continuous data)

$$
h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \ …  \ + \theta_{n}x_{n} + \epsilon
$$

```{r lm, message = FALSE}
data("Diabetes", package = "heplots")

ggplot(Diabetes, aes(x = glufast, y = sspg)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw()
```

ในการวัดความแม่นยำของ linear regression นั้นประกอบด้วยสามองค์ประกอบ

```{r residual, echo = FALSE, message = FALSE}
library(broom)
library(cowplot)

diabetes_fit <- lm(sspg~glufast, data = Diabetes) 
lm(sspg~glufast,  data = Diabetes) |> summary()
diabetes_augment <- augment(diabetes_fit)

SSE <- ggplot(Diabetes, aes(x = glufast, y = sspg)) + 
  geom_point() +
  geom_segment(data = diabetes_augment, 
               aes(xend = glufast, yend = .fitted), linetype = "dashed") +
  geom_smooth(method = "lm", se = FALSE) + theme_bw()

diabetes_mean <- lm(sspg~1, data = Diabetes) 
diabetes_mean_augment <- augment(diabetes_mean) |> 
  mutate(glufast = Diabetes$glufast)

TSS <- ggplot(Diabetes, aes(x = glufast, y = sspg)) + 
  geom_point() +
  geom_segment(data = diabetes_mean_augment, 
               aes(xend = glufast, yend = .fitted), linetype = "dashed") +
  geom_smooth(method = "lm", formula =y~1, se = FALSE) + theme_bw()

diabetes_both_augment <- left_join(diabetes_augment, 
                                   diabetes_mean_augment, 
                                   by = c("glufast","sspg"))

SSR <- ggplot(Diabetes, aes(x = glufast, y = sspg)) + 
  geom_point() +
  geom_segment(data = diabetes_both_augment, 
               aes(xend = glufast, y = .fitted.x, yend = .fitted.y), 
               linetype = "dashed") +
  geom_smooth(method = "lm", formula =y~1, se = FALSE) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE) + theme_bw()

plot_grid(TSS, SSR, SSE, nrow=2, labels = c("TSS (H0)", "SSR", "SSE (Ha)"))
```

$$
\text{Total variation} = \text{Explained variation} + \text{Unexplained variation/Error} 
$$

$$
\text{Total sum of squares} \ (TSS) = \text{Sum of squares regression} \ (SSR) + \text{Sum of squares error} \ (SSE)
$$

$$
\sum^{n}_{i=1}(y_{i}-\bar{y}_{i})^{2} = \sum^{n}_{i=1}(\hat{y}_{i}-\bar{y}_{i})^{2} + \sum^{n}_{i=1}(y_{i}-\hat{y}_{i})^{2}
$$

-   $TSS$ = Total variation = ค่าความผันผวนระหว่างข้อมูลกับค่าเฉลี่ยของข้อมูล

-   $SSR$ = Explained variation = ค่าความผันผวนระหว่างค่าเฉลี่ยของข้อมูลกับเส้น regression

-   $SSE$ = Error = ค่าความผันผวนระหว่างข้อมูลกับเส้น regression

```{r lm_summary}
diabetes_fit <- lm(sspg ~ glufast, data = Diabetes) 
summary(diabetes_fit)
```

$R^{2}$ คือ อัตราส่วนระหว่าง `Explained variation` กับ `Total variation` = $\frac{SSR}{TSS} = 1-\frac{SSE}{TSS}$ ซึ่งจะบ่งบอกความสามารถของเส้นถดถอย ในการอธิบายข้อมูล (goodness of fit)

$F$-test คือการเปรียบเทียบความสามารถของเส้นถดถอย ว่าสามารถอธิบายข้อมูลได้ดีกว่า $H_{0}$ อย่างมีนัยสำคัญหรือไม่ ภายใต้สมมติฐาน

-   $H_{0}$: $f(x) = \theta_{0} + c$ หรือ สามารถอธิบายข้อมูลได้โดยใช้แค่ค่าเฉลี่ย

-   $H_{a}$: $f(x) = \theta_{0} + x_{1}\theta_{1} + … + \epsilon$

ซึ่งเป็นการเทียบอัตราส่วน explained กับ unexplained variation เช่นเดียวกับ [ANOVA](#ANOVA)

$$F =\frac{MSR}{MSE} = \frac{TSS-SSE}{SSE}/\frac{DF_{TSS}-DF_{SSE}}{DF_{SSE}}$$

$t$-test คือการเปรียบเทียบเส้น regression เมื่อมีตัวแปรนั้น ว่าอธิบายได้ดีกว่าเมื่อไม่มีตัวแปรนั้นหรือไม่

-   $H_{0}$: $\theta = 0$

-   $H_{a}$: $\theta \neq 0$

จะเห็นว่าสมการนั้นเหมือน [One-sample t-test](#one-t) แต่เขียนในรูปแบบ regression

$$
t = \frac{\theta-\theta_{0}}{SE(\theta)}
$$

$$
SE(\theta) = \sqrt{\frac{1}{n-2} \times {\sum_{i=1}^{n}\frac{(y_{i} - \hat{y_{i}})^2}{(x_{i} - \hat{x_{i}})^2}}}
$$

### Prediction

ข้อดีของสมการถอดถอย คือ สามารถใช้ในการทำนายข้อมูลตัวแปรตามชุดใหม่ได้ จากผลลัพธ์ขั้นต้นในคอลัมน์ `coef` พบว่าทุกๆ `glufast` ที่เพิ่มขึ้น 1.186 หน่วย ส่งผลให้ `sspg` เพิ่มขึ้น 1 หน่วย ซึ่งเขียนเป็นสมการได้ว่า

$$
f(x) = 39.4354 \ + 1.1866\times(\text{glufast}) + \epsilon
$$

โดยสมการนี้จะอยู่ใน `diabetes_fit`

```{r predicted_diabetes}
new_sspg <- data.frame(glufast = 1:400) 
new_sspg <- new_sspg |> 
  mutate(predicted_sspg = predict(diabetes_fit, newdata = new_sspg))

ggplot(new_sspg, aes(x = glufast, y = predicted_sspg)) +
  geom_point(size = 0.3) + theme_bw()
```

------------------------------------------------------------------------

## Logistic regression

คือ สมการถดถอยซึ่งมีคุณสมบัติในการจำแนกตัวแปรแบบสองตัวแปร (binary classification) ซึ่งเขียนอยู่ในรูปของ ค่าลอการิธึมของอัตราส่วนความเสี่ยง (log odds)

$$
log(\frac{h(x)}{1-h(x)}) = \theta_{0} + x_{1}\theta_{1} + x_{2}\theta_{2} + \ ... \ + x_{n}\theta_{n} + \epsilon = z
$$

$$
\frac{h(x)}{1-h(x)} = e^{z}
$$

$$
h(x) = \frac{1}{1+e^{z}}
$$

$$
h(x) = \frac{1}{1+e^{\theta_{0} + x_{1}\theta_{1} + x_{2}\theta_{2} + \ ... \ + x_{n}\theta_{n} + \epsilon}}
$$

ความพิเศษของสมการนี้คือ ขอบเขตของ $h(x)$ จะอยู่ระหว่าง (0, 1) เสมอ ซึ่งส่งผลให้สามารถคำนวณกลับไปทำนายอัตราการเกิดเหตุการณ์จากอัตราส่วนความเสี่ยงได้

```{r logistic_regression}
log_df <- data.frame(x = -20:20) |> mutate(y = 1/(1+exp(0 + 0.75*x)))
ggplot(log_df, aes(x = x, y = y)) + geom_line() + theme_bw() +
  labs(y = "h(x)")
```

ต่อไปเราจะทำนายว่า `glufast` เพื่อให้ตัวแปรเป็น binary เราจะรวม `Overt_Diabetic` และ `Chemical_Diabetic` เป็นกลุ่มเดียวกัน

```{r logistc_summary, message = FALSE}
Diabetes_mixed <- Diabetes |> mutate(group = 
                                       case_match(group,
                                                  "Normal" ~ 0,
                                                  "Chemical_Diabetic" ~ 1,
                                                  "Overt_Diabetic" ~ 1))

ggplot(Diabetes_mixed, aes(x = glufast, y = group)) + 
  geom_point() +
  geom_smooth(method = "glm",  method.args = list(family = "binomial"), 
              se = FALSE) +
  theme_bw()
```

```{r predicted_logit}
logit_fit <- glm(group ~ glufast, family = "binomial", 
                 data = Diabetes_mixed) 
logit_fit

new_group <- data.frame(glufast = 1:200) 
new_group <- new_group |> 
  mutate(predicted_group = predict(logit_fit, 
                                   newdata = new_group, type = "response"))

ggplot(new_group, aes(x = glufast, y = predicted_group)) +
  geom_point() + theme_bw()
```

## Poisson, quassipoisson, and negative binomial regression

Poission regression คือ สมการถดถอยที่ใช้ในการทำนายความถี่ หรือค่าเฉลี่ยของการเกิดเหตุการณ์นั้นๆ มักใช้กับข้อมูลที่ไม่ต่อเนื่อง (discrete value) พิจารณาข้อมูลแบบ [Poisson distribution](#poisson-dist)

$$
f(k) = P(X = k) = \frac{\lambda^{k}}{k!}e^{-\lambda}
$$

จะพบว่ามีตัวแปรที่สามารถทำนายได้เมื่อมีข้อมูลอีกชนิดหนึ่ง คือ ค่าเฉลี่ยการเกิดเหตุการณ์ $(\lambda)$ และ จำนวนเหตุการณ์ที่เกิด $(k)$ จึงออกมาเป็นสมการถดถอยได้สองรูปแบบ

-   สำหรับ $\lambda$

$$
\lambda = e^{\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}}
$$

$$
ln(\lambda) = \theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}
$$

-   สำหรับ $k$ เราจำเป็นต้องเปลี่ยน $\lambda$ ให้อยู่ในรูป $k/t$ และย้ายไปเป็นตัวแปรควบคุมเวลา (offset) ซึ่งจะมี regression coefficient = 1 เสมอ

$$ \lambda = \frac{k}{t} = e^{\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}} $$

$$ ln(k) - \ln(t) = \theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n} $$

$$
ln(k) =  \theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n} + \ln(t) \rightarrow  \text{offset}
$$

ซึ่งสมการนี้เป็นสมการพื้นฐานของการนับจำนวนใดๆ ดังเช่นตัวอย่างนี้ คือความสัมพันธ์ระหว่างจำนวนครั้งที่เจ็บป่วยกับการเข้าพบแพทย์ในสองสัปดาห์ ในประเทศออสเตรเลีย

```{r poisson, warning = FALSE}
data("DoctorVisits", package = "AER")

glm(visits ~  illness, family = "poisson", data = DoctorVisits) |> summary()

ggplot(DoctorVisits, aes (x = illness, y = visits)) +
  geom_jitter() +
  geom_smooth(method = "glm", method.args = list(family = "poisson"), se = FALSE) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(breaks = 0:9) +
  theme_bw()

```

อย่างไรก็ตาม Poisson regression มีสมมติฐานที่สำคัญตาม [Poisson distribution](#0) คือ

-   ต้องเป็นจำนวนนับ
-   Variance = Mean

ซึ่งข้อมูลบางส่วนจะไม่เป็นไปตามสมมติฐานนี้ โดยจะนิยมใช้ Quassipoisson regression


$$
V = \phi\times\mu
$$
$$
\phi = \frac{\chi^{2}}{\text{DF}} = \frac{1}{n-k}{\sum_{i=1}^{n}\frac{(Y_{i}-\hat{\mu_{i}})^{2}}{\hat{\mu_{i}}}}
$$

หรือ Negative binomial regression มากกว่า
