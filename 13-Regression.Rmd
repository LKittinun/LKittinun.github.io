# Regression model

คือ การสร้างสมการถดถอยของความสัมพันธ์ระหว่าง 2 ตัวแปรขึ้นไป โดยประกอบไปด้วย

-   **ตัวแปรต้น** (Independent variable, $x$) คือ ตัวแปรที่เป็น**ต้นเหตุ**

-   **ตัวแปรตาม** (Dependent variable, $y$) คือ ตัวแปรที่เป็น**ปลายเหตุ** ซึ่งเป็นผลมาจากการเปลี่ยนแปลงของตัวแปรต้น

ความสัมพันธ์ของตัวแปรต้นและตัวแปรตามจะเขียนในรูปแบบ $f(x) \sim x$

## Linear regression

### Model summary

คือ การสร้างความสัมพันธ์ของตัวแปรแบบเชิงเส้น ใช้กับข้อมูลแบบต่อเนื่อง (Continuous data)

$$
h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \ …  \ + \theta_{n}x_{n} + \epsilon
$$

```{r lm, message = FALSE}
data("Diabetes", package = "heplots")

ggplot(Diabetes, aes(x = glufast, y = sspg)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw()
```

ในการวัดความแม่นยำของ Linear regression นั้นประกอบด้วยสามองค์ประกอบ

```{r residual, echo = FALSE, message = FALSE}
library(broom)
library(cowplot)

diabetes_fit <- lm(sspg~glufast, data = Diabetes) 
lm(sspg~glufast,  data = Diabetes) |> summary()
diabetes_augment <- augment(diabetes_fit)

SSE <- ggplot(Diabetes, aes(x = glufast, y = sspg)) + 
  geom_point() +
  geom_segment(data = diabetes_augment, 
               aes(xend = glufast, yend = .fitted), linetype = "dashed") +
  geom_smooth(method = "lm", se = FALSE) + theme_bw()

diabetes_mean <- lm(sspg~1, data = Diabetes) 
diabetes_mean_augment <- augment(diabetes_mean) |> 
  mutate(glufast = Diabetes$glufast)

TSS <- ggplot(Diabetes, aes(x = glufast, y = sspg)) + 
  geom_point() +
  geom_segment(data = diabetes_mean_augment, 
               aes(xend = glufast, yend = .fitted), linetype = "dashed") +
  geom_smooth(method = "lm", formula =y~1, se = FALSE) + theme_bw()

diabetes_both_augment <- left_join(diabetes_augment, 
                                   diabetes_mean_augment, 
                                   by = c("glufast","sspg"))

SSR <- ggplot(Diabetes, aes(x = glufast, y = sspg)) + 
  geom_point() +
  geom_segment(data = diabetes_both_augment, 
               aes(xend = glufast, y = .fitted.x, yend = .fitted.y), 
               linetype = "dashed") +
  geom_smooth(method = "lm", formula =y~1, se = FALSE) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE) + theme_bw()

plot_grid(TSS, SSR, SSE, nrow=2, labels = c("TSS (H0)", "SSR", "SSE (Ha)"))
```

$$
\text{Total variation} = \text{Explained variation} + \text{Unexplained variation/Error} 
$$

$$
\text{Total sum of squares} \ (TSS) = \text{Sum of squares regression} \ (SSR) + \text{Sum of squares error} \ (SSE)
$$

$$
\sum^{n}_{i=1}(y_{i}-\bar{y}_{i})^{2} = \sum^{n}_{i=1}(\hat{y}_{i}-\bar{y}_{i})^{2} + \sum^{n}_{i=1}(y_{i}-\hat{y}_{i})^{2}
$$

-   $TSS$ = Total variation = ค่าความผันผวนระหว่างข้อมูลกับค่าเฉลี่ยของข้อมูล

-   $SSR$ = Explained variation = ค่าความผันผวนระหว่างค่าเฉลี่ยของข้อมูลกับเส้น regression

-   $SSE$ = Error = ค่าความผันผวนระหว่างข้อมูลกับเส้น regression

```{r lm_summary}
diabetes_fit <- lm(sspg ~ glufast, data = Diabetes) 
summary(diabetes_fit)
```

$R^{2}$ คือ อัตราส่วนระหว่าง `Explained variation` กับ `Total variation` = $\frac{SSR}{TSS} = 1-\frac{SSE}{TSS}$ ซึ่งจะบ่งบอกความสามารถของเส้นถดถอย ในการอธิบายข้อมูล (goodness of fit)

$F$-test คือการเปรียบเทียบความสามารถของเส้นถดถอย ว่าสามารถอธิบายข้อมูลได้ดีกว่า $H_{0}$ อย่างมีนัยสำคัญหรือไม่ ภายใต้สมมติฐาน

-   $H_{0}$: $f(x) = \theta_{0} + c$ หรือ สามารถอธิบายข้อมูลได้โดยใช้แค่ค่าเฉลี่ย

-   $H_{a}$: $f(x) = \theta_{0} + x_{1}\theta_{1} + … + \epsilon$

ซึ่งเป็นการเทียบอัตราส่วน explained กับ unexplained variation เช่นเดียวกับ [ANOVA](#ANOVA)

$$F =\frac{MSR}{MSE} = \frac{TSS-SSE}{SSE}/\frac{DF_{TSS}-DF_{SSE}}{DF_{SSE}}$$

$t$-test คือการเปรียบเทียบเส้น regression เมื่อมีตัวแปรนั้น ว่าอธิบายได้ดีกว่าเมื่อไม่มีตัวแปรนั้นหรือไม่

-   $H_{0}$: $\theta = 0$

-   $H_{a}$: $\theta \neq 0$

จะเห็นว่าสมการนั้นเหมือน [One-sample t-test](#one-t) แต่เขียนในรูปแบบ regression

$$
t = \frac{\theta_{H_{a}}-\theta_{H_0}}{SE(\theta)} = \frac{\theta-0}{SE(\theta)}
$$

$$
SE(\theta) = \sqrt{\frac{1}{n-2} \times {\sum_{i=1}^{n}\frac{(y_{i} - \hat{y_{i}})^2}{(x_{i} - \bar{x})^2}}}
$$

### Prediction

ข้อดีของสมการถอดถอย คือ สามารถใช้ในการทำนายข้อมูลตัวแปรตามชุดใหม่ได้ จากผลลัพธ์ขั้นต้นในคอลัมน์ `coef` พบว่าทุกๆ `glufast` ที่เพิ่มขึ้น 1.186 หน่วย ส่งผลให้ `sspg` เพิ่มขึ้น 1 หน่วย ซึ่งเขียนเป็นสมการได้ว่า

$$
f(x) = 39.4354 \ + 1.1866\times(\text{glufast}) + \epsilon
$$

โดยสมการนี้จะอยู่ใน `diabetes_fit`

```{r predicted_diabetes}
new_sspg <- data.frame(glufast = 1:400) 
new_sspg <- new_sspg |> 
  mutate(predicted_sspg = predict(diabetes_fit, newdata = new_sspg))

ggplot(new_sspg, aes(x = glufast, y = predicted_sspg)) +
  geom_point(size = 0.3) + theme_bw()
```

------------------------------------------------------------------------

## Logistic regression

คือ สมการถดถอยซึ่งมีคุณสมบัติในการจำแนกตัวแปรแบบสองตัวแปร (binary classification) ซึ่งเขียนอยู่ในรูปของ ค่าลอการิธึมของอัตราส่วนความเสี่ยง (log odds)

$$
log(\frac{h(x)}{1-h(x)}) = \theta_{0} + x_{1}\theta_{1} + x_{2}\theta_{2} + \ ... \ + x_{n}\theta_{n} + \epsilon = z
$$

$$
\frac{h(x)}{1-h(x)} = e^{z}
$$

$$
h(x) = \frac{1}{1+e^{z}}
$$

$$
h(x) = \frac{1}{1+e^{\theta_{0} + x_{1}\theta_{1} + x_{2}\theta_{2} + \ ... \ + x_{n}\theta_{n} + \epsilon}}
$$

ความพิเศษของสมการนี้คือ ขอบเขตของ $h(x)$ จะอยู่ระหว่าง (0, 1) เสมอ ซึ่งส่งผลให้สามารถคำนวณกลับไปทำนายอัตราการเกิดเหตุการณ์จากอัตราส่วนความเสี่ยงได้

```{r logistic_regression}
log_df <- data.frame(x = -20:20) |> mutate(y = 1/(1+exp(0 + 0.75*x)))
ggplot(log_df, aes(x = x, y = y)) + geom_line() + theme_bw() +
  labs(y = "h(x)")
```

ต่อไปเราจะทำนายว่า `glufast` เพื่อให้ตัวแปรเป็น binary เราจะรวม `Overt_Diabetic` และ `Chemical_Diabetic` เป็นกลุ่มเดียวกัน

```{r logistc_summary, message = FALSE}
Diabetes_mixed <- Diabetes |> mutate(group = 
                                       case_match(group,
                                                  "Normal" ~ 0,
                                                  "Chemical_Diabetic" ~ 1,
                                                  "Overt_Diabetic" ~ 1))

ggplot(Diabetes_mixed, aes(x = glufast, y = group)) + 
  geom_point() +
  geom_smooth(method = "glm",  method.args = list(family = "binomial"), 
              se = FALSE) +
  theme_bw()
```

```{r predicted_logit}
logit_fit <- glm(group ~ glufast, family = "binomial", 
                 data = Diabetes_mixed) 
logit_fit

new_group <- data.frame(glufast = 1:200) 
new_group <- new_group |> 
  mutate(predicted_group = predict(logit_fit, 
                                   newdata = new_group, type = "response"))

ggplot(new_group, aes(x = glufast, y = predicted_group)) +
  geom_point() + theme_bw()
```

------------------------------------------------------------------------

## Poisson, quassipoisson, and negative binomial regression {#poisson-family}

Poisson regression คือ สมการถดถอยที่ใช้ในการทำนายความถี่ หรือค่าเฉลี่ยของการเกิดเหตุการณ์นั้นๆ มักใช้กับข้อมูลที่ไม่ต่อเนื่อง (Discrete value) พิจารณาข้อมูลแบบ [Poisson distribution](#poisson-dist)

$$
f(k) = P(X = k) = \frac{\lambda^{k}}{k!}e^{-\lambda}
$$

จะพบว่ามีตัวแปรที่สามารถทำนายได้เมื่อมีข้อมูลอีกชนิดหนึ่ง คือ ค่าเฉลี่ยการเกิดเหตุการณ์ $(\lambda)$ และ จำนวนเหตุการณ์ที่เกิด $(k)$ จึงออกมาเป็นสมการถดถอยได้สองรูปแบบ

-   สำหรับ $\lambda$

$$
\lambda = e^{\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}}
$$

$$
ln(\lambda) = \theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}
$$

-   สำหรับ $k$ เราจำเป็นต้องเปลี่ยน $\lambda$ ให้อยู่ในรูป $k/t$ และย้ายไปเป็นตัวแปรควบคุมเวลา (Offset) ซึ่งจะมี Regression coefficient = 1 เสมอ

$$ \lambda = \frac{k}{t} = e^{\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}} $$

$$ ln(k) - \ln(t) = \theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n} $$

$$
ln(k) =  \theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n} + \ln(t) \rightarrow  \text{offset}
$$

ซึ่งสมการนี้เป็นสมการพื้นฐานของการนับจำนวนใดๆ ดังเช่นตัวอย่างนี้ คือความสัมพันธ์ของตัวแปรต่างๆ กับอัตราการตายในโรคมะเร็งปากมดลูก

```{r poisson, warning = FALSE}
cervix_mort <- read.csv("Resource/cervix_mort.csv") |>  # aggregrated data for demonstration
                mutate(histo_major = fct_relevel(histo_major, "Squamous cell carcinoma",
                                                 "Adenocarcinoma",
                                                 "Adenosquamous carcinoma",
                                                 "Others"))
glm(dead ~  age_group + histo_major + stage, family = "poisson", data = cervix_mort) |> summary()

ggplot(cervix_mort, aes (x = age_group, y = dead)) +
  geom_jitter() +
  geom_smooth(method = "glm", method.args = list(family = "poisson"), se = FALSE) +
  scale_x_continuous(breaks = 1:4) +
  theme_bw()
```

สังเกตว่ายิ่งอายุเยอะ อัตราการตายยิ่งน้อยลงอย่างมีนัยสำคัญ ซึ่งไม่น่าจะเป็นไปได้ ทั้งนี้ เพราะยังไม่ได้ปรับระยะเวลาติดตาม โดยการใส่ Offset เป็น `person-month` เข้าไปคำนวณด้วย

```{r pois_fit}
pois_fit <- glm(dead ~  age_group + histo_major + stage + offset(log(person_month)),
                family = "poisson", data = cervix_mort)
summary(pois_fit)
```

จะพบว่าหลังจากปรับตามระยะเวลาติดตาม กลุ่มอายุที่เพิ่มขึ้นจะส่งผลให้การตายเพิ่มขึ้นอย่างมีนัยสำคัญ

```{r pois_plot}
new_data <- expand_grid(age_group = 1:4,
                       histo_major = unique(cervix_mort$histo_major),
                       stage = 1:4,
                       person_month = 1200)
new_data <- new_data |> 
  mutate(predicted_dead = predict(pois_fit, newdata = new_data, type = "response"))

ggplot(new_data, aes (x = age_group, y = predicted_dead, col = histo_major)) +
  geom_line() +
  facet_wrap(~stage)+
  scale_y_continuous(breaks = seq(0,60,10)) +
  theme_bw()
```

อย่างไรก็ตาม พิจารณาดูแล้วอัตราการตายไม่ได้เพิ่มขึ้นมากตามกลุ่มอายุ ซึ่งอาจจะเป็นผลบวกลวง เมื่อกลับมาพิจารณาแล้ว Poisson regression มีสมมติฐานที่สำคัญตาม [Poisson distribution](#poisson-dist) คือ

-   ต้องเป็นจำนวนนับ
-   การกระจายตัวของข้อมูล = Dispersion parameter ($\phi$) = Variance/Mean = 1

เมื่อตรวจสอบจากผลลัพธ์ของสมการถอดถอย โดยพิจารณาจาก $\text{Residuals/Deviance}$ แล้วพบว่า $\phi > 1$ ซึ่งหมายความว่าข้อมูลตั้งต้นนั้นมีการกระจายตัวของข้อมูลสูงเกินไป (Overdispersed) จึงไม่เป็นไปตามสมมติฐานของ Poisson ณ จุดนี้จึงจำเป็นต้องใช้ ส่วนขยายสมการของ Poisson regression เรียกว่า Quasipoisson regression ซึ่งจะทำการปรับความแปรปรวน ตามการเพิ่มขึ้นของความแปรปรวนต่อค่าเฉลี่ย

$$
V = \phi\mu_{i}
$$ $$
\phi = \frac{\chi^{2}}{\text{DF}} = \frac{1}{n-k}{\sum_{i=1}^{n}\frac{(Y_{i}-\hat{\mu_{i}})^{2}}{\hat{\mu_{i}}}}
$$

```{r quasi_fit}
quasipois_fit <- glm(dead ~  age_group + histo_major + stage + offset(log(person_month)),
                family = "quasipoisson", data = cervix_mort)
summary(quasipois_fit)
```

จะเห็นว่าหลังจากปรับ Dispersion parameter แล้ว `age_group` ไม่ได้ส่งผลให้การตายเพิ่มขึ้นอย่างมีนัยสำคัญแต่อย่างใด ปัญหาของ Quasipoisson คือ อาจจะไม่มี parameter ที่ต้องการ เช่น `AIC` เป็นต้น ซึ่งสามารถใช้อีกหนึ่งสมการถดถอยที่นิยมใช้คือ Negative binomial regression จาก package `MASS` ซึ่งปรับตามอัตราส่วนความแปรปรวนเช่นกัน

$$
V = u_{i}(1+\frac{u_{i}}{\phi})
$$

```{r nb_fit}
nb_fit <- MASS::glm.nb(dead ~  age_group + histo_major + stage + offset(log(person_month)), 
                 data = cervix_mort)
summary(nb_fit)
```

ผลลัพธ์ที่ได้จะใกล้เคียงกันกับสมการถดถอย Quasipoisson ทั้งสองสมการนี้จะนิยมใช้ในการตั้งสมการถดถอยในจำนวนนับจากงานทดลอง High-throughput เช่น RNA-sequencing เนื่องจากข้อมูลประเภทนี้มักมีเป็นข้อมูลจำนวนนับที่มีการกระจายตัวสูงมาก ($\phi > 1$)
