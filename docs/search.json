[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"เอกสารนี้จัดทำขึ้นเพื่อจุดประสงค์ขึ้นเพื่อแนะนำการใช้ R (R Core Team, 2023) เบื้องต้น สำหรับนักวิทยาศาสตร์ที่มีความสนใจในการใช้ R ในการวิเคราะห์ข้อมูล ซึ่งผู้ใช้งานจำเป็นจะต้องมีความรู้เรื่อง Basic R ต่างๆ เล็กน้อย เพื่อที่จะได้ใช้งานได้อย่างไม่ติดขัดท่านสามารถ Clone github repository ได้ที่นี่","code":""},{"path":"index.html","id":"r-installation","chapter":"Preface","heading":"R installation","text":"","code":""},{"path":"index.html","id":"r-console","chapter":"Preface","heading":"R console","text":"ผู้ที่ต้องการใช้ R สามารถดาวน์โหลดโปรแกรม ได้ที่นี่ โดยตัว R console จะมีหน้าตาดังภาพ","code":""},{"path":"index.html","id":"rstudio","chapter":"Preface","heading":"Rstudio","text":"อย่างไรก็ตาม การใช้งาน R ด้วยโปรแกรมนี้จะใช้งานค่อนข้างยาก โดยส่วนใหญผู้ใช้การจะต้องดาวน์โหลด IDE (Integrated development environment) มาอำนวยความสะดวกในการเขียนคำสั่ง ซึ่ง IDE ที่ได้รับความนิยมมากที่สุด คือ Rstudio สามารถดาวน์โหลดได้ที่นี่นี่คือหน้าต่างเริ่มต้นของ Rstudio โดยส่วนประกอบหลักคือText editor มุมซ้ายบน คือ ที่ๆ เราจะเขียน Script ไว้เพื่อ RunEnvironment มุมขวาบน คือ ส่วนที่เก็บข้อมูล Variable ต่างๆ ที่เรา AssignR console มุมซ้ายล่าง คือ ส่วนที่ R ทำงานจริงๆ ซึ่งก็คือ ตัว R console ที่เราโหลดมาตอนแรกนั่นเองส่วน Output ที่จะมีไว้แสดงที่อยู่ของไฟล์ รูปภาพที่ Render ออกมา และ อื่นๆ ตามที่เราจะปรับแต่งเราสามารถเขียนไว้ Script ไว้ที่ Text editor และกด Run คำสั่งแต่ละบรรทัดได้โดยการกด Ctrl + Enterยินดีด้วย! เท่านี้ท่านก็สามารถเริ่มใช้งาน R ได้แล้ว","code":""},{"path":"basic-r.html","id":"basic-r","chapter":"1 Basic R","heading":"1 Basic R","text":"","code":""},{"path":"basic-r.html","id":"basic-operation","chapter":"1 Basic R","heading":"1.1 Basic operation","text":"ท่านสามารถใช้ R ในการคำนวณต่างๆ ได้ เช่น บวก ลบ คูณ หาร ยกกำลัง เป็นต้น","code":"\n3+2## [1] 5\n3-2## [1] 1\n3*2## [1] 6\n3/2## [1] 1.5\n3^2## [1] 9\nlog(3)## [1] 1.098612\nsqrt(3)## [1] 1.732051\n3==3 # ตรวจสอบว่าข้อมูลเหมือนกันหรือไม่## [1] TRUE"},{"path":"basic-r.html","id":"variable","chapter":"1 Basic R","heading":"1.2 Variable","text":"","code":""},{"path":"basic-r.html","id":"variable-assignment","chapter":"1 Basic R","heading":"1.2.1 Variable assignment","text":"R สามารถเก็บข้อมูลต่างๆ ไว้ในตัวแปรได้ เพื่อที่สามารถนำมาใช้ในภายหลัง โดยการเก็บตัวแปรนั้นจะใช้เครื่องหมาย <-","code":"\nx <- 2\nx## [1] 2\ny <- 3\ny## [1] 3\nx+y # ท่านสามารถนำตัวแปรมาทำ operation ได้ตามปกติ## [1] 5\nx*y## [1] 6\nx <- 5 # การลงข้อมูลในตัวแปรเดิมจะเป็นการลบตัวแปรเก่า\nx## [1] 5\nhelloworld <- (x+y)^(x-y) # สามารถตั้งชื่ออะไรก็ได้ตราบใดที่ไม่เว้นวรรค\nhelloworld## [1] 64"},{"path":"basic-r.html","id":"type-of-variable","chapter":"1 Basic R","heading":"1.2.2 Type of variable","text":"R นั้นสามารถรองรับตัวแปรต่างๆ ได้หลากหลาย ซึ่งเป็นได้ทั้ง ตัวเลข หรือตัวอักษร หรือแม้กระทั่งเก็บหลายข้อมูลภายในตัวแปรเดียวได้ลักษณะตัวแปรต่างๆ ใน R มีดังนี้ในบางครั้ง Class ที่ R ทำการเดามาให้ตั้งแต่แรกอาจจะไม่ใช่ลักษณะตัวแปรที่ท่านต้องการ ท่านสามารถใช้ คำสั่ง .*() ในการเปลี่ยน Class ของตัวแปรนั้นได้","code":"\nx <- \"Hello world\" # ตัวอักษร\nx## [1] \"Hello world\"\ny <- c(1,2,3,4) # เก็บหลายตัวข้อมูลในตัวแปรเดียว\ny## [1] 1 2 3 4\nz <- list(c(1,2,3), 4, c(\"hello world\", \"I love R\"))  # เก็บข้อมูลในรูปแบบ list\nz## [[1]]\n## [1] 1 2 3\n## \n## [[2]]\n## [1] 4\n## \n## [[3]]\n## [1] \"hello world\" \"I love R\"\nclass(x) # ท่านสามารถเช็คชนิดของตัวแปรได้โดยใช้ function class()## [1] \"character\"\nx <- c(1,2,3,4,5)\nclass(x) # ไม่ต้องการให้คิดเป็นตัวเลข เช่น ตัวแปรที่จริงแล้วอาจจะเป็น กลุ่ม1 กลุ่ม2## [1] \"numeric\"\nx <- as.factor(x)\nclass(x) # เปลี่ยนเป็น factor## [1] \"factor\"\nx_list <- list(\"A\"= c(1,2,3,4,5), \"B\" = c(\"a\",\"b\",\"c\",\"d\",\"e\"))\nx_list## $A\n## [1] 1 2 3 4 5\n## \n## $B\n## [1] \"a\" \"b\" \"c\" \"d\" \"e\"\nas.data.frame(x_list) # เปลี่ยนเป็น dataframe"},{"path":"basic-r.html","id":"matrix-and-dataframe","chapter":"1 Basic R","heading":"1.3 Matrix and Dataframe","text":"เนื่องจาก R นั้นเป็นโปรแกรมที่ส่วนมากใช้ในการวิเคราะห์ทางสถิติ ซึ่งเกี่ยวข้อมูลส่วนใหญ่จะถูกเก็บในรูปของตาราง R จึงมีตัวแปรที่เก็บข้อมูลในรูปของตารางโดยเฉพาะ เรียกวา Matrix และ Dataframe ซึ่งท่านจะใช้เป็นหลักในการวิเคราะห์ข้อมูลใน Rโดยตารางนั้นจะประกอบด้วยสองส่วนหลักๆ คล้าย Excel spreadsheet ได้แก่Column (คอลัมน์): คือ ข้อมูลในแนวตั้ง ซึ่งแถวบนสุดจะเป็นชื่อ Column นั้นๆRow (แถว): คือ ข้อมูลในแนวนอนโดย Matrix นั้น สามารถเก็บ Variable ในรูปแบบเดียวกันได้เท่านั้น แต่ Dataframe สามารถเก็บข้อมูลต่างชนิดร่วมกันได้ โดยมีข้อแม้ว่า Column เดียวกัน จะต้องเป็นข้อมูลชุดเดียวกัน","code":"\nmat <- matrix(c(1,2,3,4), nrow=2)\nmat##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\nclass(mat)## [1] \"matrix\" \"array\"\ndf <- data.frame(x=c(3,4),y=c(2,5),z=c(4,7))\ndf\nclass(df)## [1] \"data.frame\""},{"path":"basic-r.html","id":"subset","chapter":"1 Basic R","heading":"1.4 Subset","text":"ท่านสามารถดึงข้อมูลแค่บางส่วนออกมาจาก Vector, List, Matrix หรือ Dataframe ได้ เรียกว่าการ Subsetในส่วนของ Matrix และ Dataframe นั้น ท่านสามารถ subset ตามตำแหน่งได้ โดยการระบุ Row และ Column ตามลำดับในส่วนของ Dataframe นั้น ท่านสามารถ Subset ได้โดยใช้ชื่อของ Column อีกด้วย","code":"\nx <- c(\"a\",\"b\",\"c\",\"d\")\nx[3] # subset โดยระบุตำแหน่ง## [1] \"c\"\nx[1:3] # subset หลายตำแหน่ง## [1] \"a\" \"b\" \"c\"\nx[c(1,3)] # subset หลากหลายตำแหน่งแบบจำเพาะ## [1] \"a\" \"c\"\ny <- list(c(1,2,3), c(\"a\",\"b\",\"c\"))\ny[1] # subset list ตามตำแหน่ง (จะได้ list ย่อยออกมา)## [[1]]\n## [1] 1 2 3\ny[[1]] # ดึงข้อมูลที่อยู่ใน list ออกมา## [1] 1 2 3\nmat##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\nmat[1,2] # 1st row, 2nd column## [1] 3\ndf\ndf[1,3] # 1st row, 3rd column## [1] 4\ndf[\"x\"] # subset เป็น column ย่อย\ndf[[\"x\"]] # subset ข้อมูลที่อยู่ใน column นั้น## [1] 3 4\ndf[[2, \"x\"]] # ระบุแถวด้วย## [1] 4\ndf$x # เหมือนกัน df[[\"x\"]]## [1] 3 4"},{"path":"r-function.html","id":"r-function","chapter":"2 R function","heading":"2 R function","text":"Function (ฟังก์ชัน) คือ ชุดของคำสั่งที่จะสั่งการให้ R ทำงานตามจุดประสงค์ที่ท่านตั้งไว้ โดยตัวฟังก์ชันนั้น จะประกอบไปด้วยฟังก์ชันที่มีมาพร้อมกับ R ตั้งแต่ต้น (Base R function)ฟังก์ชันที่ผู้นิพนธ์ท่านอื่นเขียนไว้ และรวบรวมมาเป็น ชุดของ ฟังก์ชัน เรียกว่า Packageฟังก์ชันที่ท่านเขียนขึ้นมาเอง","code":""},{"path":"r-function.html","id":"anatomy-of-function","chapter":"2 R function","heading":"2.1 Anatomy of function","text":"ฟังก์ชันนั้นประกอบด้วย 4 ส่วน คือFunction name (ชื่อฟังก์ชัน)Argument (รายละเอียดของฟังก์ชัน)Function body (รายละเอียดของฟังก์ชัน)Return (ผลลัพธ์ของฟังก์ชัน)ยกตัวอย่างเช่นฟังก์ชัน หาค่าเฉลี่ยของข้อมูลจะเห็นว่า ฟังก์ชันนี้รับข้อมูล 2 ตัวแปร คือ x และ y ซึ่งท่านจะต้องแทนค่าที่ท่านต้องการลงไปใน ฟังก์ชัน หลังจากนั้น ฟังก์ชันจะทำการประมวลผลและส่งผลลัพธ์กลับมาในผู้เริ่มต้น ส่วนใหญ่ท่านมักจะไม่ใช้ ฟังก์ชันที่เขียนขึ้นมาเองมากนัก เนื่องจาก Basic operation ส่วนใหญ่จะมีผู้นิพนธ์ขึ้นมาให้แล้ว","code":"\nfind_mean <- function(x, y){\n  (x + y)/2\n}\n\nfind_mean(2, 3)## [1] 2.5\nfind_mean(3, 5)## [1] 4"},{"path":"r-function.html","id":"base-r-function","chapter":"2 R function","heading":"2.2 Base R function","text":"Base R function คือ ฟังก์ชันที่ติดกับ R มาตั้งแต่แรก ซึ่งท่านสามารถเรียกใช้ได้เลยโดยไม่ต้องทำการเรียก Package ขึ้นมาก่อนในส่วนของการ Manipulate dataframe นั้น คำสั่งต่างๆ ที่น่ารู้มีดังนี้สามารถดูฟังก์ชันของ Base R ทั้งหมดได้ที่นี่ถ้าท่านต้องการดูว่าแต่ละฟังก์ชันนั้นใช้งานอย่างไร ให้ใส่เครื่องหมาย ? หน้า ฟังก์ชัน นั้น เช่น ?mean() ?colSums()","code":"\nmax(c(1,2,4,5,5,68)) # find max value## [1] 68\nmin(c(1,4,5,6,-20)) # find min value## [1] -20\nmean(c(1,2,3,4)) # find mean## [1] 2.5\nmedian(c(1,2,5,3,4)) # find median## [1] 3\nunique(c(1,1,1,1,2,2,4,5,5,6,7,8)) # display only unique values## [1] 1 2 4 5 6 7 8\ndf <- data.frame(x = c(3,3,6,7,8,9),y = c(2,5,8,1,2,3),z = c(4,7,9,4,7,8))\ndf\nhead(df, 5) # ดู 5 แถวแรก\ntail(df , 5) # ดู 5 แถวล่าง\nrowMeans(df) # หาค่า mean แต่ละแถว## [1] 3.000000 5.000000 7.666667 4.000000 5.666667 6.666667\ncolMeans(df) # หาค่า mean แต่ละ columns##   x   y   z \n## 6.0 3.5 6.5\nrownames(df) # ชื่อแถว## [1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\ncolnames(df) # ชื่อ column## [1] \"x\" \"y\" \"z\"\ntable(iris$Species) # สร้างตารางนับจำนวน## \n##     setosa versicolor  virginica \n##         50         50         50"},{"path":"loop.html","id":"loop","chapter":"3 Loop","heading":"3 Loop","text":"","code":""},{"path":"loop.html","id":"general-loop","chapter":"3 Loop","heading":"3.1 General loop","text":"ท่านสามารถสร้าง Loop ได้เหมือนภาษาอื่นๆ โดยทั่วไปใน Rอย่างไรก็ตาม การใช้ Loop ใน R มีข้อควรระวังที่มากกว่าภาษาอื่นๆ เช่น python, java ที่ออกแบบมาเพื่อ Programming โดยเฉพาะ คือ การ Loop เข้า Memory ที่ไม่ได้มีการ Pre-allocate ไว้จะช้ามาก เมื่อเทียบกับ Pre-allocate memory loop หรือ Vectorized functionนั่นหมายความว่า การเรียกฟังก์ชันที่เร็วที่สุดควรเลือกฟังก์ชันที่มีการเขียนให้รองรับ Vectorization (เรียกคำสั่งภายในพร้อมกันทุกตัวแปร) อยู่แล้วPre-allocate ความจำก่อน ตามจำนวน (Length) ของข้อมูลสุดท้ายที่คาดว่าจะได้การสร้างตัวแปรใหม่ใส่เข้าไปในตัวแปรเรื่อยๆ โดยไม่ได้ Pre-allocate ไว้ ควรเป็นทางเลือกสุดท้ายอย่างไรก็ตามถ้าท่านไม่ได้เขียน package ใช้ หรือทำการวิเคราะห์ที่เป็นแนว Programming มากนัก ท่านอาจจะไม่ต้องใส่ใจส่วนนี้มาก","code":"\nfor(i in 1:2){\n  for (j in c(\"a\",\"b\"))\n  print(c(i,j))\n}## [1] \"1\" \"a\"\n## [1] \"1\" \"b\"\n## [1] \"2\" \"a\"\n## [1] \"2\" \"b\"\ni <- 0\nwhile(i <= 5){\n  print(i)\n  i <- i+1\n}## [1] 0\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\nitem_list <- list(\"apple\", \"banana\", \"fish\")\nfor(i in 1:length(item_list)){\n  if (item_list[[i]]== \"fish\"){\n    print(\"not fruit\")\n  }\n  else(print(item_list[[i]]))\n}## [1] \"apple\"\n## [1] \"banana\"\n## [1] \"not fruit\"\nset.seed(123)\nmat <- matrix(rnorm(10000000, 100, 200), nrow = 10000) # Large matrix\n\nsystem.time({\nfor(i in 1:nrow(mat)){\n res1 <- c()\n res1[[i]] <- mean(mat[i,])\n}\n})##    user  system elapsed \n##    0.01    0.05    0.34\nsystem.time({\nres2 <- vector(length = nrow(mat)) # Pre-allocate memory\nfor(i in 1:nrow(mat)){\n res2[i] <- mean(mat[i,])\n}\n}) ## 3x faster##    user  system elapsed \n##    0.03    0.00    0.08\nsystem.time({\nrowMeans(mat)\n}) ## Instant for vectorized function##    user  system elapsed \n##    0.00    0.00    0.01"},{"path":"loop.html","id":"apply-family","chapter":"3 Loop","heading":"3.2 Apply family","text":"apply family เป็น Function สำหรับ Loop ใน R ซึ่งมีการ Pre-allocate memory ไว้เรียบร้อยแล้ว ซึ่งเหมาะแก่การใช้งานมากกว่าการใช้ /loop ตามปกติ ประกอบไปด้วยlapply จะทำการเรียก Function ตามลำดับและส่งผลออกมาเป็น ListNote: \\(x) ... หรือ function (x) ... คือการเรียกฟังก์ชันที่จะใช้ใน Loop นั้น โดยไม่ต้องสร้าง Function ขึ้นมาใหม่ เรียกว่า Anonymous functionsapply เหมือน lapply แต่จะส่งผลกลับมาแบบไม่เป็น list อย่างไรก็ตาม ถ้าไม่สามารถขมวดรวมกันได้ จะส่งกลับมาเป็น list เหมือน lapplyapply คือการเรียกฟังก์ชันตามมิติ โดย 1 = แถว 2 = คอลัมน์ มีไว้ใช้กับข้อมูลประเภท Matrix และ Dataframemapply คือ การเรียกฟังก์ชันจากหลาย List พร้อมกันtapply คือ การเรียกฟังก์ชันตาม Subset","code":"\nset.seed(123)\nx <- 1:5\nlapply(x, \\(x) sample(100, size = x, replace = TRUE))## [[1]]\n## [1] 31\n## \n## [[2]]\n## [1] 79 51\n## \n## [[3]]\n## [1] 14 67 42\n## \n## [[4]]\n## [1] 50 43 14 25\n## \n## [[5]]\n## [1] 90 91 69 91 57\nsample_test <- function(x){\n  sample(100, size = x, replace = TRUE)\n}\nset.seed(123)\nlapply(x , sample_test) # same result with above anonymous function## [[1]]\n## [1] 31\n## \n## [[2]]\n## [1] 79 51\n## \n## [[3]]\n## [1] 14 67 42\n## \n## [[4]]\n## [1] 50 43 14 25\n## \n## [[5]]\n## [1] 90 91 69 91 57\nset.seed(123)\nx <- c(1,1,1,1,1,1)\nsapply(x, \\(x) sample(100, size = x, replace = TRUE)) # not list## [1] 31 79 51 14 67 42\nx <- 1:5\nsapply(x, \\(x) sample(100, size = x, replace = TRUE)) # cant convert to vector## [[1]]\n## [1] 50\n## \n## [[2]]\n## [1] 43 14\n## \n## [[3]]\n## [1] 25 90 91\n## \n## [[4]]\n## [1] 69 91 57 92\n## \n## [[5]]\n## [1]  9 93 99 72 26\nset.seed(123)\nmat <- matrix(runif(100, 100, 200), nrow = 10) # 10x10 matrix\napply(mat, 1, sd) # sd by row##  [1] 37.22781 22.92317 16.93264 33.84358 32.38341 27.03874 28.94264 32.21673 25.33109 28.95514\napply(mat, 2, sd) # sd by column##  [1] 29.47400 34.48212 25.09712 32.73406 21.78202 30.93267 26.08665 25.76299 32.57488 23.84909\nx <- list(1,2,3)\ny <- list(2,4,6)\nz <- list(3,6,9)\n\nmapply(FUN = \\(x,y,z) x*y*z, x,y,z) # x1*y1*z1, x2*y2*z2, x3*y3*z3## [1]   6  48 162\nmapply(FUN = rep, times = 1:4, \n       MoreArgs = list(x = 42)) # rep(42, 1), rep(42, 2), ...## [[1]]\n## [1] 42\n## \n## [[2]]\n## [1] 42 42\n## \n## [[3]]\n## [1] 42 42 42\n## \n## [[4]]\n## [1] 42 42 42 42\ntapply(iris$Sepal.Length, iris$Species, mean)##     setosa versicolor  virginica \n##      5.006      5.936      6.588"},{"path":"what-is-tidyverse.html","id":"what-is-tidyverse","chapter":"4 What is tidyverse","heading":"4 What is tidyverse","text":"Tidyverse (Wickham, 2023) เป็น Package ที่นิพนธ์ขึ้นมาเพื่อทำใน R ซึ่งมีความสามารถหลากหลาย โดยเฉพาะการปรับแต่งข้อมูลจาก Dataframe ซึ่งจะอำนวยความสะดวกให้ท่านสามารถทำงานได้มากขึ้นกว่าการใช้ Base Rข้อเสียของ tidyverse นั้น อาจจะทำให้การประมวลผลช้ากว่า base R เล็กน้อย และมีปรับแต่งให้ตรงกับการใช้งานจำเพาะได้ยากกว่า แต่สำหรับผู้ที่ไม่ใช่ R hardcore นั้น tidyverse ถือว่าเป็น Package ที่อำนวยความสะดวกได้อย่างดีเยี่ยมโดยการจะใช้งาน Package ใดๆ นั้น เริ่มจากท่านจะต้องติดตั้ง Package นั้นลงบนเครื่องของท่านก่อน ด้วยคำสั่ง install.packages()ซึ่ง tidyverse นั้นจะเป็น Package ใหญ่ และจะแบ่งเป็นหลาย Package ย่อยๆ ได้อีก โดยท่านสามารถเรียกใช้ ทั้งหมดได้ หรือ เรียกใช้แค่ Package ย่อย","code":"\ninstall.packages(\"tidyverse\") \nlibrary(tidyverse) "},{"path":"importing-data.html","id":"importing-data","chapter":"5 Importing data","heading":"5 Importing data","text":"","code":""},{"path":"importing-data.html","id":"importing-data-with-readr","chapter":"5 Importing data","heading":"5.1 Importing data with readr","text":"ก่อนที่ท่านจะทำการวิเคราะห์ข้อมูลได้นั้น ท่านจำเป็นที่จะต้องนำข้อมูลเข้ามาใน R ให้ได้ก่อน ซึ่ง tidyverse ได้มี Package สำหรับการนำข้อมูลจากสกุลไฟล์ที่เป็นที่นิยมส่วนใหญ่เข้าสู่ R ได้เกือบทั้งหมด โดยใช้ฟังก์ชัน read_*()ไฟล์ที่ได้อ่านเข้ามานี้ คือ Gene expression ของ RNA microarray ในชิ้นเนื้อผู้ป่วยมะเร็งปากมดลูก ซึ่งจะถูกนำไปใช้ต่อใน ตัวอย่างท้ายบท","code":"\nlibrary(readr) # ต้อง run ทุกครั้งที่จะใช้งาน\n\nGSE63514 <- read_csv(\"Resource/GSE63514_norm.csv\")\n\nhead(GSE63514, 10) \nGSE63514_meta <- read_csv(\"Resource/GSE63514_meta.csv\")\n\nhead(GSE63514_meta, 10)"},{"path":"importing-data.html","id":"other-packages","chapter":"5 Importing data","heading":"5.2 Other packages","text":"ท่านสามารถเขียนข้อมูลจาก R ลงไปในไฟล์ที่ท่านต้องการด้วย write_()* อีกด้วย อย่างไรก็ตาม แม้ว่า readr นั้นจะสามารถอ่านและเขียนไฟล์ได้ครอบคลุมเป็นอย่างมาก ในบางสกุลไฟล์นั้น อาจจะต้องใช้การอ่านจาก package อื่น","code":""},{"path":"data-wrangling.html","id":"data-wrangling","chapter":"6 Data wrangling","heading":"6 Data wrangling","text":"","code":""},{"path":"data-wrangling.html","id":"data-manipulation-with-dplyr","chapter":"6 Data wrangling","heading":"6.1 Data manipulation with dplyr","text":"dplyr คือ Package ย่อยของ tidyverse ซึ่งทำหน้าที่จัดการ Dataframe ที่ท่านนำเข้าไปใน R ให้เป็นในรูปแบบที่ท่านต้องการ","code":"\nlibrary(dplyr) "},{"path":"data-wrangling.html","id":"basic-dataframe-manipulation","chapter":"6 Data wrangling","heading":"6.1.1 Basic dataframe manipulation","text":"ในกรณีนี้จะใช้ข้อมูลตัวอย่าง iris เพื่อสาธิตการใช้ dplyr โดย iris เป็นข้อมูลของความยาวกลีบของพันธุ์ดอกไม้ต่างๆรูปจาก: https://www.datacamp.com/tutorial/machine-learning--rฟังก์ชันหลักๆ ของ dplyr จะเกี่ยวข้องกับ data manipulation เป็นส่วนใหญ่ ในที่นี้จะแนะนำที่จำเป็นต้องใช้ในบทอื่นglimpse() มีไว้ดูภาพรวมข้อมูลselect() เลือก column ที่ต้องการโดยใช้ตำแหน่งหรือชื่อ column ก็ได้filter() กรองแถว (row) ที่ต้องการ โดยต้องระบุ ว่าต้องการข้อมูล ที่ column ไหน และต้องการกรองค่าที่เท่าไรสังเกตว่าจะเห็นเครื่องหมาย |> ซึ่งใน R ท่านจะเรียกว่า “pipe operator” เป็นสิ่งที่เป็นเอกลักษณ์ใน R ซึ่งส่งผลให้สามารถ run operation ได้ต่อๆ กัน เพื่อให้อ่านได้ง่ายบรรทัดสุดท้าย สำหรับ Dataframe จะไม่สามารถดึงมาทั้งคอลัมน์ได้ ซึ่งจะต้องใช้ข้อมูลอีกแบบ (Tibble) แต่จะไม่กล่าวถึง ณ ที่นี่Note: การ subset โดย dplyr นั้นสามารถทำใน Dataframe/Tibble เท่านั้น ไม่สามารถทำใน Matrix ได้ (ต้องใช้วิธีของ base R)ในส่วนการเรียงข้อมูลนั้นจะใช้ ฟังก์ชัน arrange()mutate() เป็นคำสั่งที่ใช้ในการสร้างคอลัมน์ใหม่ให้เป็นในแบบที่ต้องการได้ท่านสามารถจัดกลุ่มตัวแปรได้โดยใช้ group_by() โดยมักจะใช้คู่กับ summarize() ซึ่งเป็นคำสั่งที่ใช้ในการสรุปข้อมูลทั้งหมดตามที่ต้องการ หรือใน dplyr 1.1.0 ขึ้นไปท่านสามารถใช้ .= … ได้rename() สามารถใช้ในการเปลี่ยนชื่อคอลัมน์ ระวังว่าชื่อที่ต้องการจะอยู่ด้านซ้ายของเครื่องหมาย = ซึ่งไม่เหมือนคำสั่งอื่น","code":"\ndf <- iris # โหลด dataframe ตัวอย่างที่ติดมากับ base R\nhead(df, 5)\nglimpse(df)## Rows: 150\n## Columns: 5\n## $ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.…\n## $ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.0, 4.…\n## $ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.…\n## $ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.…\n## $ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setos…\ndf |> select(Species) |> head(5) # เลือก column \"Species\"\ndf |> select(2) |> head(5) # เลือก column ที่ 2\ndf |> select(1:2) |> head(5) # เลือก 2 column\ndf |> select(contains(\"Length\")) |> head(5) # เลือก column ที่มีคำว่า \"Length\"\n# เลือกแถวที่ Species == virginica\ndf |>  filter(Species == \"virginica\") |>  head(5)\n# เลือกแถวที่ Species = setosa, Sepal.Length = 5.4\ndf |>  \n  filter(Species == \"setosa\" & Sepal.Length == 5.4) |>  head(5)\n# เลือกแถวที่ Sepal.Length = 5.1 หรือ 4.9\ndf |>  filter(Sepal.Length == 5.1 | Sepal.Length == 4.9) |>  head(10)\n# เลือกแถวที่ Species = setosa คอลัมน์ Sepal.Length\ndf |> \n  filter(Species == \"setosa\") |> \n  select(Sepal.Length) |> head(5)\n# เหมือนกับข้างบน แต่ไม่ใช้ pipe operator จะทำความเข้าใจได้ยากกว่า\nselect(filter(df, Species == \"setosa\"), Sepal.Length) |>  head(5)\n# ใช้แค่ base R solution จะไม่สามารถดึงออกมาเป็น dataframe ได้\ndf[df[\"Species\"] == \"setosa\", \"Sepal.Length\"]##  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 5.7 5.1 5.4 5.1 4.6 5.1\n## [25] 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6\n## [49] 5.3 5.0\ndf |> \n  arrange(Sepal.Length) |> head(5) # เรียง Sepal.Length จากน้อยไปมาก\ndf |> \n  arrange(desc(Sepal.Length)) |>  head(5) # เรียง Sepal.Length จากมากไปน้อย\ndf |> \n  mutate(Sepal_mm = Sepal.Length*100) # มิลลิเมตร\ndf |> \n  mutate(Sepal.Length = log10(Sepal.Length)) # สามารถแทนที่ column เดิมได้ด้วย\ndf |>  \n  group_by(Species) |>  # จัดกลุ่มตาม Species\n  summarize(Sepal.Length = sum(Sepal.Length), \n            Sepal.Width = mean(Sepal.Width)) # รวมความยาวทั้งหมด และเฉลี่ยความกว้าง\ndf |>  \n  summarize(Sepal.Length = sum(Sepal.Length), \n            Sepal.Width = mean(Sepal.Width),\n            .by = Species) \ndf |> \n  rename(\"Sepal_length\" = \"Sepal.Length\", \"Sepal_width\" = \"Sepal.Width\") "},{"path":"data-wrangling.html","id":"tidyselect","chapter":"6 Data wrangling","heading":"6.1.2 Tidyselect","text":"ในหลายๆ ฟังก์ชัน เช่น select() ,mutate(), summarize() ท่านสามารถใช้คำสั่งตัวช่วย (Selection helper) เพื่อให้การเลือกคอลัมน์ที่ต้องการเป็นไปได้สะดวกยิ่งขึ้นในส่วนของการใช้ mutate() และ summarise() ร่วมกับคำสั่งตัวช่วยนั้น ท่านต้องใช้ภายในคำสั่ง across()","code":"\ndf |> \n  select(contains(\"Length\")) # เลือกคอลัมน์ที่มีคำว่า \"Length\"\ndf |> \n  select(last_col()) # เลือกคอลัมน์สุดท้าย\ndf |> \n  summarize(across(where(is.numeric), .fns = mean))"},{"path":"data-wrangling.html","id":"joining-data","chapter":"6 Data wrangling","heading":"6.1.3 Joining data","text":"หลายครั้งที่การจัดการกับข้อมูลนั้นมีที่มาจากหลายส่วน โดยคอลัมน์หลักร่วมเพียงไม่กี่คอลัมน์ ผู้วิเคราะห์สามารถรวมตารางจากหลายแห่งเข้าด้วยกันได้โดยการใช้คำสั่ง x_join เพื่อความสะดวกในการวิเคราะห์","code":""},{"path":"data-wrangling.html","id":"mutating-join","chapter":"6 Data wrangling","heading":"6.1.3.1 Mutating join","text":"Mutating join คือการรวมตารางสองตารางเข้าด้วยกันภายใต้เงื่อนไขต่างๆ ในคอลัมน์หลักที่กำหนดต่อไปจะใช้ตารางดังต่อไปนี้ในการแสดงตัวอย่างinner_join() รวมบรรทัดที่มีตัวแปรที่มีร่วมกันทั้งสองตารางfull_join() หรือ full outer join รวมทุกบรรทัดleft_join() รวมบรรทัดจากตาราง y ที่มีตัวแปรในตาราง x และคงบรรทัดในตาราง x ทั้งหมดright_join() รวมบรรทัดจากตาราง x ที่มีตัวแปรในตาราง y และคงบรรทัดในตาราง y ทั้งหมด","code":"\ninner_join(score_df, grade_df, by = \"Name\")\nfull_join(score_df, grade_df, by = \"Name\")\nleft_join(score_df, grade_df, by = \"Name\")\nright_join(score_df, grade_df, by = \"Name\")"},{"path":"data-wrangling.html","id":"filtering-join","chapter":"6 Data wrangling","heading":"6.1.3.2 Filtering join","text":"Filtering join คือการกรองบรรทัดในตาราง xโดยเงื่อนไขจากตาราง ysemi_join() กรองบรรทัดในตาราง x ที่มีตัวแปรในตาราง yanti_join() กรองบรรทัดในตาราง x ที่ไม่มีตัวแปรในตาราง y","code":"\nsemi_join(score_df, grade_df, by = \"Name\")\nanti_join(score_df, grade_df, by = \"Name\")"},{"path":"data-wrangling.html","id":"reshape","chapter":"6 Data wrangling","heading":"6.2 Reshaping data with tidyr","text":"","code":""},{"path":"data-wrangling.html","id":"data-structure","chapter":"6 Data wrangling","heading":"6.2.1 Data structure","text":"โดยปกติแล้วรูปแบบลักษณะของการบันทึกข้อมูลนั้นจะมีอยู่ 2 ลักษณะWide form เป็นลักษณะที่ง่ายต่อการบันทึก วิเคราะห์และอ่านผลเบื้องต้น โดยมีรูปแบบคือ ในแต่ละแถวนั้น จะมีข้อมูลหลักที่ไม่ซ้ำกัน (มักจะเป็นข้อมูลระบุตัวตน)Long form เป็นลักษณะที่ง่ายต่อการ Visualize โดยมีรูปแบบคือ สามารถมีข้อมูลหลักที่ซ้ำกันได้ลองทำการดูที่ข้อมูล iris อีกครั้งจะเห็นว่า ข้อมูลในแต่ละแถวนั้น คือ ดอกไม้ 1 ดอก จำนวนคอลัมน์จะมากกว่าข้อมูลแบบ Long form","code":"\nhead(df, 10)\ndf_id <- df |> \n  mutate(flower_id = row_number(), \n         .before = everything()) # สร้าง unique id ดอกไม้แต่ละดอก\n\nhead(df_id)"},{"path":"data-wrangling.html","id":"wide-to-long","chapter":"6 Data wrangling","heading":"6.2.2 Wide to long","text":"ท่านสามารถเปลี่ยนข้อมูลจาก Wide form เป็น Long form ได้โดย package tidyr โดยใช้ฟังก์ชัน pivot_longer()ซึ่งจะทำให้สามารถวิเคราะห์ข้อมูลได้สะดวกขึ้น ยกตัวอย่างถ้าเราต้องการสรุปข้อมูลชุดนี้ถ้าลองทำในข้อมูล Wide formจะเห็นว่าค่อนข้าง intensive และผิดพลาดง่ายปล. อย่างไรก็ตาม dplyr ในปัจจุบันมีการพัฒนาไปมาก การวิเคราะห์ ใน Wide form ก็สามารถทำได้โดยง่ายอย่างที่เคยกล่าวไปขั้นต้น ขึ้นอยู่กับว่าถนัดแบบใดมากกว่าอีก ประเด็นสำคัญ ของข้อมูลประเภท Long form นั้นคือ สามารทำ Visualization ที่ซับซ้อนได้ดีกว่า Wide form เป็นอย่างมาก ดังตัวอย่าง Boxplot1, Boxplot2","code":"\nlong_df <- df_id |> \n    pivot_longer(cols = !c(flower_id, Species), \n                 names_to = \"Metrics\", values_to = \"cm\") # ไม่รวมคอลัมน์ Species\n\nhead(long_df,10)\nsummary_df <- long_df |> \n  group_by(Species, Metrics) |>\n  summarize(`Median (cm)` = median(cm),`Mean (cm)` = mean(cm), `sd (cm)` = sd(cm))\n\nsummary_df\ndf |> \n  group_by(Species) |> \n  summarize(mean_Petal_L = mean(Petal.Length), \n            median_Petal_L = median(Petal.Length), \n            sd_Petal_L = sd(Petal.Length),\n            mean_Petal_W = mean(Petal.Width), \n            median_Petal_W = median(Petal.Width), \n            sd_Petal_W = sd(Petal.Width),\n            mean_Setal_L = mean(Sepal.Length), \n            median_Setal_L = median(Sepal.Length), \n            sd_Setal_L = sd(Sepal.Length),\n            mean_Setal_W = mean(Sepal.Width), \n            median_Setal_W = median(Sepal.Width), \n            sd_Setal_W = sd(Sepal.Width),\n            )\ndf |> \n  group_by(Species) |>    \n  summarize(across(everything(), \n                   list(median = median, mean = mean, sd = sd)))"},{"path":"data-wrangling.html","id":"long-to-wide","chapter":"6 Data wrangling","heading":"6.2.3 Long to wide","text":"ท่านสามารถเปลี่ยนกลับเป็น Wide form ได้เช่นกันหรือท่านอยากจะเปลี่ยนข้อมูลที่สรุปแล้วให้เป็น Wide form ก็เป็นได้","code":"\nwide_df <- long_df |> \n    pivot_wider(names_from = \"Metrics\", values_from = \"cm\")\n\nhead(wide_df, 10)\nsummary_df |> \n  pivot_wider(names_from = \"Metrics\", \n              values_from = c(\"Median (cm)\" ,\"Mean (cm)\", \"sd (cm)\"))"},{"path":"data-wrangling.html","id":"separate-and-unite-column-with-tidyr","chapter":"6 Data wrangling","heading":"6.3 Separate and unite column with tidyr","text":"ท่านสามารถแยกคอลัมน์ที่มีช่องว่างออกจากกันได้โดยใช้คำสั่ง separate()ถ้าท่านต้องการรวมคอลัมน์เข้าด้วยสามารถใช้คำสั่ง unite()","code":"\nsep_name_df <- name_df |> \n  separate(Names, into = c(\"First\", \"Last\"), sep = \", \")\nsep_name_df\nsep_name_df |> \n  unite(col = \"Full\", c(\"First\", \"Last\"), sep = \" \")"},{"path":"data-visualization-with-ggplot2.html","id":"data-visualization-with-ggplot2","chapter":"7 Data visualization with ggplot2","heading":"7 Data visualization with ggplot2","text":"","code":""},{"path":"data-visualization-with-ggplot2.html","id":"building-a-plot","chapter":"7 Data visualization with ggplot2","heading":"7.1 Building a plot","text":"ggplot2 คือ Package ย่อยอีกตัวของ tidyverse ซึ่งใช้สำหรับการพล็อตกราฟ","code":""},{"path":"data-visualization-with-ggplot2.html","id":"anatomy-of-ggplot","chapter":"7 Data visualization with ggplot2","heading":"7.1.1 Anatomy of ggplot","text":"aes คือ aesthetic ซึ่งหมายถึงการ map ข้อมูลของท่านเข้ากับตำแหน่งของกราฟ\nx = แกน x, y = แกน y\ncol = สี, fill = สีพื้นหลัง\nx = แกน x, y = แกน ycol = สี, fill = สีพื้นหลังgeom_*() คือ การกำหนดว่าท่านต้องการที่จะ plot กราฟอะไรtheme_*() คือ การกำหนด theme ของกราฟ เพื่อความสวยงาม เช่น theme_bw(), theme_classic()… คือการปรับแต่งอื่นๆ ในส่วนของ Customizationการสร้างภาพที่สมบูรณ์นั้นมีส่วนจำเป็นที่ประกอบด้วย aes และ geom ตรงส่วนอื่นเป็นส่วนเสริมที่จะช่วยให้ภาพมีความสวยงามขึ้นสังเกตว่าจะยังไม่มีกราฟใดๆ ปรากฏ ท่านจำเป็นต้องใช้ geom เพื่อทำการสร้างภาพนั้นขึ้น","code":"ggplot(data = your_data, aes(x = x, y = y, col = col, fill = fill)) +\n  geom_*() +\n  theme_*() +\n  ...\nlibrary(ggplot2) \ndf\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length))"},{"path":"data-visualization-with-ggplot2.html","id":"scatter-plot","chapter":"7 Data visualization with ggplot2","heading":"7.1.2 Scatter plot","text":"สังเกตการ mapping ของ aes()","code":"\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, \n               col = Species, shape = Species)) + \n  geom_point()"},{"path":"data-visualization-with-ggplot2.html","id":"straight-line","chapter":"7 Data visualization with ggplot2","heading":"7.1.3 Straight line","text":"ท่านสามารถเพิ่มเส้นที่ท่านต้องการได้โดย geom_hline (แนวตั้ง), geom_vline (แนวนอน), geom_abline (แนวเฉียง)สังเกตว่าท่านสามารถปรับค่าจำพวก สี ลักษณะเส้นต่างๆ ได้ โดย Parameter นั้น ต้องอยู่นอก aes มิเช่นนั้น ฟังก์ชันจะพยายามไปดึงข้อมูลจากกราฟมาที่ถูกต้อง ท่านต้องนำ col ไปอยู่นอก aes จึงจะได้สีที่ท่านต้องการ","code":"\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = Species)) + \n  geom_point() + \n  geom_hline(yintercept = 6, linetype = \"dashed\", \n             col = \"red\", linewidth = 1) + \n  geom_vline(xintercept = 2.7, linetype = \"dotted\", \n             col = \"blue\", linewidth = 1.25) + \n  geom_abline(intercept = 2, slope = 1, \n              linetype = \"dotdash\", col = \"black\", linewidth = 1.5)\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = \"darkviolet\")) + # ผิด\n  geom_point() + \n  geom_hline(yintercept = 6, linetype = \"dashed\", \n             col = \"red\", linewidth = 1) + \n  geom_vline(xintercept = 2.7, linetype = \"dotted\", \n             col = \"blue\", linewidth = 1.25) + \n  geom_abline(intercept = 2, slope = 1, \n              linetype = \"dotdash\", col = \"black\", linewidth = 1.5)\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length)) + # ผิด\n  geom_point(col = \"darkviolet\") + \n  geom_hline(yintercept = 6, linetype = \"dashed\", \n             col = \"red\", linewidth = 1) + \n  geom_vline(xintercept = 2.7, linetype = \"dotted\", \n             col = \"blue\", linewidth = 1.25) + \n  geom_abline(intercept = 2, slope = 1, linetype = \"dotdash\", \n              col = \"black\", linewidth = 1.5)"},{"path":"data-visualization-with-ggplot2.html","id":"bar-chart","chapter":"7 Data visualization with ggplot2","heading":"7.1.4 Bar chart","text":"geom_bar() ใช้สำหรับนับจำนวนของคอลัมน์นั้น ไม่มีค่า yส่วน geom_col() จะรับค่า y ด้วย โดยข้อมูล x ที่ซ้ำกันจะถูกนำมารวมกันสังเกตว่าค่าที่ได้เกิดจากการรวมกันของข้อมูลทั้งคอลัมน์ (สังเกตที่เส้นสีดำเป็นเส้นต่อๆ กัน ไม่ใช่เส้นเดียว) ซึ่งมักไม่เป็นที่ต้องการในการแสดง โดยมักเกิดจากความผิดพลาดมากกว่า (โดยเฉพาะถ้าไม่ได้ใส่ col = black) และส่วนใหญ่มักจะใช้ในการแสดงค่าเฉลี่ยมากกว่าผลรวม ในการนี้ ควรใช้คำสั่ง dplyr::summarize() ในการสรุปข้อมูลก่อนจะเห็นว่ากราฟแสดงค่าเฉลี่ยซึ่งตรงตามความต้องการทั่วไปมากกว่า (สังเกตแกน y)","code":"\nggplot(df, aes(x = Species, fill = Species)) + # fill ไว้สำหรับแบ่งสีใน barchart\n  geom_bar(col = \"black\", width = 0.5) # ความกว้าง 50% \nggplot(df, aes(x = Species, y = Sepal.Width, fill = Species)) + \n  geom_col(col = \"black\", width = 0.5) \ndf |> \n  group_by(Species) |> \n  summarize(across(everything(), mean)) |> \n  ggplot(aes(x = Species, y = Sepal.Width, fill = Species)) + \n  geom_col(col = \"black\", width = 0.5) "},{"path":"data-visualization-with-ggplot2.html","id":"multi_boxplot","chapter":"7 Data visualization with ggplot2","heading":"7.1.5 Box plot","text":"ทำการสร้าง Box plotถ้าท่านต้องการสร้าง Plot ที่แสดงหลาย metrics ท่านจะต้องเปลี่ยนข้อมูลเป็น Long form เสียก่อน","code":"\nggplot(df, aes(x = Species, y = Sepal.Width, fill = Species)) +\n  geom_boxplot(width = 0.5) \nhead(long_df, 10)\nlong_df |> \n    ggplot(aes(x = Species, y = cm, fill = Metrics)) +\n    geom_boxplot() "},{"path":"data-visualization-with-ggplot2.html","id":"histogram","chapter":"7 Data visualization with ggplot2","heading":"7.1.6 Histogram","text":"ในการทำงานสถิตินั้น โดยส่วนใหญ่จะต้องทำการตรวจสอบการกระจายของข้อมูลก่อนวิเคราะห์ทางสถิติ ซึ่งสามารถทำได้โดยใช้ geom_histogram() หรือ geom_density() โดยท่านสามารถเลือกประเภทของข้อมูลที่ใช้ในแกน y ได้โดยใช้ after_stat()ทั้งนี้ ท่านสามารถพล็อตหลายกราฟเข้าด้วยกันได้ ด้วยการ + ตามหลังไปเรื่อยๆ เพียงแต่ต้องระวังเรื่อง scale ที่ต้องเป็นระดับเดียวกัน","code":"\nggplot(df, aes(x = Sepal.Width)) + \n  geom_histogram(fill = \"skyblue\", binwidth = 0.1)  # binwidth = ความกว้างของแต่ละช่วงข้อมูล\nggplot(df, aes(x = Sepal.Width, y = after_stat(density))) + # ใช้ density\n  geom_histogram(fill = \"skyblue\", binwidth = 0.1)  \nggplot(df, aes(x = Sepal.Width)) + \n  geom_density(fill = \"violet\", alpha = 0.5)\nggplot(df, aes(x = Sepal.Width, y = after_stat(count))) + # ใช้ count \n  geom_density(fill = \"violet\", alpha = 0.5)\nggplot(df, aes(x = Sepal.Width)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.1, fill = \"skyblue\") + # ปรับเป็นความถี่\n  geom_density(fill = \"violet\", alpha = 0.5) +\n  theme_bw() # ลบภาพพื้นหลังสีเทาออก"},{"path":"data-visualization-with-ggplot2.html","id":"fitting-a-statistical-model","chapter":"7 Data visualization with ggplot2","heading":"7.1.7 Fitting a statistical model","text":"ท่านสามารถที่จะพล็อต Statistical model ได้โดยใช้ geom_smooth() ยกตัวอย่าง เช่น ถ้าอยากดูความสัมพันธ์ของ Sepal.Length และ Petal.Length","code":"\nggplot(df, aes(x = Sepal.Length, y = Petal.Length, color = Species)) + # สีตาม Species\n  geom_point(color = \"black\") +\n  geom_smooth(method = \"loess\") # fit a LOESS model\nggplot(df, aes(x = Sepal.Length, y = Petal.Length, color = Species)) + # สีตาม Species\n  geom_point(color = \"black\") +\n  geom_smooth(method = \"lm\") # fit a linear model"},{"path":"data-visualization-with-ggplot2.html","id":"ggplot-customization","chapter":"7 Data visualization with ggplot2","heading":"7.2 Customization","text":"","code":""},{"path":"data-visualization-with-ggplot2.html","id":"faceting","chapter":"7 Data visualization with ggplot2","heading":"7.2.1 Faceting","text":"ในบางครั้งท่านอาจจะต้องการที่จะพล็อตกราฟแยกกันเป็นส่วนๆ มากกว่ารวมกันในกราฟเดียว ท่านสามารถแบ่ง Partition ของการพล็อตแต่ละกลุ่มได้โดยใช้ facet_wrap()และถ้าท่านต้องการเรียงข้อมูลให้เป็นไปตามแกนที่ท่านต้องการ สามารถใช้ facet_grid()","code":"\nggplot(df, aes(x = Sepal.Length, y = Petal.Length, \n               color = Species)) + # สีตาม Species\n  geom_point(color = \"black\") +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~Species) + # แบ่งเป็นหลายกลุ่ม\n  theme_bw()\nggplot(df, aes(x = Sepal.Length,  fill = Species)) + # สีตาม Species\n  geom_histogram(binwidth = 0.1) +\n  facet_wrap(~Species, scales = \"free_x\", nrow = 2) + # ทำให้แกน x ไม่ fix ค่า\n  theme_bw() \nggplot(mtcars, \n       aes(x = mpg, y = disp, color = as.factor(cyl))) + \n  facet_grid(vs + am ~ gear, \n             labeller = label_both, # แสดงชื่อและข้อมูล \n             scale = \"free\") +\n  geom_point() +\n  theme_bw()"},{"path":"data-visualization-with-ggplot2.html","id":"labels","chapter":"7 Data visualization with ggplot2","heading":"7.2.2 Labels","text":"ท่านสามารถปรับแต่งชื่อของแต่ละตำแหน่งได้โดยใช้ labs()สำหรับสัญลักษณ์ทางคณิตศาสตร์สามารถแสดงได้โดยใช้คำสั่ง bquote()Mathematics quote ที่ใช้บ่อยมีดังนี้สามารถศึกษาเพิ่มเติมได้ที่ ?plotmath","code":"\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = Species)) + \n  geom_point() +\n  labs(title = \"Iris dataset\", subtitle = \"Sepal length vs Sepal width\",\n       x = \"Sepal width\", y = \"Sepal length\", \n       color = \"List of species\",\n       tag = \"1\") +\n  theme_bw()\npoly_data <- data.frame(x = seq(-10, 10, length.out = 100)) |> \n  mutate(y = x^9+x+10+rnorm(100, mean = 0, sd =3))\n\nggplot(poly_data, aes(x = x, y = y)) + geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(y = bquote(\"This function is\" ~ f(x) == x^9+x+10+epsilon ~ \".\")) +\n  theme_bw()"},{"path":"data-visualization-with-ggplot2.html","id":"scales","chapter":"7 Data visualization with ggplot2","heading":"7.2.3 Scales","text":"ท่านสามารถปรับแต่ง Scale ของแกน x แกน y และสีต่างได้โดยใช้ฟังก์ชัน scale_*_*()Note: ท่านจะเห็นข้อความเตือนว่าข้อมูลที่เกินกว่า Axis นั้นจะถูกตัดออกไปนอกจากนั้น ท่านยังสามารถเปลี่ยนข้อมูลเป็นรูปแบบอื่นๆ ได้ด้วยในส่วนตัวแปรประเภท Discrete นั้นก็สามารถปรับเปลี่ยนได้เช่นกัน","code":"\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = Species)) + \n  geom_point() +\n  scale_x_continuous(breaks = seq(0,5,0.5), # Major breaks\n                     minor_breaks = seq(0,5,0.1), # Minor breaks\n                     limits = c(1,5)) + # Axis limits\n  scale_y_continuous(breaks = seq(0,8,0.5), \n                     minor_breaks = seq(0,8,0.1),\n                     limits = c(5,8)\n                    ) +\n  theme_bw()## Warning: Removed 22 rows containing missing values (`geom_point()`).\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = Species)) + \n  geom_point() +\n  scale_x_continuous(name = bquote(sqrt(Sepal.Width)), \n                     trans = \"sqrt\") + # เปลี่ยนชื่อได้เลย\n  scale_y_continuous(name = \"Reverse Sepal.Length\",\n                  trans = \"reverse\") + # More information ?scale_x_continuous \n  theme_bw()\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = Species)) + \n  geom_point() +\n  scale_color_manual(values = c(\"darkred\", \"darkblue\", \"darkgreen\"),\n    labels = c(\"Setosa\", \"Versicolor\", \"Virginica\")) +\n  theme_bw()"},{"path":"data-visualization-with-ggplot2.html","id":"color-palettes","chapter":"7 Data visualization with ggplot2","heading":"7.2.4 Color palettes","text":"ggplot2 นั้นมีชุดของสีเตรียมไว้ให้ส่วนหนึ่งแล้วสำหรับการสร้างกราฟ โดยใช้ scale_*_brewer()สีทั้งหมดที่มาพร้อมกับ ggplot2() มีดังนี้และยังมีสีเพิ่มเติมสำหรับการวาดกราฟที่ใช้บ่อยในสารสาร ต่างๆ ใน Package ggsci ศึกษารายละเอียดเพิ่มเติมได้ที่ https://nanx./ggsci/ในส่วนของสีที่เป็นพื้นที่ จะใช้ scale_fill_*() ซึ่งมีลักษณะการใช้ในแบบเดียวกัน","code":"\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = Species)) + \n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme_bw()\nRColorBrewer::display.brewer.all()\nlibrary(ggsci)\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = Species)) + \n  geom_point() +\n  scale_color_nejm() +\n  theme_bw()"},{"path":"data-visualization-with-ggplot2.html","id":"themes","chapter":"7 Data visualization with ggplot2","heading":"7.2.5 Themes","text":"ในส่วนของภาพรวมของกราฟทั้งหมดนั้น สามารถปรับแต่งได้โดยฟังก์ชัน theme_*()ซึ่ง Theme ที่ใช้บ่อยนั้น ได้แก่theme_bw()theme_bw()theme_classic()theme_classic()theme_minimal()theme_minimal()ทั้งหมดที่แสดงนี้ เป็นเพียงกราฟพื้นฐานเท่านั้น ยังมีการปรับแต่งอื่นๆ ได้อีกมาก สามารถศึกษาเพิ่มเติมได้ที่Function reference: https://ggplot2.tidyverse.org/reference/Function reference: https://ggplot2.tidyverse.org/reference/Plot gallery: https://r-graph-gallery.com/Plot gallery: https://r-graph-gallery.com/Parameter ของ aesthetics ต่างๆ: เรียกฟังก์ชัน vignette(\"ggplot2-specs\")Parameter ของ aesthetics ต่างๆ: เรียกฟังก์ชัน vignette(\"ggplot2-specs\")","code":"\nggplot(df, aes(x = Sepal.Width, y = Sepal.Length, col = Species)) + \n  geom_point() +\n  theme_classic()"},{"path":"GSE63514.html","id":"GSE63514","chapter":"8 Example: Microarray data visualization","heading":"8 Example: Microarray data visualization","text":"ต่อไปนี้จะเป็นตัวอย่างในการใช้ R ในการวิเคราะห์ข้อมูล Molecular data เบื้องต้น โดยใช้ข้อมูลจาก GEO dataset GSE63514 Den Boon et al. (2015) ซึ่งเป็น Gene expression microarray","code":""},{"path":"GSE63514.html","id":"import-files","chapter":"8 Example: Microarray data visualization","heading":"8.1 Import files","text":"ไฟล์แรกที่อ่านเข้ามาคือ GSE63514_norm.csv ซึ่งเป็นไฟล์ Microarray expression ของชิ้นเนื้อปากมดลูก ประกอบด้วย ตัวอย่าง Normal , CIN1 , CIN2 , CIN3 , Cancer ในที่นี้เราจะทำการวิเคราะห์ 20,000 ยีนแรกส่วนที่สองคือ Metadata (ข้อมูลระบุตัวตน) ของข้อมูลนี้ส่วนที่สามคือไฟล์ Annotation ของ Probe","code":"\nlibrary(tidyverse) \n\nGSE63514 <- read_csv(\"Resource/GSE63514_norm.csv\") \nGSE63514_20000 <- read_csv(\"Resource/GSE63514_norm.csv\") |> head(2000)\nnames(GSE63514_20000)##   [1] \"probe\"                                  \"GSM1551311_Normal-01_U133_Plus2.CEL.gz\"\n##   [3] \"GSM1551312_Normal-02_U133_Plus2.CEL.gz\" \"GSM1551313_Normal-03_U133_Plus2.CEL.gz\"\n##   [5] \"GSM1551314_Normal-04_U133_Plus2.CEL.gz\" \"GSM1551315_Normal-05_U133_Plus2.CEL.gz\"\n##   [7] \"GSM1551316_Normal-06_U133_Plus2.CEL.gz\" \"GSM1551317_Normal-07_U133_Plus2.CEL.gz\"\n##   [9] \"GSM1551318_Normal-08_U133_Plus2.CEL.gz\" \"GSM1551319_Normal-09_U133_Plus2.CEL.gz\"\n##  [11] \"GSM1551320_Normal-10_U133_Plus2.CEL.gz\" \"GSM1551321_Normal-11_U133_Plus2.CEL.gz\"\n##  [13] \"GSM1551322_Normal-12_U133_Plus2.CEL.gz\" \"GSM1551323_Normal-13_U133_Plus2.CEL.gz\"\n##  [15] \"GSM1551324_Normal-14_U133_Plus2.CEL.gz\" \"GSM1551325_Normal-15_U133_Plus2.CEL.gz\"\n##  [17] \"GSM1551326_Normal-16_U133_Plus2.CEL.gz\" \"GSM1551327_Normal-17_U133_Plus2.CEL.gz\"\n##  [19] \"GSM1551328_Normal-18_U133_Plus2.CEL.gz\" \"GSM1551329_Normal-19_U133_Plus2.CEL.gz\"\n##  [21] \"GSM1551330_Normal-20_U133_Plus2.CEL.gz\" \"GSM1551331_Normal-21_U133_Plus2.CEL.gz\"\n##  [23] \"GSM1551332_Normal-22_U133_Plus2.CEL.gz\" \"GSM1551333_Normal-23_U133_Plus2.CEL.gz\"\n##  [25] \"GSM1551334_Normal-24_U133_Plus2.CEL.gz\" \"GSM1551335_CIN1-01_U133_Plus2.CEL.gz\"  \n##  [27] \"GSM1551336_CIN1-02_U133_Plus2.CEL.gz\"   \"GSM1551337_CIN1-03_U133_Plus2.CEL.gz\"  \n##  [29] \"GSM1551338_CIN1-04_U133_Plus2.CEL.gz\"   \"GSM1551339_CIN1-05_U133_Plus2.CEL.gz\"  \n##  [31] \"GSM1551340_CIN1-06_U133_Plus2.CEL.gz\"   \"GSM1551341_CIN1-07_U133_Plus2.CEL.gz\"  \n##  [33] \"GSM1551342_CIN1-08_U133_Plus2.CEL.gz\"   \"GSM1551343_CIN1-09_U133_Plus2.CEL.gz\"  \n##  [35] \"GSM1551344_CIN1-10_U133_Plus2.CEL.gz\"   \"GSM1551345_CIN1-11_U133_Plus2.CEL.gz\"  \n##  [37] \"GSM1551346_CIN1-12_U133_Plus2.CEL.gz\"   \"GSM1551347_CIN1-13_U133_Plus2.CEL.gz\"  \n##  [39] \"GSM1551348_CIN1-14_U133_Plus2.CEL.gz\"   \"GSM1551349_CIN2-01_U133_Plus2.CEL.gz\"  \n##  [41] \"GSM1551350_CIN2-02_U133_Plus2.CEL.gz\"   \"GSM1551351_CIN2-03_U133_Plus2.CEL.gz\"  \n##  [43] \"GSM1551352_CIN2-04_U133_Plus2.CEL.gz\"   \"GSM1551353_CIN2-05_U133_Plus2.CEL.gz\"  \n##  [45] \"GSM1551354_CIN2-06_U133_Plus2.CEL.gz\"   \"GSM1551355_CIN2-07_U133_Plus2.CEL.gz\"  \n##  [47] \"GSM1551356_CIN2-08_U133_Plus2.CEL.gz\"   \"GSM1551357_CIN2-09_U133_Plus2.CEL.gz\"  \n##  [49] \"GSM1551358_CIN2-10_U133_Plus2.CEL.gz\"   \"GSM1551359_CIN2-11_U133_Plus2.CEL.gz\"  \n##  [51] \"GSM1551360_CIN2-12_U133_Plus2.CEL.gz\"   \"GSM1551361_CIN2-13_U133_Plus2.CEL.gz\"  \n##  [53] \"GSM1551362_CIN2-14_U133_Plus2.CEL.gz\"   \"GSM1551363_CIN2-15_U133_Plus2.CEL.gz\"  \n##  [55] \"GSM1551364_CIN2-16_U133_Plus2.CEL.gz\"   \"GSM1551365_CIN2-17_U133_Plus2.CEL.gz\"  \n##  [57] \"GSM1551366_CIN2-18_U133_Plus2.CEL.gz\"   \"GSM1551367_CIN2-19_U133_Plus2.CEL.gz\"  \n##  [59] \"GSM1551368_CIN2-20_U133_Plus2.CEL.gz\"   \"GSM1551369_CIN2-21_U133_Plus2.CEL.gz\"  \n##  [61] \"GSM1551370_CIN2-22_U133_Plus2.CEL.gz\"   \"GSM1551371_CIN3-01_U133_Plus2.CEL.gz\"  \n##  [63] \"GSM1551372_CIN3-02_U133_Plus2.CEL.gz\"   \"GSM1551373_CIN3-03_U133_Plus2.CEL.gz\"  \n##  [65] \"GSM1551374_CIN3-04_U133_Plus2.CEL.gz\"   \"GSM1551375_CIN3-05_U133_Plus2.CEL.gz\"  \n##  [67] \"GSM1551376_CIN3-06_U133_Plus2.CEL.gz\"   \"GSM1551377_CIN3-07_U133_Plus2.CEL.gz\"  \n##  [69] \"GSM1551378_CIN3-08_U133_Plus2.CEL.gz\"   \"GSM1551379_CIN3-09_U133_Plus2.CEL.gz\"  \n##  [71] \"GSM1551380_CIN3-10_U133_Plus2.CEL.gz\"   \"GSM1551381_CIN3-11_U133_Plus2.CEL.gz\"  \n##  [73] \"GSM1551382_CIN3-12_U133_Plus2.CEL.gz\"   \"GSM1551383_CIN3-13_U133_Plus2.CEL.gz\"  \n##  [75] \"GSM1551384_CIN3-14_U133_Plus2.CEL.gz\"   \"GSM1551385_CIN3-15_U133_Plus2.CEL.gz\"  \n##  [77] \"GSM1551386_CIN3-16_U133_Plus2.CEL.gz\"   \"GSM1551387_CIN3-17_U133_Plus2.CEL.gz\"  \n##  [79] \"GSM1551388_CIN3-18_U133_Plus2.CEL.gz\"   \"GSM1551389_CIN3-19_U133_Plus2.CEL.gz\"  \n##  [81] \"GSM1551390_CIN3-20_U133_Plus2.CEL.gz\"   \"GSM1551391_CIN3-21_U133_Plus2.CEL.gz\"  \n##  [83] \"GSM1551392_CIN3-22_U133_Plus2.CEL.gz\"   \"GSM1551393_CIN3-23_U133_Plus2.CEL.gz\"  \n##  [85] \"GSM1551394_CIN3-24_U133_Plus2.CEL.gz\"   \"GSM1551395_CIN3-25_U133_Plus2.CEL.gz\"  \n##  [87] \"GSM1551396_CIN3-26_U133_Plus2.CEL.gz\"   \"GSM1551397_CIN3-27_U133_Plus2.CEL.gz\"  \n##  [89] \"GSM1551398_CIN3-28_U133_Plus2.CEL.gz\"   \"GSM1551399_CIN3-29_U133_Plus2.CEL.gz\"  \n##  [91] \"GSM1551400_CIN3-30_U133_Plus2.CEL.gz\"   \"GSM1551401_CIN3-31_U133_Plus2.CEL.gz\"  \n##  [93] \"GSM1551402_CIN3-32_U133_Plus2.CEL.gz\"   \"GSM1551403_CIN3-33_U133_Plus2.CEL.gz\"  \n##  [95] \"GSM1551404_CIN3-34_U133_Plus2.CEL.gz\"   \"GSM1551405_CIN3-35_U133_Plus2.CEL.gz\"  \n##  [97] \"GSM1551406_CIN3-36_U133_Plus2.CEL.gz\"   \"GSM1551407_CIN3-37_U133_Plus2.CEL.gz\"  \n##  [99] \"GSM1551408_CIN3-38_U133_Plus2.CEL.gz\"   \"GSM1551409_CIN3-39_U133_Plus2.CEL.gz\"  \n## [101] \"GSM1551410_CIN3-40_U133_Plus2.CEL.gz\"   \"GSM1551411_Cancer-01_U133_Plus2.CEL.gz\"\n## [103] \"GSM1551412_Cancer-02_U133_Plus2.CEL.gz\" \"GSM1551413_Cancer-03_U133_Plus2.CEL.gz\"\n## [105] \"GSM1551414_Cancer-04_U133_Plus2.CEL.gz\" \"GSM1551415_Cancer-05_U133_Plus2.CEL.gz\"\n## [107] \"GSM1551416_Cancer-06_U133_Plus2.CEL.gz\" \"GSM1551417_Cancer-07_U133_Plus2.CEL.gz\"\n## [109] \"GSM1551418_Cancer-08_U133_Plus2.CEL.gz\" \"GSM1551419_Cancer-09_U133_Plus2.CEL.gz\"\n## [111] \"GSM1551420_Cancer-10_U133_Plus2.CEL.gz\" \"GSM1551421_Cancer-11_U133_Plus2.CEL.gz\"\n## [113] \"GSM1551422_Cancer-12_U133_Plus2.CEL.gz\" \"GSM1551423_Cancer-13_U133_Plus2.CEL.gz\"\n## [115] \"GSM1551424_Cancer-14_U133_Plus2.CEL.gz\" \"GSM1551425_Cancer-15_U133_Plus2.CEL.gz\"\n## [117] \"GSM1551426_Cancer-16_U133_Plus2.CEL.gz\" \"GSM1551427_Cancer-17_U133_Plus2.CEL.gz\"\n## [119] \"GSM1551428_Cancer-18_U133_Plus2.CEL.gz\" \"GSM1551429_Cancer-19_U133_Plus2.CEL.gz\"\n## [121] \"GSM1551430_Cancer-20_U133_Plus2.CEL.gz\" \"GSM1551431_Cancer-21_U133_Plus2.CEL.gz\"\n## [123] \"GSM1551432_Cancer-22_U133_Plus2.CEL.gz\" \"GSM1551433_Cancer-23_U133_Plus2.CEL.gz\"\n## [125] \"GSM1551434_Cancer-24_U133_Plus2.CEL.gz\" \"GSM1551435_Cancer-25_U133_Plus2.CEL.gz\"\n## [127] \"GSM1551436_Cancer-26_U133_Plus2.CEL.gz\" \"GSM1551437_Cancer-27_U133_Plus2.CEL.gz\"\n## [129] \"GSM1551438_Cancer-28_U133_Plus2.CEL.gz\"\nhead(GSE63514_meta, 10)\nhgu133plus2_genenames <- read_csv(\"Resource/hgu133plus2_genenames.csv\") |> \n  select(-1) # remove row number\nhead(hgu133plus2_genenames, 10)"},{"path":"GSE63514.html","id":"cleaning-data","chapter":"8 Example: Microarray data visualization","heading":"8.2 Cleaning data","text":"สมมติว่าในตัวอย่างนี้ จะทำการวิเคราะห์แค่ระหว่าง Normal, CIN1, และ Cancer เท่านั้น จึงจำเป็นที่จะต้องกรองข้อมูลที่ไม่ต้องการออกไปเสียก่อนเมื่อดูในตัวแปร exps_nc จะพบว่า probe นั้นเป็นชื่อเฉพาะของตัวเครื่อง ไม่ใช้ชื่อสากล ในที่นี้จะทำการเปลี่ยน Probe ให้เป็นชื่อยีนนั้นๆ แต่ว่าชื่อ List รายชื่อนั้นเป็นชื่อทั้งหมดของ Probe สังเกตได้จากจำนวนแถวที่ไม่เท่ากันในที่นี้ การใช้คำสั่ง *_join() จะทำให้สามารถรวมแค่แถวที่ต้องการได้","code":"\nexprs_nc <- GSE63514_20000 |> select(probe, contains(c(\"Normal\", \"CIN1\", \"Cancer\")))\nmeta_nc <- GSE63514_meta |> \n  filter(grepl(\"Normal|CIN1|Cancer\", title)) |> \n  select(title, `characteristics_ch1.1`, `dissection:ch1`)\nnames(exprs_nc) <- c(\"prob\", meta_nc$title) # เปลี่ยนชื่อให้อ่านง่าย\n\nhead(exprs_nc, 10)\nhead(meta_nc, 10)\nnrow(exprs_nc)## [1] 2000\nnrow(hgu133plus2_genenames)## [1] 54675\ngene_nc <- exprs_nc |> \n            left_join(hgu133plus2_genenames, by = c(\"prob\"=\"PROBEID\")) |> \n            relocate(c(\"SYMBOL\", \"ENTREZID\", \"GENENAME\"), .after = \"prob\")"},{"path":"GSE63514.html","id":"top10_boxplot","chapter":"8 Example: Microarray data visualization","heading":"8.3 Visualization","text":"ต่อไป จะทำการแสดงผล Gene expression 10 ตัวที่มีการแสดงออกมากที่สุดในทั้งการทดลองนี้ขั้นแรก เราจะทำการรวม Intensity ทั้งหมดใน 1 gene ผ่าน function rowSums() และเรียง total_intensity จากมากไปน้อย หลังจากนั้นเลือก Top 10 intensity ออกมาโดยใช้ function head()จะเห็นว่าข้อมูลของเรา อยู่ในลักษณะ wide form ในการสร้าง boxplot นั้น ข้อมูลจำเป็นต้องอยู่ในลักษณะ long form เราจะใช้ function pivot_longer()ท่านอาจจะอยากเพิ่มเส้นค่าเฉลี่ยเพื่อดูว่า Global mean intensity เป็นเท่าไรตัวกลุ่ม RPL ดูน่าสนใน เนื่องจาก Intensity ใน cancer ต่ำกว่า global mean แต่จะมีนัยสำคัญหรือไม่ต้องใช้การวิเคราะห์ทางสถิติเพิ่มเติมทีหลังต่อไป เราอยากที่จะแบ่งว่า tissue ที่เป็น whole section กับ laser captured มีการแสดงออกที่แตกต่างกันอย่างไร เราจะใช้ facet_wrap() เข้ามาช่วยพล็อตอัตราส่วนจำนวนของ laser capture vs whole sectionเห็นว่าผลที่ได้ประหลาด เนื่องจาก laser capturedและ laser-captured เป็นตัวแปรซ้ำ ต้องแก้ไขเสียก่อนเมื่อข้อมูลที่ได้ถูกต้อง จะเห็นว่า มีเฉพาะกลุ่ม cancer เท่านั้น ที่มีการตัดแบบ whole sectionหลังจากนั้นเราจะทำการพล็อต Intensity ในแต่ละยีน","code":"\ngene_symbol_mat <- gene_nc |> \n  select(-prob, -ENTREZID, -GENENAME) |> \n  mutate(total_intensity = \n           rowSums(select(gene_nc, -prob, -ENTREZID, -GENENAME, -SYMBOL)),\n         .before = \"Normal-01\") |> \n  arrange(desc(total_intensity)) \n\ntop10_intensity <- head(gene_symbol_mat,10)\ntop10_intensity\ntop10_long <- top10_intensity |> \n              select(-total_intensity) |> \n              pivot_longer(-SYMBOL, names_to = \"Case\", values_to = \"Intensity\") |> \n              separate(Case, into = c(\"Group\", \"Number\"), sep = \"-\", remove = FALSE)\nggplot(top10_long, aes(x = SYMBOL, y = Intensity, fill = Group)) + \n  geom_boxplot() +\n  theme_bw() +\n  theme(axis.text.x = \n          element_text(angle = 45, vjust = 1, hjust=1)) # หมุนแกน x เพื่อความสวยงาม\nglobal_mean_intensity <- mean(top10_long$Intensity)\nglobal_mean_intensity## [1] 12.99896\nggplot(top10_long, aes(x = SYMBOL, y = Intensity, fill = Group)) + \n  geom_boxplot() +\n  geom_hline(yintercept = global_mean_intensity, linetype = \"dashed\", color = \"darkviolet\", linewidth = 1) +\n  theme_bw() +\n  theme(axis.text.x = \n          element_text(angle = 45, vjust = 1, hjust=1))\ntop10_long_dissec <- top10_long |> \n                      left_join(select(meta_nc, title, `dissection:ch1`), \n                                by = c(\"Case\" = \"title\"))\ntop10_long_dissec\nggplot(top10_long_dissec, aes(x = `dissection:ch1`, fill = Group)) + \n  geom_bar(col = \"black\", width = 0.5, position = \"dodge\") + \n  theme_bw()\ntop10_long_dissec <- top10_long_dissec |> \n                      mutate(`dissection:ch1` = gsub(\"-\", \" \", `dissection:ch1`))\n\nggplot(top10_long_dissec, aes(x = `dissection:ch1`, fill = Group)) + \n  geom_bar(col = \"black\", width = 0.5, position = \"dodge\") + \n  theme_bw()\nggplot(top10_long_dissec, aes(x = SYMBOL, y = Intensity, fill = Group)) + \n  geom_boxplot() +\n  facet_wrap(~`dissection:ch1`) +\n  theme_bw() +\n  theme(axis.text.x = \n          element_text(angle = 45, vjust = 1, hjust=1))"},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"9 Hypothesis testing","heading":"9 Hypothesis testing","text":"","code":""},{"path":"hypothesis-testing.html","id":"principle","chapter":"9 Hypothesis testing","heading":"9.1 Principle","text":"การทดสอบสมมติฐาน คือ การใช้วิธีทางสถิติในตอบคำถามสมมติฐานที่ต้องการ โดยมีขั้นตอน คือ","code":""},{"path":"hypothesis-testing.html","id":"สรางสมมตฐาน-construct-the-hypothesis","chapter":"9 Hypothesis testing","heading":"9.1.1 สร้างสมมติฐาน (Construct the hypothesis)","text":"สมมติฐานว่าง (Null hypothesis; \\(H_{0}\\)) คือสมมติฐานที่ต้องการทดสอบ ซึ่งเปรียบเทียบได้กับสิ่งที่ทุกคนมีความเชื่อเดิมอยู่แล้ว (Default belief) หรือไม่ทราบแน่ชัดว่าเป็นอย่างไร ซึ่งจะเป็นแนวปฏิเสธไว้ก่อน คือ ไม่มีความแตกต่างกันสมมติฐานทางเลือก (Alternative hypothesis; \\(H_{}\\)) คือสมมติฐานที่คาดหวังว่าจะเป็นการค้นพบใหม่","code":""},{"path":"hypothesis-testing.html","id":"ทำการวเคราะหคาทางสถต-construct-the-test-statistics","chapter":"9 Hypothesis testing","heading":"9.1.2 ทำการวิเคราะห์ค่าทางสถิติ (Construct the test statistics)","text":"โดยเครื่องมือจะมีหลายรูปแบบตามลักษณะของข้อมูล เช่น t-test, Chi-square เป็นต้น ซึ่งผลลัพธ์จากการหาค่าทางสถิติจะเป็นไปตามการทดสอบนั้นๆ เช่น \\(t\\)-test: \\(t\\), Chi-square: \\(X^{2}\\), \\(F\\)-test: \\(F\\) เป็นต้น","code":""},{"path":"hypothesis-testing.html","id":"หา-p-value","chapter":"9 Hypothesis testing","heading":"9.1.3 หา \\(p\\)-value","text":"คือ โอกาสที่สามารถสังเกตค่าทางสถิติที่มากกว่าค่าการวิเคราะห์ทางสถิติที่โดยไม่ได้อยู่ภายใต้ \\(H_{0}\\) โดยทางปฏิบัติแล้วคือ การสร้างแบบจำลองข้อมูลภายใต้ \\(H_{0}\\) ขึ้นมาแล้วแล้วดูว่า ที่ค่าสถิตินั้น มีโอกาสไม่เกิดเท่าไรยกตัวอย่าง เมื่อวิเคราะห์ \\(t\\)-test จะทำการสร้าง t-distribution ขึ้นมา (ตัวอย่างของการวิเคราะห์ อยู่บทถัดไป ) ภายใต้ \\(H_{0}\\) ว่า mean = 10ที่ -0.7345 < \\(t\\) < 0.7345 (พื้นที่ใต้กราฟสีขาว) หมายถึงโอกาสที่ค่านั้นเกิดจากความบังเอิญภายใต้ \\(H_{0}\\) ที่ยังเป็นจริง คิดเป็น AUC ได้ที่ \\(t\\) < -0.7345 หรือ \\(t\\) > 0.7345 (พื้นที่ใต้กราฟสีฟ้า) หมายถึงโอกาสที่ค่านั้นไม่ได้เกิดจากความบังเอิญภายใต้ \\(H_{0}\\) ที่ยังเป็นจริง (extreme value) คิดเป็น AUC ได้โดย AUC_blue = \\(p\\)-value = พื้นที่ใต้กราฟสีฟ้า = ~46.4%","code":"\nlibrary(tidyverse)\nt_dist <- dt(seq(-5,5,0.01), df = 99) # t-distribution มี mean = 0 sd = 1\n\ntval <- 0.7345 # t-value วิธีคำนวณอยู่ในบท t-test\n\ndensity_df <- data.frame(x = seq(-5,5,0.01), y = t_dist) |>\n  mutate(area = ifelse(between(x,-tval,tval), TRUE,FALSE))\n\nggplot(density_df, aes(x,y)) + \n  geom_area(fill = \"skyblue\", color = \"black\") +\n  geom_area(data = filter(density_df, area), fill = \"white\", color = \"black\") +\n  geom_vline(xintercept = c(-tval,tval), linetype = \"dashed\") +\n  theme_bw()\ndensity_df |> \n  filter(area) |> \n  summarize(AUC_white = sum(y)) |> pull()## [1] 53.59248\ndensity_df |> \n  filter(!area) |> \n  summarize(AUC_blue = sum(y)) |> pull()## [1] 46.40728\np <- 2*pt(0.7345, 99, lower.tail = FALSE) \np # p-value## [1] 0.4643803"},{"path":"hypothesis-testing.html","id":"p-crit","chapter":"9 Hypothesis testing","heading":"9.1.4 เปรียบเทียบ \\(p\\)-value กับ ค่าวิกฤติ (Critical value) ที่ยอมรับได้","text":"จุดนี้จะเป็นการตัดว่า ท่านสามารถยอมรับความบังเอิญนี้ที่กี่ % โดยทั่วไปมักใช้ที่น้อยกว่า 5% (0.05) หรือ 1% (0.01) ซึ่งแบ่งเป็นLeft-tailed คือ โอกาสที่ Critical value \\(\\leq H_{0}\\)Left-tailed คือ โอกาสที่ Critical value \\(\\leq H_{0}\\)Right-tailed คือ โอกาสที่ Critical value \\(\\geq H_{0}\\)Right-tailed คือ โอกาสที่ Critical value \\(\\geq H_{0}\\)Two-tailed คือ โอกาสที่ Critical value \\(\\neq H_{0}\\) นิยมใช้วิเคราะห์มากที่สุดTwo-tailed คือ โอกาสที่ Critical value \\(\\neq H_{0}\\) นิยมใช้วิเคราะห์มากที่สุดจะเห็นว่า โอกาสที่ความแตกต่างนั้นไม่ได้เกิดจากความบังเอิญภายใต้ \\(H_{0}\\) นั้นอยู่ที่ 46% ซึ่งมากกว่า 5% ที่ต้องการ จึงไม่สามารถปฏิเสธ \\(H_{0}\\) ได้ เรียกอีกแบบหนึ่งว่า ไม่ได้แตกต่างอย่างมีนัยสำคัญ (จนสามารถปฏิเสธ \\(H_{0}\\) ได้)ตรงส่วนพื้นที่ระหว่าง Critical value (ระหว่างเส้นประสีแดง) คือ พิสัยของความแตกต่างที่สามารถรับได้ ซึ่งจะถูกนำไปใช้ในการคำนวณค่าความเชื่อมั่น (Confidence interval) ต่อไป","code":"\ncrit <- qt(1-0.05/2, df = 99) # ตัดที่ <= p 0.05, two-tailed\n\ndensity_df <- density_df |> \n  mutate(crits = ifelse(between(x,-crit,crit), TRUE,FALSE))\n\nggplot(density_df, aes(x,y)) + \n  geom_area(fill = \"darkred\", color = \"black\") +\n  geom_area(data = filter(density_df, crits), fill = \"skyblue\", color = \"black\") + \n  geom_area(data = filter(density_df, area), fill = \"white\", color = \"black\") +\n  geom_vline(xintercept = c(-tval,tval), linetype = \"dashed\") +\n  geom_vline(xintercept = c(-crit,crit), linetype = \"dashed\", color = \"red\") +\n  theme_bw()"},{"path":"hypothesis-testing.html","id":"power","chapter":"9 Hypothesis testing","heading":"Power","text":"คือ โอกาสการเกิดผลบวกจริง (True positive) ของ \\(H_{}\\) = 1 - Powerพิจารณาการคำนวณทางสถิติ t-test ภายใต้สมมติฐาน \\(H_{0}\\): \\(\\mu = 12\\) และ \\(H_{}\\): \\(\\mu > 12\\) ที่ \\(S = 1.7\\)จากการคำนวณ สามารถสรุปได้ว่าสามารถปฏิเสธ \\(H_{0}\\) นั่นคือ ข้อมูลของกลุ่มตัวอย่างนี้มีค่าเฉลี่ยมากกว่าจาก 12 อย่างมีนัยสำคัญ แต่ว่า \\(H_{}\\) นั้น มีโอกาสเป็นจริงหรือไม่นั้น เมื่อพิจารณาของกระจายตัวของข้อมูล t-distribution แล้วจากกราฟ เส้นประสีแดงคือการกระจายตัวของข้อมูล \\(H_{0}\\): \\(\\mu = 12\\) ส่วนเส้นประสีฟ้าคือ \\(H_{}\\): \\(\\mu = 12 + \\frac{1.379}{4}\\times2.156 = 12.743\\) พื้นที่สีแดงคือ Critical value = \\(\\alpha\\) ที่ 0.05ดังนั้น พื้นที่สีฟ้าคือ พื้นที่ของกลุ่มตัวอย่างจาก \\(H_{}\\): \\(\\mu = 12.743\\) ซึ่งแท้จริงแล้ว สามารถอยู่ในกลุ่ม \\(H_{0}\\): \\(\\mu = 12\\) ได้เช่นกัน ซึ่งก็คือโอกาสการเกิดผลบวกลวง (False positive) = Type-II error นั้นเองส่วนตรงที่เป็นเส้นแนวเฉียงสีฟ้า คือพื้นที่ๆ ไม่มีทางมาจาก \\(H_{0}\\) ได้ ซึ่งก็คือ ความสามารถในการแยกแยะกลุ่มตัวอย่างที่เป็นความจริง เรียกว่า ผลบวกที่แท้จริง (True positive) = Power = 1 - Type-II error ดังนั้น ถ้าท่านเพิ่มค่า \\(\\alpha\\) สูงสุดที่รับได้ โอกาสการเกิด False positive ก็จะเพิ่มขึ้น แต่โอกาสการเกิด False negative ก็จะต่ำลง (= Power สูงขึ้น)อย่างไรก็ตาม ในด้านงานวิจัยนั้น ส่วนใหญ่จะให้ความสำคัญกับ False positive มากกว่า เนื่องจากผลบวกลวงอาจจะส่งผลให้ตัดสินใจเปลี่ยนแปลงการรักษาเดิมที่เป็นมาตรฐาน ซึ่งอาจเกิดผลเสียแก่ผู้ป่วยได้ ดังนั้น จึงไม่ค่อยมีการปรับเพิ่ม \\(\\alpha\\) แต่จะเลือกเพิ่ม Power จากการเพิ่ม Sample size มากกว่า โดยจำนวนตัวอย่างที่มากขึ้น ส่งผลให้ความแปรปรวนในการวัด (Standard error) ลดลง ส่งผลให้การกระจายตัวของข้อมูลนั้นแคบลงด้วยจากกราฟ เส้นประสีแดงคือการกระจายตัวของข้อมูล \\(H_{0}\\): \\(\\mu = 12\\) ส่วนเส้นประสีฟ้าคือ \\(H_{}\\): \\(\\mu = 12 + \\frac{1.654}{19}\\times3.356 = 12.29\\) พื้นที่สีแดงคือ Critical value = \\(\\alpha\\) ที่ 0.05 จะเห็นว่าการกระจายตัวของข้อมูลทั้งสองกลุ่มนั้นแคบกว่า (สังเกตตัวเลขแกน x) จึงมีส่วนที่พื้นที่ร่วมกันที่น้อยกว่าด้วยเมื่อพล็อตกราฟความสัมพันธ์ระหว่าง ขนาดตัวอย่าง, \\(\\alpha\\), Power จะได้ดังรูปNote: ทั้งหมดนี้เป็นการคำนวณตัวอย่างของ t-test เท่านั้น การกระจายตัวแบบอื่นจะมีวิธีคำนวณ Power ที่ต่างกันไป อย่างไรก็ตามหลักการพื้นฐานจะคล้ายกัน","code":"\nset.seed(123)\nmean_5_sample <- rnorm(5, mean = 13, sd = 1.7)\nsd(mean_5_sample) #sd## [1] 1.378737\nt_test_5 <- t.test(mean_5_sample, mu = 12, alternative = \"greater\")\nt_test_5## \n##  One Sample t-test\n## \n## data:  mean_5_sample\n## t = 2.1555, df = 4, p-value = 0.04869\n## alternative hypothesis: true mean is greater than 12\n## 95 percent confidence interval:\n##  12.01459      Inf\n## sample estimates:\n## mean of x \n##  13.32907\nlibrary(pwrss)\npower.t.test(ncp = t_test_5$statistic, df = 4, alpha = 0.05,\n             alternative = \"greater\", plot = TRUE, verbose = FALSE)\npower.t.test(ncp = t_test_5$statistic, df = 4, alpha = 0.1,\n             alternative = \"greater\", plot = TRUE, verbose = FALSE)\nset.seed(123)\nmean_20_sample <- rnorm(20, mean = 13, sd = 1.7)\nsd(mean_20_sample) # sd## [1] 1.653531\nsd(mean_20_sample)/sqrt(length(mean_20_sample)) # standard error## [1] 0.3697408\nt_test_20 <- t.test(mean_20_sample, mu = 12, alternative = \"greater\")\nt_test_20## \n##  One Sample t-test\n## \n## data:  mean_20_sample\n## t = 3.3558, df = 19, p-value = 0.00166\n## alternative hypothesis: true mean is greater than 12\n## 95 percent confidence interval:\n##  12.60143      Inf\n## sample estimates:\n## mean of x \n##  13.24076\npower.t.test(ncp = t_test_20$statistic, df = 19, alpha = 0.05,\n             alternative = \"greater\", plot = TRUE, verbose = FALSE) \nmean_diff <- 1 # ความต่างคงที่ 1\nn <- 3:50\nalphas <- c(0.5, 0.2, 0.1,0.05,0.01,0.001)\n\npower_df <- expand_grid(n = n, alphas = alphas) |> \n  mutate(power = stats::power.t.test(n = n, delta = mean_diff, sig.level = alphas, sd = 1)$power) |> \n  mutate(alpha = as.factor(alphas)) |> \n  mutate(alpha = fct_reorder(alpha, desc(alphas)))\n\n\nggplot(power_df, aes(x = n, y = power, col = alpha, linetype = alpha)) + \n  geom_line(linewidth = 1.25) + theme_bw() +\n  labs(y = \"Power (1-beta)\", x = \"Sample size\")"},{"path":"hypothesis-testing.html","id":"tradeoff","chapter":"9 Hypothesis testing","heading":"Trade-off ของ p-value threshold","text":"การตั้งค่าวิกฤตินั้นมี Trade-ระหว่าง False positive และ False negative ที่ต้องพิจารณาType error = False positive = \\(\\alpha\\) = Critical valueType error = False positive = \\(\\alpha\\) = Critical valueType II error = False negative = \\(\\beta\\) = 1 - PowerType II error = False negative = \\(\\beta\\) = 1 - Power\\(\\therefore\\) Power = True positive","code":""},{"path":"parametric-test.html","id":"parametric-test","chapter":"10 Parametric test","heading":"10 Parametric test","text":"คือ การวิเคราะห์ทางสถิติที่เราทราบลักษณะการกระจายตัวของข้อมูลอย่างชัดเจน แต่มีข้อดีคือการเปรียบเทียบข้อมูลจะมีความแม่นยำสูง","code":""},{"path":"parametric-test.html","id":"sec-t-test","chapter":"10 Parametric test","heading":"10.1 \\(t\\)-test","text":"คือ การคำนวณทางสถิติที่เปรียบเทียบค่าเฉลี่ย (Mean) และความผันผวนของการวัด (Standard Error; SE) ระหว่างสองกลุ่ม แบ่งเป็น","code":""},{"path":"parametric-test.html","id":"one-t","chapter":"10 Parametric test","heading":"10.1.1 One sample \\(t\\)-test","text":"เป็นการเปรียบเทียบความแตกต่างของค่าเฉลี่ยระหว่างกลุ่มตัวอย่าง (Sample) กับประชากร (Population)\\[\nt = \\frac{\\bar{x} - \\mu_{0}}{SE} = \\frac{\\bar{x} - \\mu_{0}}{s/\\sqrt{n}}\n\\]เพื่อให้เห็นภาพของความต่างในการทดสอบนี้ เราจะทำการสร้างกราฟเปรียบว่ากลุ่มตัวอย่างนั้น มีค่าเฉลี่ยต่างจากประชากร ที่ ค่าเฉลี่ย = 10 และ ค่าเฉลี่ย = 30 หรือไม่จะเห็นว่าเมื่อดูลักษณะการกระจายตัวของข้อมูลแล้ว Samp_10 ที่มี ค่าเฉลี่ย = 10 นั้น ไม่ต่างจากประชากรที่มี ค่าเฉลี่ย = 10 แต่ดูแตกต่างอย่างเห็นได้ชัดกับประชากรที่มี mean = 30 t.test จะช่วยตัดสินว่าค่าที่ได้นั้นแตกต่างกันจริงหรือไม่ โดยสมมติฐานที่ตั้งคือCase 1: ประชากรมีค่าเฉลี่ย = 10\\(H_{0}\\): ค่าเฉลี่ยของกลุ่มตัวอย่างนั้น ไม่ต่างจากค่าเฉลี่ยของประชากร = 10\\(H_{0}\\): ค่าเฉลี่ยของกลุ่มตัวอย่างนั้น ไม่ต่างจากค่าเฉลี่ยของประชากร = 10\\(H_{}\\): ค่าเฉลี่ยของกลุ่มตัวอย่างนั้น ต่างจากค่าเฉลี่ยของประชากร = 10\\(H_{}\\): ค่าเฉลี่ยของกลุ่มตัวอย่างนั้น ต่างจากค่าเฉลี่ยของประชากร = 10Case 2: ประชากรมีค่าเฉลี่ย = 40\\(H_{0}\\): ค่าเฉลี่ยของกลุ่มตัวอย่างนั้น ไม่ต่างจากค่าเฉลี่ยของประชากร = 40\\(H_{0}\\): ค่าเฉลี่ยของกลุ่มตัวอย่างนั้น ไม่ต่างจากค่าเฉลี่ยของประชากร = 40\\(H_{}\\): ค่าเฉลี่ยของกลุ่มตัวอย่างนั้น ต่างจากค่าเฉลี่ยของประชากร = 40\\(H_{}\\): ค่าเฉลี่ยของกลุ่มตัวอย่างนั้น ต่างจากค่าเฉลี่ยของประชากร = 40จะเห็นว่าต่างจากประชากรที่ค่าเฉลี่ย = 30 อย่างมีนัยสำคัญ (\\(p\\) = \\(2.2 \\times 10^{-16}\\) < Critical point = 0.05)พิจารณาที่มาของ \\(p\\)-value นั้นมีที่มาจากสูตรขั้นต้นจะเห็นว่าค่า \\(t\\) นั้นเท่ากับ ค่าที่ได้จาก t.test ขั้นต้น เมื่อสร้าง \\(t\\)-distribution ที่มี \\(H_{0}\\) คือ ค่าเฉลี่ย = 0 แล้วจะพบว่าค่านี้มากกว่า Critical value","code":"\nset.seed(123)\nnorm_10_pop <- rnorm(1000, mean = 10, sd = 4) # สร้างประชากร mean = 10, sd = 4\nnorm_10_sample <- sample(norm_10_pop, 100) # สุ่มตัวอย่างมาจากประชากร 100 ราย\nnorm_30_pop <- rnorm(1000, mean = 30, sd = 4)\nnorm_30_sample <- sample(norm_30_pop, 100)\nnorm_pop_df <- data.frame(Pop_10 = norm_10_pop, Pop_30 = norm_30_pop) |>\n  pivot_longer(everything(), names_to = \"Pop\", values_to = \"Values\")  \n\nnorm_sample_df <- data.frame(Samp_10 = norm_10_sample, Samp_30 = norm_30_sample) |>    pivot_longer(everything(), names_to = \"Samp\", values_to = \"Values\")  \n\nggplot(norm_pop_df, aes(x = Values, fill = Pop)) +    \n  geom_density(aes(y = after_stat(count)),color = \"black\", alpha = 0.5) + \n  geom_density(data = norm_sample_df, \n               aes(x  = Values, y = after_stat(count), fill = Samp), \n               color = \"black\", alpha = 0.7) + theme_bw()\nt.test(norm_10_sample, mu = 10) # p > 0.05## \n##  One Sample t-test\n## \n## data:  norm_10_sample\n## t = 0.73547, df = 99, p-value = 0.4638\n## alternative hypothesis: true mean is not equal to 10\n## 95 percent confidence interval:\n##   9.533936 11.015052\n## sample estimates:\n## mean of x \n##  10.27449\nt.test(norm_10_sample, mu = 30) # p <= 0.05## \n##  One Sample t-test\n## \n## data:  norm_10_sample\n## t = -52.852, df = 99, p-value < 2.2e-16\n## alternative hypothesis: true mean is not equal to 30\n## 95 percent confidence interval:\n##   9.533936 11.015052\n## sample estimates:\n## mean of x \n##  10.27449\nhypothesized_mean <- 30\nmean_sample <- mean(norm_10_sample)\nsd_sample <- sd(norm_10_sample)\n\nt <- (mean_sample - hypothesized_mean)/(sd_sample/sqrt(length(norm_10_sample)))\n\nt## [1] -52.85159\n2*pt(t, df = 99) # p-value## [1] 2.306252e-74"},{"path":"parametric-test.html","id":"ind-t","chapter":"10 Parametric test","heading":"10.1.2 Independent t-test","text":"เป็นการเปรียบเทียบค่าเฉลี่ยระหว่าง ตัวอย่างสองกลุ่มที่ไม่เกี่ยวเนื่องกัน\\[\nt = \\frac{\\bar{x_{1}}-\\bar{x_{2}}-\\mu_{0}}{\\sqrt{\\frac{s^{2}_1}{n_{1}}+\\frac{s^{2}_2}{n_{2}}}}\n\\]\\[\ns_{p}^{2} = \\frac{(n_{1}-1)s^{2}_{1}+(n_{2}-1)s^{2}_{2}}{{n_{1}} + n_{2} - 2}\n\\]กลับไปดูภาพขั้นต้น ครั้งนี้จะเทียบระหว่างกลุ่มตัวอย่างสองกลุ่ม คือ norm_10_sample และ norm_30_sample ว่ามี ค่าเฉลี่ยที่แตกต่างกันอย่างมีนัยสำคัญหรือไม่\\(H_{0}\\): ค่าเฉลี่ยของ norm_10_sample และ norm_30_sample นั้นไม่แตกต่างกัน (\\(\\bar{x_{1}} - \\bar{x_{2}} = 0\\))\\(H_{0}\\): ค่าเฉลี่ยของ norm_10_sample และ norm_30_sample นั้นไม่แตกต่างกัน (\\(\\bar{x_{1}} - \\bar{x_{2}} = 0\\))\\(H_{}\\): ค่าเฉลี่ยของ norm_10_sample และ norm_30_sample นั้นแตกต่างกัน (\\(\\bar{x_{1}} - \\bar{x_{2}} \\neq 0\\))\\(H_{}\\): ค่าเฉลี่ยของ norm_10_sample และ norm_30_sample นั้นแตกต่างกัน (\\(\\bar{x_{1}} - \\bar{x_{2}} \\neq 0\\))สรุปได้ว่าค่าเฉลี่ยของกลุ่มตัวอย่างทั้งสองกลุ่มนั้นแตกต่างกันอย่างมีนัยสำคัญปล. บางครั้งข้อมูลอาจจะมีความแปรปรวนไม่เท่ากัน ในที่นี้มักจะใช้ Welch’s \\(t\\)-test โดย t.test(…, var.equal = FALSE) โดยสมการนี้จะทำการปรับความแปรปรวนให้ด้วย","code":"\nt.test(norm_10_sample, norm_30_sample, var.equal = TRUE)## \n##  Two Sample t-test\n## \n## data:  norm_10_sample and norm_30_sample\n## t = -38.469, df = 198, p-value < 2.2e-16\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -20.93623 -18.89440\n## sample estimates:\n## mean of x mean of y \n##  10.27449  30.18981"},{"path":"parametric-test.html","id":"pair-t","chapter":"10 Parametric test","heading":"10.1.3 Paired t-test","text":"เป็นการเปรียบเทียบค่าเฉลี่ยระหว่างตัวอย่างสองกลุ่มที่เกี่ยวเนื่องกัน (ก่อน-หลัง แม่-ลูก เป็นต้น)\\[\nt = \\frac{\\bar{x}_{d}-\\mu_{0}}{s_{x_{d}}/\\sqrt{n}}\n\\]\\[\nx_{di} = x_{1_{}}-x_{2_{}}\n\\]\\[\ns_{x_{d}} = \\sqrt{\\frac{\\sum_{d=1}^{n}(x_{d}-\\bar{x}_{d})}{(n-1)}}\n\\]จะเห็นว่ามีความแตกต่างในส่วนของ \\(DF\\) (Degree freedom) ซึ่งเกิดจากการจับคู่หาความต่าง 100 ครั้ง เนื่องจากค่าที่เปรียบเทียบนั้นอยู่ในตัวอย่างเดียวกัน (แม่ลูก คู่ที่ 1 แม่ลูกคู่ที่ 2 … เป็นต้น) เมื่อเปรียบเทียบกับ Independent \\(t\\)-test เนื่องจากเป็นการหา mean ในกลุ่มของตัวเอง 2 ครั้ง และมาเทียบความต่างกันดังนั้น การเลือก Paired vs Independent นั้นมีความสำคัญ ขึ้นอยู่กับโจทย์การศึกษาของท่านด้วย","code":"\nt.test(norm_10_sample, norm_30_sample, paired = TRUE)## \n##  Paired t-test\n## \n## data:  norm_10_sample and norm_30_sample\n## t = -37.802, df = 99, p-value < 2.2e-16\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  -20.96067 -18.86996\n## sample estimates:\n## mean difference \n##       -19.91532"},{"path":"parametric-test.html","id":"ANOVA","chapter":"10 Parametric test","heading":"10.2 Analysis of Variance (ANOVA)","text":"คือ การคำนวณทางสถิติที่เปรียบเทียบค่าเฉลี่ยและความแปรปรวนระหว่างสองกลุ่มขึ้นไป โดยมีสมมติฐาน คือ\\(H_{0}\\): ค่าเฉลี่ยของทุกกลุ่มเท่ากัน (\\(\\bar{x_{1}} = \\bar{x_{2}} = \\bar{x_{3}} = … = \\bar{x_{n}}\\))\\(H_{0}\\): ค่าเฉลี่ยของทุกกลุ่มเท่ากัน (\\(\\bar{x_{1}} = \\bar{x_{2}} = \\bar{x_{3}} = … = \\bar{x_{n}}\\))\\(H_{}\\): ค่าเฉลี่ยของกลุ่มใดกลุ่มหนึ่งต่างจากกลุ่มอื่น\\(H_{}\\): ค่าเฉลี่ยของกลุ่มใดกลุ่มหนึ่งต่างจากกลุ่มอื่นการวิเคราะห์ ANOVA นั้นมีหลายแบบOne-way ANOVA เป็นการหาความแตกต่างของ 1 ตัวแปรOne-way ANOVA เป็นการหาความแตกต่างของ 1 ตัวแปรTwo-way ANOVA เป็นการหาความแตกต่างของ 2 ตัวแปรTwo-way ANOVA เป็นการหาความแตกต่างของ 2 ตัวแปรMANOVA เป็นการหาความแตกต่างที่มากกว่า 2 ตัวแปรMANOVA เป็นการหาความแตกต่างที่มากกว่า 2 ตัวแปรNested ANOVA เป็นการหาความแตกต่างที่ใน 1 ตัวแปรนั้น มีตัวแปรย่อยอีกNested ANOVA เป็นการหาความแตกต่างที่ใน 1 ตัวแปรนั้น มีตัวแปรย่อยอีกณ ที่นี้จะกล่าวถึงแค่ one ANOVA ซึ่งมีความซับซ้อนน้อย และนิยมใช้มากที่สุด โดยหลักการโดยง่ายของ ANOVA คือ การเปรียบเทียบความต่างของ ค่าเฉลี่ยทั้งกลุ่ม (Global mean) เปรียบเทียบกับ ผลรวมของค่าเฉลี่ยแต่ละกลุ่ม (group mean)การวิเคราะห์ทางสถิติของ ANOVA นั้นใช้ \\(F\\)-test ซึ่งเป็นการเปรียบเทียบระหว่าง ความแปรปรวนที่อธิบายได้ และความแปรปรวนที่อธิบายไม่ได้\\[F^{*} = \\frac{\\text{Explained variance}}{\\text{Unexplained variance}} =  \\frac{\\text{group variance}}{\\text{Within groups variance}}\\]group variance คือ ความแปรปรวนของค่าเฉลี่ยแต่ละกลุ่มกับค่าเฉลี่ยทั้งหมดBetween group variance คือ ความแปรปรวนของค่าเฉลี่ยแต่ละกลุ่มกับค่าเฉลี่ยทั้งหมดWithin group variance คือ ความแปรปรวนของข้อมูลในกลุ่มนั้น ซึ่งไม่สามารถอธิบายได้ภายใต้สมมติฐานงานวิจัยนั้นWithin group variance คือ ความแปรปรวนของข้อมูลในกลุ่มนั้น ซึ่งไม่สามารถอธิบายได้ภายใต้สมมติฐานงานวิจัยนั้นอธิบายโดยใช้ตัวอย่าง iris ในที่นี้เราจะเปรียบเทียบตวามต่างของ Sepal.Width ในแต่ละ Speciesอธิบายด้วยภาพรูปสามเหลี่ยม คือ ค่าเฉลี่ยของ Petal.Width ในดอกไม้แต่ละ Speciesรูปสามเหลี่ยม คือ ค่าเฉลี่ยของ Petal.Width ในดอกไม้แต่ละ Speciesจุดสีดำ คือ ค่า Petal.Width ในดอกไม้แต่ละดอกจุดสีดำ คือ ค่า Petal.Width ในดอกไม้แต่ละดอกจุดสีเขียว คือ Global mean (Grand mean) คือ ค่าเฉลี่ย Petal.Width เมื่อรวมดอกไม้ทุก Speciesจุดสีเขียว คือ Global mean (Grand mean) คือ ค่าเฉลี่ย Petal.Width เมื่อรวมดอกไม้ทุก Speciesจุดประสงค์ของ F-test คือการเปรียบเทียบอัตราส่วนระหว่าง ความแปรปรวนของค่าเฉลี่ยแต่ละกลุ่มกับค่าเฉลี่ยทั้งหมด (mean ของความต่างจุดเขียวกับสามเหลี่ยม) ใกล้กันกับความแปรปรวนของข้อมูลในกลุ่มนั้น (mean ของความต่างระหว่างจุดดำกับกับสามเหลี่ยม) หรือไม่ (ratio ~ 1) ซึ่งค่า \\(F\\) นั้นจะถูกนำไปคิด \\(p\\)-value จาก \\(F\\)-distribution (หลักการเดียวกันกับ \\(t\\)-test)สังเกตว่า ท่านไม่สามารถบอกได้ว่ากลุ่มไหนเป็นกลุ่มที่มีค่าเฉลี่ยที่แตกต่างจากทั้งกลุ่ม การที่จะทราบนั้นต้องทำ t.test เปรียบเทียบกันในแต่ละกลุ่ม \\(3\\choose2\\) = 3 ครั้ง การค้นหากลุ่มที่มีความต่างหลัง ANOVA นี้ เรียกว่า Post-hoc test\\(p\\)-value < 0.05 ทุกกลุ่ม หมายความว่า ทุกกลุ่มมีค่าเฉลี่ยของ Petal.Width ที่แตกต่างกัน","code":"\naov(Sepal.Width ~ Species, data = iris) |> summary()##              Df Sum Sq Mean Sq F value Pr(>F)    \n## Species       2  11.35   5.672   49.16 <2e-16 ***\n## Residuals   147  16.96   0.115                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlibrary(granova)\ngranova.1w(iris$Sepal.Width, group = iris$Species)## $grandsum\n##     Grandmean        df.bet       df.with        MS.bet       MS.with        F.stat        F.prob \n##          3.06          2.00        147.00          5.67          0.12         49.16          0.00 \n## SS.bet/SS.tot \n##          0.40 \n## \n## $stats\n##            Size Contrast Coef Wt'd Mean Mean Trim'd Mean Var. St. Dev.\n## versicolor   50         -0.29      2.77 2.77        2.80 0.10     0.31\n## virginica    50         -0.08      2.97 2.97        2.96 0.10     0.32\n## setosa       50          0.37      3.43 3.43        3.41 0.14     0.38\npairwise.t.test(iris$Sepal.Width, iris$Species)## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  iris$Sepal.Width and iris$Species \n## \n##            setosa  versicolor\n## versicolor < 2e-16 -         \n## virginica  9.1e-10 0.0031    \n## \n## P value adjustment method: holm"},{"path":"parametric-test.html","id":"correlation-test","chapter":"10 Parametric test","heading":"10.3 Correlation test","text":"","code":""},{"path":"parametric-test.html","id":"pearson","chapter":"10 Parametric test","heading":"10.3.1 Pearson correlation","text":"เป็นการทดสอบความสัมพันธ์เชิงเส้นของตัวแปรสองตัวแปร\\[\nr_{xy} =\\frac{cov(x,y)}{s(x) s(y)}\n\\]\\[\ncov(x,y) = \\frac{\\sum^{n}_{=1}(x_{}-\\bar{x})(y_{}-\\bar{y})}{n-1}\n\\]\\[\n\\therefore r_{xy} = \\frac{\\sum^{n}_{=1}(x_{}-\\bar{x})(y_{}-\\bar{y})}{\\sqrt{\\sum^{n}_{=1}(x_{}-\\bar{x})^{2}}\\sqrt{\\sum^{n}_{=1}(y_{}-\\bar{y})^{2}}}\n\\]การแปลผลของ Correlation ขึ้นอยู่กับระดับของความสัมพันธ์ = 0-1 ยิ่งเข้าใกล้ 1 ยิ่งสัมพันธ์กันมากระดับของความสัมพันธ์ = 0-1 ยิ่งเข้าใกล้ 1 ยิ่งสัมพันธ์กันมากทิศทางของความสัมพันธ์ = + ไปทิศทางเดียวกัน - ไปทิศทางตรงข้ามกันทิศทางของความสัมพันธ์ = + ไปทิศทางเดียวกัน - ไปทิศทางตรงข้ามกันท่านสามารถสร้างตาราง Correlation และ \\(p\\)-value ได้ด้วย Package corrplotข้อควรระวัง \\(p\\)-value ของ Correlation test นั้นอยู่ภายใต้สมมติฐาน (\\(t\\)-test/\\(F\\)-test)\\(H_{0}\\): ไม่มีความสัมพันธ์เชิงเส้นของทั้งสองตัวแปร (\\(r_{xy} = 0\\))\\(H_{0}\\): ไม่มีความสัมพันธ์เชิงเส้นของทั้งสองตัวแปร (\\(r_{xy} = 0\\))\\(H_{0}\\): มีความสัมพันธ์เชิงเส้นของทั้งสองตัวแปร (\\(r_{xy} \\neq 0\\))\\(H_{0}\\): มีความสัมพันธ์เชิงเส้นของทั้งสองตัวแปร (\\(r_{xy} \\neq 0\\))ซึ่งถ้า \\(p\\) < 0.05 หมายความว่า ท่านมีความมั่นใจมากเพียงพอว่า ความสัมพันธ์เชิงเส้นของทั้งสองตัวแปรนั้น มีมากกว่าการวาดเส้นจากการสร้างจุดแบบสุ่ม ท่านยังคงต้องแปรผลร่วมกับค่า Correlation coefficient ต่อไปเช่น\\(r_{x,y}\\) = 0.2, \\(p\\) < 0.05 มั่นใจว่ามีความสัมพันธ์เชิงเส้นของสองตัวแปร ความสัมพันธ์เป็นไปในทิศทางเดียวกัน แต่ความสัมพันธ์เชิงเส้นอยู่ในระดับน้อย\\(r_{x,y}\\) = 0.2, \\(p\\) < 0.05 มั่นใจว่ามีความสัมพันธ์เชิงเส้นของสองตัวแปร ความสัมพันธ์เป็นไปในทิศทางเดียวกัน แต่ความสัมพันธ์เชิงเส้นอยู่ในระดับน้อย\\(r_{x,y}\\) = 1, \\(p\\) = 1 ความสัมพันธ์เชิงเส้นอยู่ในระดับดีเยี่ยม แต่ไม่มั่นใจว่ามีความสัมพันธ์กันจริงหรือไม่ เนื่องจากข้อมูลน้อย (เช่น มีข้อมูลเพียงสองจุด \\(r_{x,y}\\) ย่อมเท่ากับ 1)\\(r_{x,y}\\) = 1, \\(p\\) = 1 ความสัมพันธ์เชิงเส้นอยู่ในระดับดีเยี่ยม แต่ไม่มั่นใจว่ามีความสัมพันธ์กันจริงหรือไม่ เนื่องจากข้อมูลน้อย (เช่น มีข้อมูลเพียงสองจุด \\(r_{x,y}\\) ย่อมเท่ากับ 1)\\(r_{x,y}\\) = 0.3, \\(p\\) = 1 ความสัมพันธ์เชิงเส้นอยู่ในระดับน้อย แต่ไม่มั่นใจว่ามีความสัมพันธ์กันจริงหรือไม่ เนื่องจากข้อมูลน้อยเกินไป ควรเก็บข้อมูลเพิ่ม\\(r_{x,y}\\) = 0.3, \\(p\\) = 1 ความสัมพันธ์เชิงเส้นอยู่ในระดับน้อย แต่ไม่มั่นใจว่ามีความสัมพันธ์กันจริงหรือไม่ เนื่องจากข้อมูลน้อยเกินไป ควรเก็บข้อมูลเพิ่ม\\(r_{x,y}\\) = -1, \\(p\\) < 0.05 มั่นใจว่ามีความสัมพันธ์เชิงเส้นของสองตัวแปร ความสัมพันธ์เป็นไปในทิศทางตรงข้าม และความสัมพันธ์เชิงเส้นอยู่ในระดับดีเยี่ยม\\(r_{x,y}\\) = -1, \\(p\\) < 0.05 มั่นใจว่ามีความสัมพันธ์เชิงเส้นของสองตัวแปร ความสัมพันธ์เป็นไปในทิศทางตรงข้าม และความสัมพันธ์เชิงเส้นอยู่ในระดับดีเยี่ยมนั่นหมายความว่า ยิ่งจำนวนตัวอย่างเพิ่มขึ้น ความมั่นใจยิ่งเพิ่มขึ้น ไม่ใช่ความสัมพันธ์เพิ่มขึ้น","code":"\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) + \n  geom_point(aes(col = Species)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw()\ncor(iris$Sepal.Length, iris$Petal.Length)## [1] 0.8717538\nlibrary(corrplot)\n\niris_cor <- cor(iris[1:4])\niris_cor##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\niris_cor_p <- cor.mtest(iris[1:4])\niris_cor_p## $p\n##              Sepal.Length  Sepal.Width Petal.Length  Petal.Width\n## Sepal.Length 0.000000e+00 1.518983e-01 1.038667e-47 2.325498e-37\n## Sepal.Width  1.518983e-01 0.000000e+00 4.513314e-08 4.073229e-06\n## Petal.Length 1.038667e-47 4.513314e-08 0.000000e+00 4.675004e-86\n## Petal.Width  2.325498e-37 4.073229e-06 4.675004e-86 0.000000e+00\n## \n## $lowCI\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.2726932    0.8270363   0.7568971\n## Sepal.Width    -0.2726932   1.0000000   -0.5508771  -0.4972130\n## Petal.Length    0.8270363  -0.5508771    1.0000000   0.9490525\n## Petal.Width     0.7568971  -0.4972130    0.9490525   1.0000000\n## \n## $uppCI\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length   1.00000000  0.04351158    0.9055080   0.8648361\n## Sepal.Width    0.04351158  1.00000000   -0.2879499  -0.2186966\n## Petal.Length   0.90550805 -0.28794993    1.0000000   0.9729853\n## Petal.Width    0.86483606 -0.21869663    0.9729853   1.0000000\ncorrplot(iris_cor, method = \"shade\",type = \"upper\",order = \"AOE\", \n         p.mat = iris_cor_p$p, insig = \"p-value\")"},{"path":"non-parametric-test.html","id":"non-parametric-test","chapter":"11 Non-parametric test","heading":"11 Non-parametric test","text":"คือ การวิเคราะห์ทางสถิติที่เราไม่ทราบลักษณะการกระจายตัวของข้อมูลอย่างชัดเจน ข้อดีคือมีความยืดหยุ่นกว่า แต่มีความแม่นยำน้อยกว่า","code":""},{"path":"non-parametric-test.html","id":"proportion-test","chapter":"11 Non-parametric test","heading":"11.1 Proportion test","text":"คือ การทดสอบทางสถิติเพื่อทำการเปรียบเทียบอัตราส่วนของจำนวน โดยจะใช้กับข้อมูลประเภทจำนวนนับของตัวแปรจัดประเภท (Nominal variable) เช่น จำนวนคน จำนวนเซลล์ เป็นต้นข้อสังเกต การทดลองที่มีการทำ Technical replicate แล้วหาค่าเฉลี่ยนั้น ตัวข้อมูลยังเป็น จำนวนนับ การจะหาความต่างค่าเฉลี่ยโดยใช้ t.test นั้น จำนวน Sample ควรจะมากพอตาม Central limit theorem นอกเหนือจากนั้นควรใช้ Proportional test หรือ ควรใช้ Generalized linear model ประเภทอื่นมากกว่า","code":""},{"path":"non-parametric-test.html","id":"chi-square-test","chapter":"11 Non-parametric test","heading":"11.1.1 Chi-square test","text":"คือ การทดสอบความต่างของอัตราส่วนโดยใช้การประมาณการของ ค่าที่คาดหวัง (Expected value) และ ค่าที่สังเกตได้จริง (Observed value)\\[\n\\chi^{2} = \\sum_{=1}^{n}\\frac{(O-E)^{2}}{E}\n\\]","code":""},{"path":"non-parametric-test.html","id":"chisq-f","chapter":"11 Non-parametric test","heading":"11.1.1.1 Chi-square goodness of fit","text":"เป็นการเปรียบเทียบว่า Observed value นั้นมาจากประชากรทางทฤษฎีหรือไม่ โดย Expected value นั้นคำนวนจาก\\[\nE_{} = CDF_{}(Y_{u})-CDF_{}(Y_{l})\n\\]ยกตัวอย่างการทอดลูกเต๋า ซึ่งมีโอกาสการเกิดทุกหน้า = \\(1/6\\)\\(H_{0}\\) การกระจายตัวของการทอดลูกเต๋านี้เท่ากับการกระจายทางทฤษฎี\\(H_{0}\\) การกระจายตัวของการทอดลูกเต๋านี้เท่ากับการกระจายทางทฤษฎี\\(H_{}\\) การกระจายตัวของการทอดลูกเต๋านี้ไม่เท่ากับการกระจายทางทฤษฎี\\(H_{}\\) การกระจายตัวของการทอดลูกเต๋านี้ไม่เท่ากับการกระจายทางทฤษฎีจะเห็นว่าความแตกต่างระหว่าง Observed และ Expected นั้น อยู่ในพิสัยของของ \\(H_{0}\\)แต่ถ้าลูกเต๋านั้นเป็นลูกเต๋าถ่วงน้ำหนักเมื่อวิเคราะห์ Chi-square ตามการกระจายตัวของลูกเต๋าทั่วไป จะแตกต่างอย่างมีนัยสำคัญแต่ถ้าวิเคราะห์เทียบกับการกระจายตัวเดียวกับลูกเต๋าถ่วงน้ำหนัก จะไม่แตกต่างกัน","code":"\nset.seed(123)\ndice <- sample(6, 1000, replace = TRUE) # โยนลูกเต๋า 1000 ครั้ง\nchisq_dice <- chisq.test(table(dice))\nchisq_dice## \n##  Chi-squared test for given probabilities\n## \n## data:  table(dice)\n## X-squared = 1.436, df = 5, p-value = 0.9203\ndata.frame(O = chisq_dice$observed, E.Freq = chisq_dice$expected)\nweight_dice <- sample(6,1000, replace = TRUE, prob = c(3,2,1,1,1,1)/9)\nchisq_weight_norm <- chisq.test(table(weight_dice))\nchisq_weight_norm## \n##  Chi-squared test for given probabilities\n## \n## data:  table(weight_dice)\n## X-squared = 248.55, df = 5, p-value < 2.2e-16\ndata.frame(O = chisq_weight_norm$observed, E.freq = chisq_weight_norm$expected)\nchisq_weight_weight <- chisq.test(table(weight_dice), p = c(3,2,1,1,1,1)/9)\nchisq_weight_weight## \n##  Chi-squared test for given probabilities\n## \n## data:  table(weight_dice)\n## X-squared = 2.9135, df = 5, p-value = 0.7133\ndata.frame(O = chisq_weight_weight$observed, E.freq = chisq_weight_weight$expected)"},{"path":"non-parametric-test.html","id":"chi-square-test-of-independence","chapter":"11 Non-parametric test","heading":"11.1.1.2 Chi-square test of independence","text":"เป็นการเปรียบเทียบข้อมูลจำนวนสองกลุ่มขึ้นไปว่า มีความสัมพันธ์ที่ทำให้การกระจายตัวของข้อมูลเปลี่ยนไปจากปกติหรือไม่\\(H_{0}\\): ข้อมูลทั้ง 2+ กลุ่มนั้นไม่มีความสัมพันธ์ต่อกัน\\(H_{}\\): ข้อมูลทั้ง 2+ กลุ่มนั้นมีความสัมพันธ์ต่อกันโดยการวิเคราะห์นั้นจะใช้กับข้อมูลความถี่แบบ \\(n \\times n\\) โดย Expected event นั้นคิดจาก\\[\nE_{,j} = P(G_{,j}) \\times P(Con_{,j}) \\times \\text{total counts}\n\\] \\[\nE = \\frac{\\text{Row total} \\times \\text{Column total}}{\\text{Total sample size}}\n\\]อย่างเช่น Expected event สำหรับช่อง \\(\\) คือ \\[\n\\frac{(+ B) \\times (+ C)}{(+ B + C + D)^{2}}(+B+C+D)\n\\]ยกตัวอย่างว่าอยากทราบว่าเพศมีผลต่ออัตราการตายในมะเร็งปอดหรือไม่\\(p\\) < 0.05 หมายความว่าเพศมีผลต่ออัตราการตายอย่างมีนัยสำคัญลองคำนวณเองตามสูตรขั้นต้น","code":"\nlung_ob##         status\n## sex      Alive Dead\n##   Female    37   53\n##   Male      26  112\nlung_chisq <- chisq.test(lung_ob, correct = FALSE)\nlung_chisq## \n##  Pearson's Chi-squared test\n## \n## data:  lung_ob\n## X-squared = 13.511, df = 1, p-value = 0.0002371\nlung_ex <- apply(expand.grid(rowSums(lung_ob), colSums(lung_ob)),1, prod) |> \n  matrix(nrow=2)/sum(lung_ob) # สร้าง expected table\n\nlung_ex##          [,1]     [,2]\n## [1,] 24.86842 65.13158\n## [2,] 38.13158 99.86842\nchi_value <- sum((lung_ob - lung_ex)^2/lung_ex)\nchi_value # chi-squared## [1] 13.51117\npchisq(chi_value, df = lung_chisq$parameter, lower.tail = FALSE) # p-value## [1] 0.000237147"},{"path":"non-parametric-test.html","id":"fisher","chapter":"11 Non-parametric test","heading":"11.1.2 Fisher’s exact test","text":"คือการทดสอบว่าข้อมูลนั้นมีความสัมพันธ์หรือไม่ โดยการเทียบกับความสัมพันธ์แบบสุ่ม การทดสอบนี้จะมีความแม่นยำกว่า Chi-square เนื่องจากเป็นการคำนวณความน่าจะเป็นโดยตรง\\[\np = \\frac{{+B \\choose }{C+D \\choose C}}{{n \\choose +C}} = \\frac{{+B \\choose B}{C+D \\choose D}}{{n \\choose B+D}}\n\\]พิจารณาสูตร จุดประสงค์คือ การคำนวณความน่าจะเป็นของการหยิบสุ่มแบบไม่คืนให้ได้ตาม Condition ที่ต้องการนั่นเอง\\(p\\) < 0.05 หมายความว่าเพศมีผลต่ออัตราการตายอย่างมีนัยสำคัญ สังเกตว่า \\(p\\) จะมากกว่า Chi-square เนื่องจากการทดสอบนี้มีความ Conservative กว่าลองคำนวณเองตามสูตรขั้นต้นปล. การใช้ lung_ob[1] - 1 นั้นมีที่มาจากว่า เมื่อใช้ lower.tail = TRUE จะคำนวณโอกาสที่ ได้ \\(P[X > x]\\) ซึ่งเราต้องการ \\(P[X \\geq x]\\) จึงต้อง ลบ Condition ที่ต้องการออกด้วย","code":"\nfisher.test(lung_ob, alternative = \"two.sided\")## \n##  Fisher's Exact Test for Count Data\n## \n## data:  lung_ob\n## p-value = 0.0004349\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.583762 5.727861\n## sample estimates:\n## odds ratio \n##   2.991585\nalive_dead <- colSums(lung_ob)\nalive_dead # total alive and dead## Alive  Dead \n##    63   165\nmale_female <- rowSums(lung_ob)\nmale_female # total male and female## Female   Male \n##     90    138\nfisher_p <- (choose(alive_dead[1], lung_ob[1]-1)*choose(alive_dead[2], lung_ob[3])/\n               (choose(sum(lung_ob), male_female[1])))\n\nunname(fisher_p)*2 # remove name## [1] 0.0004388966\n## or\n2*(phyper(lung_ob[1]-1,alive_dead[1], \n       alive_dead[2], male_female[1],lower.tail=FALSE))## [1] 0.0004635661"},{"path":"non-parametric-test.html","id":"rank-test","chapter":"11 Non-parametric test","heading":"11.2 Rank test","text":"เป็นการหาความต่างของลำดับ แทนที่จะหาความต่างของ Mean/Variance ในภาวะที่ไม่ทราบการกระจายตัวของข้อมูลที่ชัดเจน","code":""},{"path":"non-parametric-test.html","id":"test-for-normality","chapter":"11 Non-parametric test","heading":"11.2.1 Test for normality","text":"กลับมาที่ตัวอย่าง iris ดูการกระจายของข้อมูลพิจารณาแล้ว Petal.Width ไม่น่าจะใช่ Normal distribution จะทำการทดสอบต่อโดย shapiro.test() ซึ่งเป็นการทดสอบการกระจายตัวของข้อมูลว่าหลุดออกจาก Normal distribution หรือไม่จะเห็นว่า Petal.Width มี \\(p\\) < 0.05 พอสมควร (ไม่เป็น Normal distribution) จึงสมควรใช้ Non-parametric test","code":"\n  ggplot(long_df, aes(x = cm, fill = Metrics)) + geom_histogram() +\n  facet_grid(Metrics~Species, scales = \"free\") +\n  theme_bw()\nlong_df |> \n  group_by(Metrics, Species) |> \n  summarise(normality = round(shapiro.test(cm)$p.value, 4))"},{"path":"non-parametric-test.html","id":"wilcox-rs","chapter":"11 Non-parametric test","heading":"11.2.2 Wilcoxon’s rank sum test (Mann-Whitney U test)","text":"คือ การคำนวณว่ากลุ่มตัวอย่าง 2 กลุ่มนั้นมาจากประชากรกลุ่มเดียวกันหรือไม่ จากการพิจารณาผลรวมของลำดับข้อมูลทั้งสองกลุ่มลักษณะการใช้คล้าย Independent t-test สำหรับ Non-normal distribution\\[\nW_{j} = n_{1}n_{2} + \\frac{n_{j}(n_{j}+1)}{2} - R_{n}\n\\]\\[\nW = min(W_{1}, W_{2})\n\\]ลองคำนวณเอง หลักการคือจัดอันดับของข้อมูลโดยเรียงจาก น้อยไปมาก และให้อันดับเป็นตัวเลข (อันดับที่เท่ากัน ให้เป็นค่าเฉลี่ยของลำดับนั้น เช่น อันดับ 2, 3, 4 ที่มีค่าเท่ากัน ให้อันดับเป็น (2+3+4)/3 = 5 ทุกตัว)จัดอันดับของข้อมูลโดยเรียงจาก น้อยไปมาก และให้อันดับเป็นตัวเลข (อันดับที่เท่ากัน ให้เป็นค่าเฉลี่ยของลำดับนั้น เช่น อันดับ 2, 3, 4 ที่มีค่าเท่ากัน ให้อันดับเป็น (2+3+4)/3 = 5 ทุกตัว)คำนวณค่า \\(W\\) โดยแยกกลุ่ม 1 และ กลุ่ม 2 และเลือกค่า \\(W\\) ที่น้อยที่สุดคำนวณค่า \\(W\\) โดยแยกกลุ่ม 1 และ กลุ่ม 2 และเลือกค่า \\(W\\) ที่น้อยที่สุดNote: เมื่อจำนวนตัวอย่าง >50 ค่า \\(W\\) จะเข้าสู่ Normal distribution ซึ่งสามารถเปลี่ยนเป็นค่า \\(Z\\) ได้ ส่งผลให้การคำนวณ \\(p\\)-value จะแม่นยำขึ้น\\[\nZ = \\frac{W-m_{w}}{s_{w}}\n\\]\\[\nm_{w} = \\frac{n_{1}n_{2}}{2} = \\text{mean W}\n\\]\\[\ns_{w} = \\sqrt{\\frac{n_{1}n_{2}(n_{1}+n_{2}+1)}{12}}\n\\]","code":"\niris_pw <- df |> \n  select(Petal.Width, Species) |> \n  filter(Species != \"virginica\" )\n\niris_wx <- wilcox.test(Petal.Width~Species, data = iris_pw)\n\niris_wx## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  Petal.Width by Species\n## W = 0, p-value < 2.2e-16\n## alternative hypothesis: true location shift is not equal to 0\niris_wx$p.value## [1] 2.284669e-18\niris_pw_rank <- iris_pw |> \n  arrange(`Petal.Width`) |> \n  mutate(Rank = rank(`Petal.Width`, ties.method = \"average\")) # rank all values\n\nsetosa <- split(iris_pw_rank, iris_pw$Species)$setosa\nversicolor <- split(iris_pw_rank, iris_pw$Species)$versicolor\n\nall_combn <- nrow(setosa)*nrow(versicolor)\nall_combn## [1] 2500\nsetosa_rank_sum <- all_combn + (nrow(setosa)*(nrow(setosa)+1))/2 - sum(setosa$Rank)\nsetosa_rank_sum## [1] 2500\nversicolor_rank_sum <- all_combn + (nrow(versicolor)*(nrow(versicolor)+1))/2 - sum(versicolor$Rank)\nversicolor_rank_sum## [1] 0\nW <- min(setosa_rank_sum,versicolor_rank_sum)\nW## [1] 0\n## Normal approximation\nmW <- all_combn/2\nsdW <- sqrt(all_combn*(nrow(setosa)+nrow(versicolor)+1)/12)\nz = (W-mW)/sdW\npval <- 2*pnorm(-abs(z)) \npval # not exactly equal due to tie adjustment in wilcox.test()## [1] 6.856641e-18"},{"path":"non-parametric-test.html","id":"wilcox-sign","chapter":"11 Non-parametric test","heading":"11.2.3 Wilcoxon’s signed-rank test","text":"คือ การคำนวณว่ากลุ่มตัวอย่าง 2 กลุ่มนั้นมาจากประชากรกลุ่มเดียวกันหรือไม่ จากการคำนวณลำดับข้อมูลของทั้งสองกลุ่มลักษณะการใช้คล้าย Paired t-test สำหรับ Non-normal distribution\\[\nV = \\sum_{=1}^{N_{r}}[sgn(x_{2,} - x_{1,}) \\cdot R_{}]\n\\]\\[\nsgn(x) =\\begin{cases} -1 \\quad \\text{} \\, x < 1,  \\\\0 \\quad \\ \\ \\  \\text{} \\, x = 0,\\\\1 \\quad \\ \\ \\  \\text{} \\, x > 0\\end{cases}\n\\]\\[\nV = min(V_{-}, V_{+})\n\\]สมมติการวัด Wound healing assay (วัดการเคลื่อนที่ของเซลล์) ก่อนและหลัง Treat ยาลองคำนวณเอง หลักการคือคำนวณความต่างของก่อนและหลัง (หรือคู่เทียบ)คำนวณความต่างของก่อนและหลัง (หรือคู่เทียบ)จัดอันดับของข้อมูลโดยเรียงจาก น้อยไปมาก และให้อันดับเป็นตัวเลข (อันดับที่เท่ากัน ให้เป็นค่าเฉลี่ยของลำดับนั้น เช่น อันดับ 2, 3, 4 เท่ากัน ให้อันดับเป็น (2+3+4)/3 = 5 ทุกตัวแปร)จัดอันดับของข้อมูลโดยเรียงจาก น้อยไปมาก และให้อันดับเป็นตัวเลข (อันดับที่เท่ากัน ให้เป็นค่าเฉลี่ยของลำดับนั้น เช่น อันดับ 2, 3, 4 เท่ากัน ให้อันดับเป็น (2+3+4)/3 = 5 ทุกตัวแปร)คำนวณค่า \\(V\\) โดยแยกเครื่องหมาย + และ - และเลือกค่า \\(V\\) ที่น้อยที่สุดคำนวณค่า \\(V\\) โดยแยกเครื่องหมาย + และ - และเลือกค่า \\(V\\) ที่น้อยที่สุด","code":"\nset.seed(123)\n\ntreat <- data.frame(sample = 1:20,\n  Before = runif(20, min = 400, max = 500),\n           After = detectnorm::rnonnorm(20, mean = 400, sd = 30, skew = 10, kurt = 5)$dat)\n\nhead(treat, 10)\nwilcox.test(treat$Before, treat$After, paired = TRUE, exact = TRUE)## \n##  Wilcoxon signed rank exact test\n## \n## data:  treat$Before and treat$After\n## V = 169, p-value = 0.01531\n## alternative hypothesis: true location shift is not equal to 0\ntreat_rank <- treat |> mutate(Diff = Before-After) |> \n                mutate(Absdiff = abs(Diff)) |> \n                mutate(Rank = rank(Absdiff, ties.method = \"average\")) \ntreat_rank\nsign_rank <- treat_rank |> group_by(sign(Diff)) |> \n              summarize(rank_sum = sum(Rank))\n\nV <- min(sign_rank$rank_sum)\nV## [1] 41\npval <- psignrank(V, 20,20)*2\npval## [1] 0.01531219"},{"path":"non-parametric-test.html","id":"kruskal-wallis-test","chapter":"11 Non-parametric test","heading":"11.2.4 Kruskal-Wallis test","text":"คือ การเปรียบเทียบค่าเฉลี่ยของลำดับว่ามาจากประชากรเดียวกันหรือไม่ ลักษณะการใช้เช่นเดียวกับ ANOVA\\[\nH = (N-1)\\frac{\\sum^{g}_{=1}n_{}(\\bar{r_{}}-\\bar{r})^{2}}{\\sum^{g}_{=1}\\sum^{n_{}}_{j=1}(r_{ij}-\\bar{r})^{2}}\n\\]ลองคำนวณเอง หลักการการจัดอันดับเหมือน Wilcoxon’s rank sum test","code":"\niris_pw_all <- df |> \n               select(Petal.Width, Species) # 3 groups\n\niris_kw <- kruskal.test(Petal.Width ~ Species, data = iris_pw_all)\niris_kw$p.value## [1] 3.261796e-29\niris_pw_all_ranked <- iris_pw_all |> arrange(`Petal.Width`) |> \n                        mutate(Rank = rank(`Petal.Width`, ties.method = \"average\"))\niris_pw_all_ranked\naverage_rank <- mean(iris_pw_all_ranked$Rank) # also = (nrow(iris_pw_all_ranked)+1)/2\naverage_rank## [1] 75.5\nbetween_group_var <- iris_pw_all_ranked |>\n                  group_by(Species) |> \n                   summarise(rank_var = ((mean(Rank) - average_rank)^2)*n())\nbetween_group_var\nwithin_group_var <- iris_pw_all_ranked |> \n                    mutate(rank_var = (Rank-average_rank)^2)\nwithin_group_var\nH <- ((nrow(iris_pw_all_ranked)-1))*sum(between_group_var$rank_var)/sum(within_group_var$rank_var)\nH## [1] 131.1854\npval <- pchisq(H, 2, lower.tail = FALSE)\npval## [1] 3.261796e-29"},{"path":"non-parametric-test.html","id":"correlation-test-1","chapter":"11 Non-parametric test","heading":"11.3 Correlation test","text":"","code":""},{"path":"non-parametric-test.html","id":"spearmans-correlation","chapter":"11 Non-parametric test","heading":"11.3.1 Spearman’s correlation","text":"เป็นการทดสอบความสัมพันธ์ในทิศทางของตัวแปรสองตัวแปรว่าเป็นไปในทางเดียวกันหรือไม่ (Monotonic relationship)\\[ \\rho_{xy} =\\frac{cov(x,y)}{s(x) s(y)} \\]\\[\n\\rho_{xy} = \\frac{\\sum^{n}_{=1}(x_{}-\\bar{x})(y_{}-\\bar{y})}{\\sqrt{\\sum^{n}_{=1}(x_{}-\\bar{x})^{2}}\\sqrt{\\sum^{n}_{=1}(y_{}-\\bar{y})^{2}}}\n\\]โดย \\(x, y\\) คือ ลำดับของข้อมูล (ไม่ใช่ตัวข้อมูลเอง) ส่งผลให้การทดสอบนี้ เป็นการทดสอบการเพิ่มขึ้นของลำดับ ไม่ใช่การเพิ่มขึ้นของข้อมูล ดังนั้น จึงไม่ใช่การทดสอบความสัมพันธ์เป็นเชิงเส้นเหมือน Pearson’s correlation แต่เป็นการทดสอบเพียงว่าข้อมูลไปในทิศทางเดียวันหรือไม่การทดสอบใน R ใช้ลักษณะ code เดียวกันกับ Pearson’s correlation แต่เปลี่ยน Argument เป็น method = \"pearson\" และข้อควรระวังก็คิดเหมือนกัน","code":"\npoly_data <- data.frame(x = seq(-10, 10, length.out = 100)) |> \n  mutate(y = x^9+x+10+rnorm(100, mean = 0, sd =3))\n\nggplot(poly_data, aes(x = x,y = y)) + geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(y = quote(f(x) == x^9+x+10+epsilon)) +\n  theme_bw()\ncor(poly_data$x, poly_data$y, method = \"pearson\") # not quite linear## [1] 0.6870804\ncor(poly_data$x, poly_data$y, method = \"spearman\") # monotonic## [1] 0.9993399"},{"path":"false-discovery.html","id":"false-discovery","chapter":"12 False discovery","heading":"12 False discovery","text":"","code":""},{"path":"false-discovery.html","id":"p-value-histogram","chapter":"12 False discovery","heading":"12.1 \\(p\\)-value histogram","text":"พิจารณา \\(p\\)-value histogram ที่เกิดจากการเปรียบเทียบ \\(t\\)-test ของกลุ่มตัวอย่างที่มาจากประชากรเดียวกัน ทั้งหมด 10,000 ครั้งท่านจะเห็นว่าการกระจายตัวของ \\(p\\)-value นั่นมีลักษณะเป็น Uniform distribution สังเกตว่าจะมีบางส่วนที่ \\(p\\)-value น้อยกว่า Critical-value ที่ตั้งไว้ ซึ่งในที่นี้คือ \\(p\\)-value \\(\\leq\\) 0.05 ซึ่งจะเห็นว่ามีประมาณ 5% เหตุเพราะใน \\(H_{0}\\) ที่ไม่มีความแตกต่างของค่าเฉลี่ยระหว่างประชากรนั้น มีโอกาสที่จะมีความแตกต่างที่มากกว่า 0 ได้ประมาณ 5% อยู่แล้ว ซึ่งถือว่านี่คือ False positiveส่วนอีกตัวอย่าง เป็นกลุ่มตัวอย่างสองกลุ่มที่มาจากประชากรที่ต่างกัน 100%จะเห็นว่า \\(p\\)-value นั้นจะมีการกระจายตัวที่ไม่เป็น Uniform distribution นั่นเพราะความต่างของข้อมูลนั้น ได้อยู่ในระดับที่จะ ปฏิเสธ \\(H_{0}\\) ได้อยู่แล้ว การทำการทดสอบหลายๆ ครั้ง จึงได้ผลที่ \\(p\\)-value \\(\\leq\\) 0.05 นั้นอยู่ที่ระดับใกล้ 0สุดท้ายจะทำการจำลองข้อมูลจากการทดลอง 10000 ครั้งที่มี True positive 20% และทดสอบที่ \\(\\alpha\\) = 0.05เมื่อท่านคำนวณ % ของ Rejected \\(p\\)-value จะได้ว่า\\(p\\)-value ที่ถูก Reject \\(H_{0}\\) มีมากถึง 24%! เมื่อเทียบกับ True positive ซึ่งมีเพียง 20% เท่านั้นเมื่อมาลองพล็อต \\(p\\)-value histogramในที่นี้ พื้นที่สีเขียวคือ True positive สีแดงเข้มคือ False positive และสีแดงอ่อน คือ True negative จะได้ว่าอัตราส่วนผลบวกลวง (False discovery proprotion) คือ\\[\nFDP = \\frac{TP}{TP +FP}\n\\]จะสังเกตว่าถ้าทำการทดสอบไม่กี่ครั้ง เช่น 10 ครั้ง 5% ไม่ได้มีความหมายมาก (0.05 \\(\\times\\) 10 = 0.5 ครั้ง ไม่ถึง 1 ด้วยซ้ำ) แต่ถ้าท่านทำการทดสอบนี้ 10,000 ครั้ง ท่านมีโอกาสที่จะพบ False positive ถึง 0.05 \\(\\times\\) 10,000 ~ 500 ครั้ง! ซึ่งนับว่าอันตรายกับกระเป๋าเงินของท่านมากเมื่อนำผลนี้ไป Validate ด้วยการทดลองวิธีอื่น โดยเฉพาะผลจากงาน High-throughput ที่มีการทดสอบสมมตติฐานนับพันนับหมื่นครั้ง (ตามจำนวนยีน/โปรตีน)","code":"\nset.seed(123)\nsame_dist <- sapply(1:10000, \\(x) t.test(rnorm(100,10,4), \n                                         rnorm(100,10,4))$p.value) # Mean 10 vs 10\nset.seed(123)\nvery_diff_dist <- sapply(1:10000, \\(x) t.test(rnorm(100,10,4), \n                                         rnorm(100,15,4))$p.value) # Mean 10 vs 15\nset.seed(123)\n\nnum_tests <- 10000\ntrue_positives <- 2000\nalpha <- 0.05  \n\nnull_pvalues <- runif(num_tests - true_positives) # Null p-value\nalt_pvalues <- runif(true_positives, 0, 0.05) # True p-value\nall_pvalues <- c(null_pvalues, alt_pvalues) # All p-value\nrejected_p <- sum(all_pvalues <= 0.05)/length(all_pvalues)\nrejected_p*100## [1] 24.19"},{"path":"false-discovery.html","id":"dealing-with-false-discovery","chapter":"12 False discovery","heading":"12.2 Dealing with false discovery","text":"จะเห็นว่าสถานการณ์ขั้นต้นเป็นสถานการณ์ที่ไม่น่าอภิรมย์นัก เมื่อจะต้องนำข้อมูลไป Validate ซึ่งนับเป็นโชคดีที่นักสถิติได้คิดค้นวิธีต่างๆ ขึ้นมาเพื่อพยายามปรับอัตราของผลบวกลวงนี้ให้ลดลงก่อนจะไปเสียหายในการทดลองจริง ซึ่งวิธีการมีดังนี้","code":""},{"path":"false-discovery.html","id":"family-wise-error-rate-fwer","chapter":"12 False discovery","heading":"12.2.1 Family-wise error rate (FWER)","text":"คือ โอกาสที่จะเกิดความผิดพลาดอย่างน้อย 1 ครั้งจากการทดสอบทั้งหมด\\[\nFWER(\\alpha) = 1- \\prod_{j=1}^{m}(1-\\alpha) = 1-(1-\\alpha)^m\n\\]ตัวอย่างเช่น ที่ \\(\\alpha\\) = 0.05 ถ้าท่านทำการทดลอง 100 ครั้ง ท่านมีโอกาสที่จะเกิด False positive 1 ครั้ง ถึง \\(1-(1-0.05)^{100} = 0.994\\) = 99%เมื่อทำการเปรียบเทียบที่ \\(\\alpha\\) Threshold ต่างๆ จะได้ว่า","code":"\nFWER <- expand_grid(times = 1:500, alpha = c(0.001, 0.005, 0.01, 0.05, 0.1)) |> \n  mutate(FWER = 1-(1-alpha)^times)\n\nggplot(FWER, aes(x = times, y = FWER, col = as.factor(alpha), linetype = as.factor(alpha))) + \n  geom_line(linewidth = 1) +\n  labs(x = \"No. of hypothesis\", col = \"alpha\", linetype = \"alpha\") +\n  theme_bw()"},{"path":"false-discovery.html","id":"controlling-fwer","chapter":"12 False discovery","heading":"12.2.2 Controlling FWER","text":"การควบคุม FWER นั้น คือการปรับเปลี่ยนที่ \\(\\alpha\\) โดยตรง","code":""},{"path":"false-discovery.html","id":"bonferroni","chapter":"12 False discovery","heading":"12.2.2.1 Bonferroni","text":"คือการเพิ่ม Threshold ของ \\(\\alpha\\) โดยตรง\\[\nFWER(\\alpha) \\leq m \\times \\frac{\\alpha}{m} = \\alpha\n\\]กล่าวโดยง่ายก็คือการปรับ Threshold ของ \\(\\alpha\\) ตามจำนวนครั้งที่ทดสอบนั่นเอง เช่น \\(\\alpha\\) เดิม = 0.05 ทดสอบ 10 ครั้ง \\(\\alpha\\) ที่ควรจะเป็นคือ 0.005 หรือคิดในทางกลับกัน คือ \\(p\\)-value ใหม่ = \\(m \\times p_{org}\\)ยกตัวอย่าง \\(p\\)-value ที่ \\(\\alpha\\) เดิม = 0.05","code":"\npval_df <- data.frame(hypo = LETTERS[1:5], \n                      pval = c(0.01, 0.75, 0.045, 0.1, 0.012), \n                      alpha = rep(0.05,5)) |> \n          mutate(original_reject = pval <= 0.05)\n\npval_df |> \n  mutate(adj_p = pval*(n())) |>  # new treshold\n  mutate(reject = adj_p <= 0.05)"},{"path":"false-discovery.html","id":"holms-stepdown-procedure","chapter":"12 False discovery","heading":"12.2.2.2 Holm’s stepdown procedure","text":"คือการตั้ง Threshold ของ \\(\\alpha\\) ให้สูงขึ้นเรื่อยๆ ตามจำนวนครั้งที่ทดสอบ ซึ่งมีหลักการ คือเรียง \\(p\\)-value จากน้อยไปมากสร้าง \\(\\alpha\\) Criteria ใหม่โดย \\(\\alpha_{new_{j}} = \\frac{\\alpha}{m+1-j}\\) เมื่อ \\(m\\) คือ จำนวน \\(p\\) ทั้งหมด และ \\(j\\) คือ ลำดับของ \\(p\\)-valueต่อไปจะลองใช้ข้อมูลเดิมมาปรับ \\(\\alpha\\) แบบ Holm’sจะเห็นว่าทั้งสองวิธีขั้นต้นนั้นเป็นวิธีที่ Conservative มาก และมีข้อเสียคือ Power นั้นจะลดลงอย่างรวดเร็วตามจำนวนครั้งที่ทดสอบ","code":"\npval_df |> \n  mutate(sequence = row_number()) |> \n  arrange(pval) |>  # sort p-val\n  mutate(rank = row_number()) |> # assign rank\n  mutate(new_alpha = alpha/(n()+1-rank)) |>  # new threshold\n  mutate(reject = pval <= new_alpha) |> \n  arrange(sequence) # sort back to original order"},{"path":"false-discovery.html","id":"false-discovery-rate-fdr","chapter":"12 False discovery","heading":"12.3 False discovery rate (FDR)","text":"จะเห็นว่า FWER นั้นมีข้อเสียมากเกินไป จึงมีวิธีการที่ Conservative น้อยกว่า โดยจะไปควบคุม FDP แทน ตามที่เคยกล่าวไปต้นบทอย่างไรก็ตาม ในการทดลองจริงนั้น ไม่มีทางที่จะสามารถประเมิน FDP ได้เนื่องจากไม่มีทางทราบว่า อัตราส่วนของ True positive เป็นเท่าไร จึงจำเป็นต้องใช้การประมาณของ FDP ภายใต้ขอบเขตความรู้ที่มีอยู่แทน จะได้ว่า\\[\nFDR = E(FDP) = E(\\frac{TP}{TP+FP})\n\\]","code":""},{"path":"false-discovery.html","id":"controlling-fdr","chapter":"12 False discovery","heading":"12.3.1 Controlling FDR","text":"","code":""},{"path":"false-discovery.html","id":"benjaminis-hochberg","chapter":"12 False discovery","heading":"12.3.2 Benjamini’s Hochberg","text":"เป็นวิธีที่นิยมใช้ที่สุดในการควบคุม FDR โดยมีวิธีคือเรียง \\(p\\)-value จากมากไปน้อย\\(p_{new}\\) มากที่สุด = 1คำนวน \\(p_{new_{j}} = cummin(1, \\frac{m}{j} \\times p)\\) (Cumulative min)ซึ่งวิธีนี้คือการควบคุมที่ระดับ FDR เนื่องจากไม่ได้เปลี่ยน \\(\\alpha\\) แต่ลดจำนวน Positive ตาม % ของข้อมูล","code":"\npval_df |> mutate(rank = order(pval)) |> # rank of pval\n  mutate(sequence = row_number()) |> # original sequence\n  arrange(desc(pval)) |> # arrange p\n  mutate(adj_p = cummin(n()*pval/rank)) |> # cumulative min\n  mutate(adj_p = ifelse(adj_p > 1, 1, adj_p)) |> # limit at 1\n  arrange(sequence) |> \n  mutate(reject = adj_p <= 0.05)"},{"path":"false-discovery.html","id":"adjust-p-in-r","chapter":"12 False discovery","heading":"12.4 Adjust \\(p\\) in R","text":"ท่านไม่ต้องทำวิธีนี้ทั้งหมดด้วยตนเอง เนื่องจาก R มี p.adjust() ไว้ให้อยู่แล้ว","code":"\npval_df$pval## [1] 0.010 0.750 0.045 0.100 0.012\np.adjust(pval_df$pval, method = \"bonferroni\") ## [1] 0.050 1.000 0.225 0.500 0.060\np.adjust(pval_df$pval, method = \"BH\")## [1] 0.030 0.750 0.075 0.125 0.030"},{"path":"regression-model.html","id":"regression-model","chapter":"13 Regression model","heading":"13 Regression model","text":"คือ การสร้างสมการถดถอยของความสัมพันธ์ระหว่าง 2 ตัวแปรขึ้นไป โดยประกอบไปด้วยตัวแปรต้น (Independent variable, \\(x\\)) คือ ตัวแปรที่เป็นต้นเหตุตัวแปรต้น (Independent variable, \\(x\\)) คือ ตัวแปรที่เป็นต้นเหตุตัวแปรตาม (Dependent variable, \\(y\\)) คือ ตัวแปรที่เป็นปลายเหตุ ซึ่งเป็นผลมาจากการเปลี่ยนแปลงของตัวแปรต้นตัวแปรตาม (Dependent variable, \\(y\\)) คือ ตัวแปรที่เป็นปลายเหตุ ซึ่งเป็นผลมาจากการเปลี่ยนแปลงของตัวแปรต้นความสัมพันธ์ของตัวแปรต้นและตัวแปรตามจะเขียนในรูปแบบ \\(f(x) \\sim x\\)","code":""},{"path":"regression-model.html","id":"linear-regression","chapter":"13 Regression model","heading":"13.1 Linear regression","text":"","code":""},{"path":"regression-model.html","id":"model-summary","chapter":"13 Regression model","heading":"13.1.1 Model summary","text":"คือ การสร้างความสัมพันธ์ของตัวแปรแบบเชิงเส้น ใช้กับข้อมูลแบบต่อเนื่อง (Continuous data)\\[\nh_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\ …  \\ + \\theta_{n}x_{n} + \\epsilon\n\\]","code":"\ndata(\"Diabetes\", package = \"heplots\")\n\nggplot(Diabetes, aes(x = glufast, y = sspg)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw()"},{"path":"regression-model.html","id":"model-performance","chapter":"13 Regression model","heading":"13.1.2 Model performance","text":"ในการวัดความแม่นยำของ Linear regression นั้นประกอบด้วยสามองค์ประกอบ\\[\n\\text{Total variation} = \\text{Explained variation} + \\text{Unexplained variation/Error}\n\\]\\[\n\\text{Total sum squares} \\ (TSS) = \\text{Sum squares regression} \\ (SSR) + \\text{Sum squares error} \\ (SSE)\n\\]\\[\n\\sum^{n}_{=1}(y_{}-\\bar{y}_{})^{2} = \\sum^{n}_{=1}(\\hat{y}_{}-\\bar{y}_{})^{2} + \\sum^{n}_{=1}(y_{}-\\hat{y}_{})^{2}\n\\]\\(TSS\\) = Total variation = ค่าความผันผวนระหว่างข้อมูลกับค่าเฉลี่ยของข้อมูล\\(TSS\\) = Total variation = ค่าความผันผวนระหว่างข้อมูลกับค่าเฉลี่ยของข้อมูล\\(SSR\\) = Explained variation = ค่าความผันผวนระหว่างค่าเฉลี่ยของข้อมูลกับเส้น Regression\\(SSR\\) = Explained variation = ค่าความผันผวนระหว่างค่าเฉลี่ยของข้อมูลกับเส้น Regression\\(SSE\\) = Error = ค่าความผันผวนระหว่างข้อมูลกับเส้น Regression\\(SSE\\) = Error = ค่าความผันผวนระหว่างข้อมูลกับเส้น RegressionEstimate คือ ค่าสัมประสิทธิ์ของตัวแปรที่มีผลต่อสมการนั้นๆ โดย (Intercept) คือ จุดตัดแกนของ Dependent variable เมื่อไม่คิดผลกระทบจากตัวแปรอื่น และ ในส่วน glufast คือ ค่าที่เพิ่มขึ้น เมื่อ glufast เพิ่มขึ้น 1 หน่วย ซึ่งเขียนเป็นสมการได้ว่า\\[ f(x) = 39.4354 \\ + 1.1866\\times(\\text{glufast}) + \\epsilon \\]\\(R^{2}\\) คือ อัตราส่วนระหว่าง Explained variation กับ Total variation = \\(\\frac{SSR}{TSS} = 1-\\frac{SSE}{TSS}\\) ซึ่งจะบ่งบอกความสามารถของเส้นถดถอย ในการอธิบายข้อมูล (Goodness fit)\\(R^{2}\\) คือ อัตราส่วนระหว่าง Explained variation กับ Total variation = \\(\\frac{SSR}{TSS} = 1-\\frac{SSE}{TSS}\\) ซึ่งจะบ่งบอกความสามารถของเส้นถดถอย ในการอธิบายข้อมูล (Goodness fit)\\(F\\)-test คือการเปรียบเทียบความสามารถของเส้นถดถอย ว่าสามารถอธิบายข้อมูลได้ดีกว่า \\(H_{0}\\) อย่างมีนัยสำคัญหรือไม่ ภายใต้สมมติฐาน\n\\(H_{0}\\): \\(f(x) = \\theta_{0} + c\\) หรือ สามารถอธิบายข้อมูลได้โดยใช้แค่ค่าเฉลี่ย\n\\(H_{}\\): \\(f(x) = \\theta_{0} + x_{1}\\theta_{1} + … + \\epsilon\\)\nซึ่งเป็นการเทียบอัตราส่วน Explained กับ Unexplained variation เช่นเดียวกับ ANOVA\\(F\\)-test คือการเปรียบเทียบความสามารถของเส้นถดถอย ว่าสามารถอธิบายข้อมูลได้ดีกว่า \\(H_{0}\\) อย่างมีนัยสำคัญหรือไม่ ภายใต้สมมติฐาน\\(H_{0}\\): \\(f(x) = \\theta_{0} + c\\) หรือ สามารถอธิบายข้อมูลได้โดยใช้แค่ค่าเฉลี่ย\\(H_{0}\\): \\(f(x) = \\theta_{0} + c\\) หรือ สามารถอธิบายข้อมูลได้โดยใช้แค่ค่าเฉลี่ย\\(H_{}\\): \\(f(x) = \\theta_{0} + x_{1}\\theta_{1} + … + \\epsilon\\)\\(H_{}\\): \\(f(x) = \\theta_{0} + x_{1}\\theta_{1} + … + \\epsilon\\)ซึ่งเป็นการเทียบอัตราส่วน Explained กับ Unexplained variation เช่นเดียวกับ ANOVA\\[F =\\frac{MSR}{MSE} = \\frac{TSS-SSE}{SSE}/\\frac{DF_{TSS}-DF_{SSE}}{DF_{SSE}}\\]\\(t\\)-test คือการเปรียบเทียบเส้น Regression เมื่อมีตัวแปรนั้น ว่าอธิบายได้ดีกว่าเมื่อไม่มีตัวแปรนั้นหรือไม่\\(t\\)-test คือการเปรียบเทียบเส้น Regression เมื่อมีตัวแปรนั้น ว่าอธิบายได้ดีกว่าเมื่อไม่มีตัวแปรนั้นหรือไม่\\(H_{0}\\): \\(\\theta = 0\\)\\(H_{0}\\): \\(\\theta = 0\\)\\(H_{}\\): \\(\\theta \\neq 0\\)\\(H_{}\\): \\(\\theta \\neq 0\\)จะเห็นว่าสมการนั้นเหมือน One-sample t-test แต่เขียนในรูปแบบ Regression\\[\nt = \\frac{\\theta_{H_{}}-\\theta_{H_0}}{SE(\\theta)} = \\frac{\\theta-0}{SE(\\theta)}\n\\]\\[\nSE(\\theta) = \\sqrt{\\frac{1}{n-2} \\times {\\sum_{=1}^{n}\\frac{(y_{} - \\hat{y_{}})^2}{(x_{} - \\bar{x})^2}}}\n\\]","code":"## \n## Call:\n## lm(formula = sspg ~ glufast, data = Diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -197.030  -59.050   -0.371   68.950  142.060 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  39.4534    13.3348   2.959  0.00362 ** \n## glufast       1.1866     0.0969  12.247  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 74.33 on 143 degrees of freedom\n## Multiple R-squared:  0.5119, Adjusted R-squared:  0.5085 \n## F-statistic:   150 on 1 and 143 DF,  p-value: < 2.2e-16\ndiabetes_fit <- lm(sspg ~ glufast, data = Diabetes) \nsummary(diabetes_fit)## \n## Call:\n## lm(formula = sspg ~ glufast, data = Diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -197.030  -59.050   -0.371   68.950  142.060 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  39.4534    13.3348   2.959  0.00362 ** \n## glufast       1.1866     0.0969  12.247  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 74.33 on 143 degrees of freedom\n## Multiple R-squared:  0.5119, Adjusted R-squared:  0.5085 \n## F-statistic:   150 on 1 and 143 DF,  p-value: < 2.2e-16"},{"path":"regression-model.html","id":"prediction","chapter":"13 Regression model","heading":"13.1.3 Prediction","text":"ข้อดีของสมการถอดถอย คือ สามารถใช้ในการทำนายข้อมูลตัวแปรตามชุดใหม่ได้ จากผลลัพธ์ขั้นต้นในคอลัมน์ coef พบว่าทุกๆ glufast ที่เพิ่มขึ้น 1.186 หน่วย ส่งผลให้ sspg เพิ่มขึ้น 1 หน่วยโดยสมการนี้จะอยู่ใน diabetes_fit","code":"\nnew_sspg <- data.frame(glufast = 1:400) \nnew_sspg <- new_sspg |> \n  mutate(predicted_sspg = predict(diabetes_fit, newdata = new_sspg))\n\nggplot(new_sspg, aes(x = glufast, y = predicted_sspg)) +\n  geom_point(size = 0.3) + theme_bw()"},{"path":"regression-model.html","id":"logistic-regression","chapter":"13 Regression model","heading":"13.2 Logistic regression","text":"","code":""},{"path":"regression-model.html","id":"model-summary-1","chapter":"13 Regression model","heading":"13.2.1 Model summary","text":"คือ สมการถดถอยซึ่งมีคุณสมบัติในการจำแนกตัวแปรแบบสองตัวแปร (Binary classification) ซึ่งเขียนอยู่ในรูปของ ค่าลอการิธึมของอัตราส่วนความเสี่ยง (Log odds)\\[\nlog(\\frac{h(x)}{1-h(x)}) = \\theta_{0} + x_{1}\\theta_{1} + x_{2}\\theta_{2} + \\ ... \\ + x_{n}\\theta_{n} + \\epsilon = z\n\\]\\[\n\\frac{h(x)}{1-h(x)} = e^{z}\n\\]\\[\nh(x) = \\frac{1}{1+e^{z}}\n\\]\\[\nh(x) = \\frac{1}{1+e^{\\theta_{0} + x_{1}\\theta_{1} + x_{2}\\theta_{2} + \\ ... \\ + x_{n}\\theta_{n} + \\epsilon}}\n\\]ความพิเศษของสมการนี้คือ ขอบเขตของ \\(h(x)\\) จะอยู่ระหว่าง (0, 1) เสมอ ซึ่งส่งผลให้สามารถคำนวณกลับไปทำนายอัตราการเกิดเหตุการณ์จากอัตราส่วนความเสี่ยงได้ต่อไปเราจะทำนายว่า glufast เพื่อให้ตัวแปรเป็น binary เราจะรวม Overt_Diabetic และ Chemical_Diabetic เป็นกลุ่มเดียวกัน","code":"\nlog_df <- data.frame(x = -20:20) |> \n  mutate(y = 1/(1+exp(0 + 0.75*x)))\n\nggplot(log_df, aes(x = x, y = y)) + geom_line() + theme_bw() +\n  labs(y = \"h(x)\")\nDiabetes_mixed <- Diabetes |> \n  mutate(group = case_match(group,\n                            \"Normal\" ~ 0,\n                            \"Chemical_Diabetic\" ~ 1,\n                            \"Overt_Diabetic\" ~ 1))\n\nggplot(Diabetes_mixed, aes(x = glufast, y = group)) + \n  geom_point() +\n  geom_smooth(method = \"glm\",  method.args = list(family = \"binomial\"), \n              se = FALSE) +\n  theme_bw()"},{"path":"regression-model.html","id":"model-performance-1","chapter":"13 Regression model","heading":"13.2.2 Model performance","text":"Estimate ในที่นี้คือ \\(log(\\frac{h(x)}{1-h(x)})\\) โดยที่ \\(\\frac{h(x)}{1-h(x)}\\) คือ Odd ratio ของการเกิดเหตุการณ์Estimate ในที่นี้คือ \\(log(\\frac{h(x)}{1-h(x)})\\) โดยที่ \\(\\frac{h(x)}{1-h(x)}\\) คือ Odd ratio ของการเกิดเหตุการณ์Likelihood คือ ค่าของตวามเป็นไปได้ที่จะเกิดเหตุการณ์นั้นๆ\n\\[\n\\prod_{=1}^{n}p_{}^{y_{}}(1-p_{}^{y_{}})\n\\]โดย \\(y_{}\\) คือ ค่าของความเป็นไปได้ของการเกิดเหตุการณ์ตาม Binomial distribution ซึ่งในที่นี่คือ group = 0 และ group = 1 ซึ่งการคำนวณนั้นจะนิยมใช้ค่า log มากกว่า เพิ่มความสะดวก\n\\[\nLL = log(\\prod_{=1}^{n}p_{}^{y_{}}(1-p_{}^{1-y_{}})) = \\sum_{=1}^{n}log(p_{}^{y_{}}(1-p_{}^{1-y_{}}))\n\\]Likelihood คือ ค่าของตวามเป็นไปได้ที่จะเกิดเหตุการณ์นั้นๆ\\[\n\\prod_{=1}^{n}p_{}^{y_{}}(1-p_{}^{y_{}})\n\\]โดย \\(y_{}\\) คือ ค่าของความเป็นไปได้ของการเกิดเหตุการณ์ตาม Binomial distribution ซึ่งในที่นี่คือ group = 0 และ group = 1 ซึ่งการคำนวณนั้นจะนิยมใช้ค่า log มากกว่า เพิ่มความสะดวก\\[\nLL = log(\\prod_{=1}^{n}p_{}^{y_{}}(1-p_{}^{1-y_{}})) = \\sum_{=1}^{n}log(p_{}^{y_{}}(1-p_{}^{1-y_{}}))\n\\]Null deviance คือ \\(2(LL(\\text{Saturated model}) - LL(\\text{Null model}))\\)Null deviance คือ \\(2(LL(\\text{Saturated model}) - LL(\\text{Null model}))\\)Residual deviance คือ \\(2(LL(\\text{Saturated model}) - LL(\\text{Fitted model}))\\)Residual deviance คือ \\(2(LL(\\text{Saturated model}) - LL(\\text{Fitted model}))\\)Akaike information criterion (AIC) คือ ค่าการประมาณความผิดพลาดของสมการ \\(2(\\text{Parameters}- LL(\\text{Fitted model}))\\) มักใช้ในการเปรียบเทียบกับสมการตัวแปรชนิดอื่น เพื่อเทียบคุณภาพของแบบจำลองที่มีตัวแปรต่างกันAkaike information criterion (AIC) คือ ค่าการประมาณความผิดพลาดของสมการ \\(2(\\text{Parameters}- LL(\\text{Fitted model}))\\) มักใช้ในการเปรียบเทียบกับสมการตัวแปรชนิดอื่น เพื่อเทียบคุณภาพของแบบจำลองที่มีตัวแปรต่างกันPseudo \\(R^{2}\\) สามารถคำนวณได้จาก \\(1 - \\frac{D_{Fitted}}{D_{Null}}\\)Pseudo \\(R^{2}\\) สามารถคำนวณได้จาก \\(1 - \\frac{D_{Fitted}}{D_{Null}}\\)ค่าที่เกี่ยวข้องกับ Deviance นั้น ยิ่งน้อยยิ่งดี เนื่องหมายความว่ามี Deviation ต่ำจากข้อมูลนี้ สามารถคำนวณ Likelihood ratio statistics ได้\\[\nL = -2log(\\frac{LL_{Fitted}}{LL_{Null}}) =D_{Null} - D_{Fitted}\n\\]ซึ่งค่า \\(L\\) นั้นจะมีลักษณะการกระจายตัวเป็นแบบเดียวกับ Chi-square","code":"\nlogit_fit <- glm(group ~ glufast, family = \"binomial\", \n                 data = Diabetes_mixed) \nsummary(logit_fit)## \n## Call:\n## glm(formula = group ~ glufast, family = \"binomial\", data = Diabetes_mixed)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -11.76109    2.42017  -4.860 1.18e-06 ***\n## glufast       0.11583    0.02492   4.649 3.34e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 200.67  on 144  degrees of freedom\n## Residual deviance: 121.94  on 143  degrees of freedom\n## AIC: 125.94\n## \n## Number of Fisher Scoring iterations: 8\nlogit_performance <- Diabetes_mixed |> select(glufast, group) |> \n  mutate(predicted_group = predict(logit_fit, newdata = Diabetes_mixed,\n                                   type = \"response\")) |> \n  mutate(binomial_response = \n           (predicted_group^group)*(1-predicted_group)^(1-group)) |> \n  mutate(saturated_response = 1) |> \n  mutate(null_response = 0.5)\n\nlogit_likelihood <- logit_performance |> \n  summarize(across(contains(\"response\"), ~sum(log(.x))))\nlogit_likelihood\n## Null deviance\nnd <- with(logit_likelihood, 2*(saturated_response - null_response))\nnd## [1] 201.0127\n## Residual deviance\nrd <- with(logit_likelihood, 2*(saturated_response - binomial_response))\nrd ## [1] 121.944\n## AIC\naic <- 2*(2-logit_likelihood$binomial_response)\naic## [1] 125.944\nL <- nd - rd\nL## [1] 79.06869\npchisq(L, df = 144-143, lower.tail = FALSE)## [1] 5.998755e-19\nlibrary(lmtest) # Use package\nnull_fit <- glm(group ~ 1, data = Diabetes_mixed, family = \"binomial\")\nlrtest(logit_fit, null_fit)"},{"path":"regression-model.html","id":"prediction-1","chapter":"13 Regression model","heading":"13.2.3 Prediction","text":"ในส่วนของ Prediction นั้น ใช้ฟังก์ชันลักษณะเดียวกันกับ Linear regression โดยให้ Argument type = response","code":"\nnew_group <- data.frame(glufast = 1:200) \nnew_group <- new_group |> \n  mutate(predicted_response = predict(logit_fit, \n                                   newdata = new_group, type = \"response\")) \n\nggplot(new_group, aes(x = glufast, y = predicted_response)) +\n  geom_point() + \n  theme_bw()"},{"path":"regression-model.html","id":"poisson-family","chapter":"13 Regression model","heading":"13.3 Poisson, quassipoisson, and negative binomial regression","text":"","code":""},{"path":"regression-model.html","id":"poisson-regression","chapter":"13 Regression model","heading":"13.3.1 Poisson regression","text":"Poisson regression คือ สมการถดถอยที่ใช้ในการทำนายความถี่ หรือค่าเฉลี่ยของการเกิดเหตุการณ์นั้นๆ มักใช้กับข้อมูลที่ไม่ต่อเนื่อง (Discrete value) พิจารณาข้อมูลแบบ Poisson distribution\\[\nf(k) = P(X = k) = \\frac{\\lambda^{k}}{k!}e^{-\\lambda}\n\\]จะพบว่ามีตัวแปรที่สามารถทำนายได้เมื่อมีข้อมูลอีกชนิดหนึ่ง คือ ค่าเฉลี่ยการเกิดเหตุการณ์ \\((\\lambda)\\) และ จำนวนเหตุการณ์ที่เกิด \\((k)\\) จึงออกมาเป็นสมการถดถอยได้สองรูปแบบสำหรับ \\(\\lambda\\)\\[\n\\lambda = e^{\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+…+\\theta_{n}x_{n}}\n\\]\\[\nln(\\lambda) = \\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+…+\\theta_{n}x_{n}\n\\]สำหรับ \\(k\\) เราจำเป็นต้องเปลี่ยน \\(\\lambda\\) ให้อยู่ในรูป \\(k/t\\) และย้ายไปเป็นตัวแปรควบคุมเวลา (Offset) ซึ่งจะมี Regression coefficient = 1 เสมอ\\[ \\lambda = \\frac{k}{t} = e^{\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+…+\\theta_{n}x_{n}} \\]\\[ ln(k) - \\ln(t) = \\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+…+\\theta_{n}x_{n} \\]\\[\nln(k) =  \\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+…+\\theta_{n}x_{n} + \\ln(t) \\rightarrow  \\text{offset}\n\\]ซึ่งสมการนี้เป็นสมการพื้นฐานของการนับจำนวนใดๆ ดังเช่นตัวอย่างนี้ คือความสัมพันธ์ของตัวแปรต่างๆ กับอัตราการตายในโรคมะเร็งปากมดลูกสังเกตว่ายิ่งอายุเยอะ อัตราการตายยิ่งน้อยลงอย่างมีนัยสำคัญ ซึ่งไม่น่าจะเป็นไปได้ ทั้งนี้ เพราะยังไม่ได้ปรับระยะเวลาติดตาม โดยการใส่ Offset เป็น person-month เข้าไปคำนวณด้วยจะพบว่าหลังจากปรับตามระยะเวลาติดตาม กลุ่มอายุที่เพิ่มขึ้นจะส่งผลให้การตายเพิ่มขึ้นอย่างมีนัยสำคัญอย่างไรก็ตาม พิจารณาดูแล้วอัตราการตายไม่ได้เพิ่มขึ้นมากตามกลุ่มอายุ ซึ่งอาจจะเป็นผลบวกลวง เมื่อกลับมาพิจารณาแล้ว Poisson regression มีสมมติฐานที่สำคัญตาม Poisson distribution คือต้องเป็นจำนวนนับการกระจายตัวของข้อมูล = Dispersion parameter (\\(\\phi\\)) = Variance/Mean = 1","code":"\ncervix_mort <- read.csv(\"Resource/cervix_mort.csv\") |>  # aggregrated data for demonstration\n                mutate(histo_major = fct_relevel(histo_major, \"Squamous cell carcinoma\",\n                                                 \"Adenocarcinoma\",\n                                                 \"Adenosquamous carcinoma\",\n                                                 \"Others\"))\nglm(dead ~  age_group + histo_major + stage, family = \"poisson\", data = cervix_mort) |> summary()## \n## Call:\n## glm(formula = dead ~ age_group + histo_major + stage, family = \"poisson\", \n##     data = cervix_mort)\n## \n## Coefficients:\n##                                     Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)                         7.172968   0.031870  225.07   <2e-16 ***\n## age_group                          -0.748325   0.010905  -68.62   <2e-16 ***\n## histo_majorAdenocarcinoma          -1.413401   0.025945  -54.48   <2e-16 ***\n## histo_majorAdenosquamous carcinoma -2.850308   0.049905  -57.12   <2e-16 ***\n## histo_majorOthers                  -1.731075   0.029393  -58.89   <2e-16 ***\n## stage                               0.209258   0.008536   24.52   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 25897.8  on 59  degrees of freedom\n## Residual deviance:  9499.4  on 54  degrees of freedom\n## AIC: 9832.1\n## \n## Number of Fisher Scoring iterations: 5\nggplot(cervix_mort, aes (x = age_group, y = dead)) +\n  geom_jitter() +\n  geom_smooth(method = \"glm\", method.args = list(family = \"poisson\"), se = FALSE) +\n  scale_x_continuous(breaks = 1:4) +\n  theme_bw()\npois_fit <- glm(dead ~  age_group + histo_major + stage + offset(log(person_month)),\n                family = \"poisson\", data = cervix_mort)\nsummary(pois_fit)## \n## Call:\n## glm(formula = dead ~ age_group + histo_major + stage + offset(log(person_month)), \n##     family = \"poisson\", data = cervix_mort)\n## \n## Coefficients:\n##                                     Estimate Std. Error  z value Pr(>|z|)    \n## (Intercept)                        -7.138354   0.035957 -198.523  < 2e-16 ***\n## age_group                           0.037519   0.014553    2.578  0.00994 ** \n## histo_majorAdenocarcinoma          -0.076841   0.025947   -2.961  0.00306 ** \n## histo_majorAdenosquamous carcinoma -0.001698   0.049964   -0.034  0.97289    \n## histo_majorOthers                   0.408249   0.029643   13.772  < 2e-16 ***\n## stage                               0.918984   0.007915  116.101  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 15452.8  on 59  degrees of freedom\n## Residual deviance:  2386.7  on 54  degrees of freedom\n## AIC: 2719.4\n## \n## Number of Fisher Scoring iterations: 5\nnew_data <- expand_grid(age_group = 1:4,\n                       histo_major = unique(cervix_mort$histo_major),\n                       stage = 1:4,\n                       person_month = 1200)\nnew_data <- new_data |> \n  mutate(predicted_dead = predict(pois_fit, newdata = new_data, type = \"response\"))\n\nggplot(new_data, aes (x = age_group, y = predicted_dead, col = histo_major)) +\n  geom_line() +\n  facet_wrap(~stage)+\n  scale_y_continuous(breaks = seq(0,60,10)) +\n  theme_bw()"},{"path":"regression-model.html","id":"quasipoisson-regression","chapter":"13 Regression model","heading":"13.3.2 Quasipoisson regression","text":"เมื่อตรวจสอบจากผลลัพธ์ของสมการถอดถอย โดยพิจารณาจาก \\(\\text{Residuals/Deviance}\\) แล้วพบว่า \\(\\phi > 1\\) ซึ่งหมายความว่าข้อมูลตั้งต้นนั้นมีการกระจายตัวของข้อมูลสูงเกินไป (Overdispersed) จึงไม่เป็นไปตามสมมติฐานของ Poisson ณ จุดนี้จึงจำเป็นต้องใช้ ส่วนขยายสมการของ Poisson regression เรียกว่า Quasipoisson regression ซึ่งจะทำการปรับความแปรปรวน ตามการเพิ่มขึ้นของความแปรปรวนต่อค่าเฉลี่ย\\[\nV = \\phi\\mu_{}\n\\] \\[\n\\phi = \\frac{\\chi^{2}}{\\text{DF}} = \\frac{1}{n-k}{\\sum_{=1}^{n}\\frac{(Y_{}-\\hat{\\mu_{}})^{2}}{\\hat{\\mu_{}}}}\n\\]จะเห็นว่าหลังจากปรับ Dispersion parameter แล้ว age_group ไม่ได้ส่งผลให้การตายเพิ่มขึ้นอย่างมีนัยสำคัญแต่อย่างใด","code":"\nquasipois_fit <- glm(dead ~ age_group + histo_major + stage + \n                       offset(log(person_month)),\n                family = \"quasipoisson\", data = cervix_mort)\nsummary(quasipois_fit)## \n## Call:\n## glm(formula = dead ~ age_group + histo_major + stage + offset(log(person_month)), \n##     family = \"quasipoisson\", data = cervix_mort)\n## \n## Coefficients:\n##                                     Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                        -7.138354   0.256738 -27.804   <2e-16 ***\n## age_group                           0.037519   0.103913   0.361    0.719    \n## histo_majorAdenocarcinoma          -0.076841   0.185263  -0.415    0.680    \n## histo_majorAdenosquamous carcinoma -0.001698   0.356744  -0.005    0.996    \n## histo_majorOthers                   0.408249   0.211657   1.929    0.059 .  \n## stage                               0.918984   0.056517  16.260   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasipoisson family taken to be 50.98103)\n## \n##     Null deviance: 15452.8  on 59  degrees of freedom\n## Residual deviance:  2386.7  on 54  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 5"},{"path":"regression-model.html","id":"negative-binomial-regression","chapter":"13 Regression model","heading":"13.3.3 Negative binomial regression","text":"ปัญหาของ Quasipoisson คือ อาจจะไม่มี Parameter ที่ต้องการ เช่น AIC เป็นต้น ซึ่งสามารถใช้อีกหนึ่งสมการถดถอยที่นิยมใช้คือ Negative binomial regression จาก Package MASS ซึ่งปรับตามอัตราส่วนความแปรปรวนเช่นกัน\\[\nV = u_{}(1+\\frac{u_{}}{\\phi})\n\\]ผลลัพธ์ที่ได้จะใกล้เคียงกันกับสมการถดถอย Quasipoisson ทั้งสองสมการนี้จะนิยมใช้ในการตั้งสมการถดถอยในจำนวนนับจากงานทดลอง High-throughput เช่น RNA-sequencing เนื่องจากข้อมูลประเภทนี้มักมีเป็นข้อมูลจำนวนนับที่มีการกระจายตัวสูงมาก (\\(\\phi > 1\\))","code":"\nnb_fit <- MASS::glm.nb(dead ~  \n                         age_group + histo_major + stage + \n                         offset(log(person_month)), \n                 data = cervix_mort)\nsummary(nb_fit)## \n## Call:\n## MASS::glm.nb(formula = dead ~ age_group + histo_major + stage + \n##     offset(log(person_month)), data = cervix_mort, init.theta = 1.720086921, \n##     link = log)\n## \n## Coefficients:\n##                                    Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)                        -6.74220    0.37648 -17.908   <2e-16 ***\n## age_group                           0.13619    0.09497   1.434    0.152    \n## histo_majorAdenocarcinoma          -0.11835    0.28789  -0.411    0.681    \n## histo_majorAdenosquamous carcinoma -0.09216    0.30632  -0.301    0.764    \n## histo_majorOthers                   0.16574    0.28407   0.583    0.560    \n## stage                               0.82974    0.08937   9.285   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for Negative Binomial(1.7201) family taken to be 1)\n## \n##     Null deviance: 144.871  on 59  degrees of freedom\n## Residual deviance:  69.168  on 54  degrees of freedom\n## AIC: 598.94\n## \n## Number of Fisher Scoring iterations: 1\n## \n## \n##               Theta:  1.720 \n##           Std. Err.:  0.339 \n## \n##  2 x log-likelihood:  -584.937"},{"path":"survival-analysis.html","id":"survival-analysis","chapter":"14 Survival analysis","heading":"14 Survival analysis","text":"ในงานวิจัยที่กระทำกับผู้ป่วย หรือแม้กระทั้งเซลล์นั้น บางครั้งจะมีความจำเป็นที่ต้องทำการวิเคราะห์ข้อมูลเพื่อเปรียบเทียบสร้างแบบจำลองที่สามารถทำนายเวลาที่ใช้ก่อนที่จะเกิดเหตุการณ์ที่ท่านสนใจ (Event) เช่น เวลาที่ผู้ป่วยจะเสียชีวิตจากโรคมะเร็งนับตั้งแต่วันวินิจฉัย เวลาของเซลล์ที่จะตายหลังจากใส่สารบางอย่างที่สนใจ เป็นต้นลักษณะพิเศษของการวิเคราะห์ Survival analysis คือ มีการวิเคราะห์โดยใช้ปัจจัยที่เรียกว่า Censoring ร่วม ซึ่งคือการที่ เหตุการณ์ที่คาดหวังว่าจะเกิดนั้นไม่มาถึงแม้ว่าจะครบตามเวลาที่ผู้วิจัยสังเกตการณ์แล้ว ซึ่งทำให้ไม่สามารถมั่นใจได้ว่าเหตุการณ์นั้นจะเกิดต่อไปหรือไม่ ณ เวลาหลังจากนี้การ Censor โดยหลักมี 3 แบบ คือRight-censor (ดังรูป) คือ ไม่แน่ใจข้อมูลการเกิด event เวลาสุดท้ายที่พบ ซึ่งพบมากที่สุดRight-censor (ดังรูป) คือ ไม่แน่ใจข้อมูลการเกิด event เวลาสุดท้ายที่พบ ซึ่งพบมากที่สุดLeft-censor คือ ไม่แน่ใจข้อมูลช่วงเวลาเริ่มต้น เช่น Diagnosis วันไหนLeft-censor คือ ไม่แน่ใจข้อมูลช่วงเวลาเริ่มต้น เช่น Diagnosis วันไหนInterval-censor คือ เวลาช่วงใดช่วงหนึ่งหายไปInterval-censor คือ เวลาช่วงใดช่วงหนึ่งหายไป","code":""},{"path":"survival-analysis.html","id":"kaplein-meier-estimate-km","chapter":"14 Survival analysis","heading":"14.1 Kaplein-Meier estimate (KM)","text":"คือ กราฟแสดงอัตราการเกิดของเหตุการณ์เมื่อเทียบกับเวลาที่ผ่านไป โดยหลักการคำนวณ Survival คือ\\[ \\hat{S}(t) = \\prod_{: t_{} \\leq t}(1- \\frac{d_{}}{n_{}})\\]\\[\n\\text{Survival proability} = \\frac{n.risk - n.event}{n.risk}\n\\]อธิบายหลักการอย่างง่ายของ KM นั่นคือ ทุกเคสทียังไม่เกิดเหตุการณ์นั้น จะเป็น เคสที่เสี่ยงต่อการเกิดเหตุการณ์ (risk) ซึ่งจำนวนเคสแรกเริ่มที่เสี่ยง (Number risk) จะเท่ากับจำนวนเคสทั้งหมด (Sample size) โดยจะนับการเกิด Event ตามปกติ เพียงแต่ถ้าเคสนั้นถูก Censor นั้น n.risk จะลดลงด้วย ทำให้อัตราการเกิด Event ยังไม่เปลี่ยนแปลง ดังตัวอย่างตามตารางเมื่อนำไปพล็อตกราฟแล้วจะได้ผลดังนี้สังเกตุว่าส่วนที่ Censor (มีสัญลักษณ์ +) จะไม่มีการตกลงของกราฟ แต่เมื่อถึงเวลาที่มี Event เกิดขึ้น การตกลงของกราฟจะสูงกว่าเมื่อไม่มี Censor นำมาก่อน","code":""},{"path":"survival-analysis.html","id":"การสราง-km-ใน-r","chapter":"14 Survival analysis","heading":"14.1.1 การสร้าง KM ใน R","text":"ตัวอย่างข้อมูลของผู้ป่วยมะเร็งรังไข่ที่ได้รับการรักษาโดยการผ่าตัดอธิบายตัวแปร:age = อายุage = อายุfutime = ระยะเวลาติดตามตั้งแต่วินิจฉัยจนเสียชีวิต/มาพบแพทย์ครั้งสุดท้ายfutime = ระยะเวลาติดตามตั้งแต่วินิจฉัยจนเสียชีวิต/มาพบแพทย์ครั้งสุดท้ายfustat = 0 - censor, 1 - deadfustat = 0 - censor, 1 - deadresid.ds = มีชิ้นส่วนของมะเร็งหลงเหลือหลังจากการผ่าตัด (ผ่าตัดได้ไม่หมด)resid.ds = มีชิ้นส่วนของมะเร็งหลงเหลือหลังจากการผ่าตัด (ผ่าตัดได้ไม่หมด)rx = กลุ่มการรักษาrx = กลุ่มการรักษาecog.ps = ECOG performance status คะแนนน้อยแปลว่าผู้ป่วยมีสุขภาพโดยรวมดีecog.ps = ECOG performance status คะแนนน้อยแปลว่าผู้ป่วยมีสุขภาพโดยรวมดีเมื่อใช้ Function Surv() จะทำการเปลี่ยน futime ให้รับรู้การ Censor สังเกตว่าผู้ป่วยที่ ไม่เกิดเหตุการณ์จะมีสัญลักษณ์ + อยู่ข้างหลัง บ่งบอกว่าข้อมูลนั้นถูก Censor นั่นหมายความว่า ผู้ป่วยจะเกิดเหตุการณ์หรือไม่ก็ได้หลังจากนี้ เพียงแต่ผู้วิจัยไม่สามารถทราบได้แล้วการพล็อต KM นั้นสามารถทำได้โดยใช้ Package survminer โดยเริ่มจากการสร้างตาราง Survival curve จากคำสั่ง survfit()หลังจากนั้นใช้คำสั่ง ggsurvplot() เพื่อทำการสร้างกราฟจะเห็นว่าผู้ป่วยกลุ่มนี้มี Median survival อยู่ประมาณ 1.75 ปีจะพบว่า ถ้าผ่าตัดแล้วไม่เหลือร่องรอยของโรค จะมีอัตราการรอดชีวิตที่ดีกว่า แต่ยังไม่ถึงระดับมีนัยสำคัญ","code":"\nlibrary(dplyr) \nlibrary(survival)\nnames(ovarian)## [1] \"futime\"   \"fustat\"   \"age\"      \"resid.ds\" \"rx\"       \"ecog.ps\"\ncensored_df <- ovarian |>    \n  mutate(censored_futime = Surv(ovarian$futime, ovarian$fustat)) \n\nhead(select(censored_df, futime, fustat, censored_futime))\nlibrary(survminer)\n\novarian_surv <- survfit(\n  Surv(futime/365.25, fustat) ~ 1, data = ovarian # เปลี่ยนเป็นปี\n  )\n\novarian_surv |> tidy() |> head(10)\nggsurvplot(ovarian_surv, risk.table = TRUE, break.time.by = 0.25, \n           surv.median.line = \"hv\") \novarian_surv_resid <- survfit(\n  Surv(futime/365.25, fustat) ~ resid.ds, data = ovarian\n  )\n\nggsurvplot(ovarian_surv_resid, risk.table = TRUE, break.time.by = 0.25,\n           surv.median.line = \"hv\", pval = TRUE)"},{"path":"survival-analysis.html","id":"log-rank-test","chapter":"14 Survival analysis","heading":"14.2 Log-rank test","text":"Log-rank test คือ Non-parametric test สำหรับ Univariate analysis ที่เปรียบเทียบความแตกต่างของอัตราการเกิด Event ว่าแตกต่างอย่างมีนัยสำคัญหรือไม่Observed คือ จำนวน event ที่เกิดขึ้นในแต่ละกลุ่มObserved คือ จำนวน event ที่เกิดขึ้นในแต่ละกลุ่มExpected คือ จำนวน event ที่คาดว่าจะเกิดขึ้นในแต่ละกลุ่มExpected คือ จำนวน event ที่คาดว่าจะเกิดขึ้นในแต่ละกลุ่ม(O-E)^2/E = Chi-square statistics ของค่า observed และ expected(O-E)^2/E = Chi-square statistics ของค่า observed และ expectedChisq = ผลสุดท้ายของ Chi-square statistics = sum(O-E)^2/EChisq = ผลสุดท้ายของ Chi-square statistics = sum(O-E)^2/Ep = p-value ของ Chi-square statisticsp = p-value ของ Chi-square statisticsค่า Log-rank นี้ สามารถแสดงใน KM ได้โดยใช้ pval = TRUE ตามหัวข้อเบื้องต้น","code":"\novarian_surv_diff <- survdiff(\n  Surv(futime/365.25, fustat) ~ resid.ds, data = ovarian\n  )\n\novarian_surv_diff## Call:\n## survdiff(formula = Surv(futime/365.25, fustat) ~ resid.ds, data = ovarian)\n## \n##             N Observed Expected (O-E)^2/E (O-E)^2/V\n## resid.ds=1 11        3     6.26      1.70      3.62\n## resid.ds=2 15        9     5.74      1.85      3.62\n## \n##  Chisq= 3.6  on 1 degrees of freedom, p= 0.06"},{"path":"survival-analysis.html","id":"cox-proportional-hazard-cph-model","chapter":"14 Survival analysis","heading":"14.3 Cox-proportional hazard (CPH) model","text":"คือ Semi-parametric model ซึ่งวัด Risk ของการเกิด Event นั้นๆ โดยมีสมการ คือ\\[\nh(t) = h_{0}(t) \\times exp(b_{1}X_{1} + b_{2}X_{2}+ ... + b_{s}X_{p})\n\\]\\[\nln(\\frac{h(t)}{h_{0}(t)}) = b_{1}X_{1} + b_{2}X_{2}+ ... + b_{s}X_{p}\n\\]ซึ่ง \\(h(t)/h_{0}(t)\\) นั้นคือ Hazard ratio (HR) หรือ ความเสี่ยงของการเกิด event นั้นๆการวิเคราะห์ CPH นั้นมีข้อดีกว่า Log-rank คือสามารถประมาณการเชิงปริมาณ (Quantitative measurement) ผ่าน HR และสามารถวิเคราะห์สมการแบบ Multivariate analysis ได้exp(coef) = HR = \\(h(t)/h_{0}(t)\\) ในที่นี้ท่านสามารถอภิปรายได้ว่า อายุที่เพิ่มขึ้น 1 ปีนั้น ส่งผลให้เกิดอัตราการเสียชีวิตในผู้ป่วยมะเร็งรังไข่เพิ่มขึ้น 1.13 เท่า (13%) และมีนัยสำคัญทางสถิติexp(coef) = HR = \\(h(t)/h_{0}(t)\\) ในที่นี้ท่านสามารถอภิปรายได้ว่า อายุที่เพิ่มขึ้น 1 ปีนั้น ส่งผลให้เกิดอัตราการเสียชีวิตในผู้ป่วยมะเร็งรังไข่เพิ่มขึ้น 1.13 เท่า (13%) และมีนัยสำคัญทางสถิติp ในตาราง คือ ค่าคำนวณ \\(p\\)-value จาก Wald’s test ของแต่ละตัวแปรว่ามีผลต่ออัตราการรอดชีวิตหรือไม่p ในตาราง คือ ค่าคำนวณ \\(p\\)-value จาก Wald’s test ของแต่ละตัวแปรว่ามีผลต่ออัตราการรอดชีวิตหรือไม่p ข้างล่าง คือ overall \\(p\\) จาก Likelihood ratio test ว่าจากทั้งหมด มีตัวแปรใดตัวแปรหนึ่งส่งผลให้อัตรากการรอดชีวิตเปลี่ยนไปอย่างมีนัยสำคัญทางสถิติหรือไม่p ข้างล่าง คือ overall \\(p\\) จาก Likelihood ratio test ว่าจากทั้งหมด มีตัวแปรใดตัวแปรหนึ่งส่งผลให้อัตรากการรอดชีวิตเปลี่ยนไปอย่างมีนัยสำคัญทางสถิติหรือไม่","code":"\novarian_cox <- coxph(Surv(futime/365.25, fustat) ~ \n                       resid.ds + age + factor(rx), data = ovarian) \n\novarian_cox## Call:\n## coxph(formula = Surv(futime/365.25, fustat) ~ resid.ds + age + \n##     factor(rx), data = ovarian)\n## \n##                coef exp(coef) se(coef)      z       p\n## resid.ds     0.6964    2.0065   0.7585  0.918 0.35858\n## age          0.1285    1.1372   0.0473  2.718 0.00657\n## factor(rx)2 -0.8489    0.4279   0.6392 -1.328 0.18416\n## \n## Likelihood ratio test=16.77  on 3 df, p=0.0007889\n## n= 26, number of events= 12"},{"path":"survival-analysis.html","id":"การตรวจสอบ-assumption-validity-ของ-cph","chapter":"14 Survival analysis","heading":"14.3.1 การตรวจสอบ Assumption validity ของ CPH","text":"CPH นั้นมี Assumption ดังนี้:ตัวแปรแต่ละกลุ่มมีอัตราการเกิด Event ที่แตกต่างกันตัวแปรแต่ละกลุ่มมีอัตราการเกิด Event ที่แตกต่างกันHR เท่ากันทุกช่วงเวลา เช่น ที่ 1, 2, 5 ปี อัตราส่วนการเสียชีวิตระหว่างตัวแปรเท่ากันหมดHR เท่ากันทุกช่วงเวลา เช่น ที่ 1, 2, 5 ปี อัตราส่วนการเสียชีวิตระหว่างตัวแปรเท่ากันหมดตัวแปรมีความสัมพันธ์แบบ Linear continuous variableตัวแปรมีความสัมพันธ์แบบ Linear continuous variableไม่จำเป็นต้องทราบลักษณะการกระจายตัวของข้อมูลก่อน (จึงเป็น Semi-parametric model)ไม่จำเป็นต้องทราบลักษณะการกระจายตัวของข้อมูลก่อน (จึงเป็น Semi-parametric model)สามารถตรวจสอบ HR ได้โดยใช้ Proportionality assumption test จาก Schoenfeld residuals โดย cox.zph()โดยการทดสอบนี้ จะทำการเปรียบเทียบ Residuals ระหว่าง Risk-weight average กับ ตัวแปรนั้นๆ ว่ามีการเปลี่ยนแปลงไปในทิศทางใดทิศทางหนึ่งหรือไม่ ถ้ามี (\\(p\\) < 0.05) หมายความว่า เวลาที่ผ่านไปอาจจะส่งผลให้ HR นั้นมีความแตกต่างกัน ซึ่งจะต้องทำ Time-varying CPH เพิ่มเติมโดยในข้อมูล ovarian นี้ ไม่มีตัวใดที่ \\(p\\)-value < 0.05 จึงถือได้ว่า อัตราส่วนนั้นคงที่ และทำให้ CPH นั้น Validในส่วนของ Linearity สามารถตวจสอบโดยใช้ ggcoxfunctional()จะเห็นว่า age นั้นการเพิ่มขึ้นแบบ Linearity โดยมี Deviation เล็กน้อย","code":"\novarian_coxzph <- cox.zph(ovarian_cox) \nggcoxzph(ovarian_coxzph)\novarian_linear_age <- ggcoxfunctional(Surv(futime/365.25, fustat) ~ age +                                 + I(log(age)) + I(sqrt(age)), data = ovarian) \novarian_linear_age"},{"path":"principal-component-analysis.html","id":"principal-component-analysis","chapter":"15 Principal component analysis","heading":"15 Principal component analysis","text":"","code":""},{"path":"principal-component-analysis.html","id":"principle-1","chapter":"15 Principal component analysis","heading":"15.1 Principle","text":"Principal component analysis (PCA) คือ วิธีการลดจำนวนมิติของข้อมูล (Dimensional reduction) เพื่อให้ง่ายต่อการวิเคราะห์และสร้างภาพ โดยที่ PCA จะพยายามเก็บข้อมูลที่สำคัญไว้ยกตัวอย่างเป็นภาพ สมมติท่านต้องการที่จะวิเคราะห์แบ่งลักษณะของรูปจากสีของรูปภาพ แต่เฉดสีนั้นมีมากมายถ้าเป็นข้อมูลอาจจะมีรูปแบบลักษณะนี้ซึ่งการที่จะนำข้อมูลสีทุกจุดมาวิเคราะห์นั้น อาจจะต้องใช้เครื่องคอมพิวเตอร์ที่มีสมรรถนะสูงมาก และข้อมูลส่วนใหญ่ก็ไม่ได้ต่างกันมากนัก ท่านจึงตัดสินใจรวมสีที่มีลักษณะใกล้เคียงกันเป็นกลุ่มๆจากกระบวนการนี้ รวมสีที่ใกล้กันเป็นกลุ่มเดียว ส่งผลให้เกิดการลดจำนวนมิติของข้อมูลลง โดยที่ยังเหลือข้อมูลที่มีความสำคัญอยู่ซึ่ง PCA นั้นทำการลดมิติของข้อมูลลง โดยรวมข้อมูลอื่นๆ เข้าหาข้อมูลที่มีความสำคัญมากที่สุด นั่นคือ ข้อมูลที่มีความแปรปรวนสูงที่สุด ยกตัวอย่างข้อมูลสองมิติชุดหนึ่งในการวิเคราะห์ PCA ข้อมูลจะถูกลดมิติลงโดยการพยายามดึงเข้าสู่เส้นที่สามารถอธิบายความแปรปรวนได้ดีที่สุด ซึ่งก็คือ Principal componentสังเกตว่า เส้นแนวเฉียงนั้น เป็นเส้นที่ข้อมูลแต่ละจุดห่างกันมากที่สุดใน 1 มิติ ซึ่งก็คือเส้นที่สามารถอธิบายความแปรปรวนได้ดีที่สุดนั่นเองส่วนเส้นต่อมาที่ตั้งฉาก คือเส้นที่อธิบายความแปรปรวนได้ดีที่สุดเป็นลำดับสองเมื่อนำข้อมูลทั้งหมดมาสร้างเป็นกราฟใหม่ จะได้ข้อมูลที่ยังลงเหลือความแปรปรวนที่มากที่สุดไว้ทั้งสองมิติการลดมิติของข้อมูลนี้ อาจจะยังไม่เห็นผลนัก เนื่องจากข้อมูลมีแค่สองมิติเท่านั้น แต่ในสถานการณ์ซึ่งมีข้อมูลสูงมาก โดยเฉพาะเมื่อมี Feature มากกว่า Observations นั้น การลดข้อมูลจะมีประสิทธิภาพมาก โดยเฉพาะข้อมูลประเภท High-throughput (จำนวนยีนมากกว่าตัวอย่าง)Credit: https://stackoverflow.com/questions/2639430/graphing-perpendicular-offsets---least-squares-regression-plot--r","code":""},{"path":"principal-component-analysis.html","id":"manually-calculate-pca","chapter":"15 Principal component analysis","heading":"15.2 Manually calculate PCA","text":"ต่อไปจะแสดงวิธีการคำนวณ PCA เพื่อให้เข้าใจลำดับของการวิเคราะห์เริ่มต้นด้วยการ Normalize ข้อมูลให้อยู่ในรูป Mean-centeringหลังจากนั้นเราจะทำการคำนวณ Covariance matrix ในที่นี้จะใช้แบบ Pearson’sNote: ถ้ากลับไปพิจารณาสูตร Covariance Pearson’s แล้ว จะได้ว่า\\[\ncov = \\frac{1}{n-1}({^{T}})\n\\]โดย \\(n\\) คือ จำนวนแถวของ Matrix \\(\\)หลังจากนั้นเราทำการแยกทิศทางของความแปรปรวนออกมาโดยวิธี Eigendecomposition ซึ่งสำหรับ PCA แล้ว ซึ่ง Eigenvector นั้นบ่งบอกถึงทิศทางของความแปรปรวนของข้อมูลที่ตั้งฉากกัน โดยมีขนาดของการกระจายตัวของข้อมูล คือ Eigenvalueโดย vectors คือ Eigenvector หรือ ทิศทางของความแปรปรวน (เรียงตามคอลัมน์) ส่วน values คือ Eigenvalues หรือ ขนาดของความแปรปรวนNote: เราสามารถคำนวณ Eigenvalue และ Eigenvector กลับไปเป็น Covariance matrix\\[\n= VQV^{-1}\n\\]ต่อไปจะต้องเรียง Eigenvector ตามขนาดของ Eigenvalues จากน้อยไปมาก ซึ่งนั้นก็คือ Principal component ของข้อมูล (PC1, PC2, PC3, ….) นั่นหมายความว่า \\(n_{PC} \\leq n_{Obs}\\) เสมอสุดท้ายเมื่อทำการคูณข้อมูลเดิมกลับไปด้วย Eigenvectors ที่เรียงแล้ว จะได้ ข้อมูลของความแปรปรวนข้อมูลแต่ละคอลัมน์ในแต่ละ PC","code":"\nset.seed(123)\ndata <- matrix(c(\n  rnorm(10, mean = 20, sd = 2),\n  rnorm(10, mean = 1, sd = 2),\n  rnorm(10, mean = 4, sd = 5)),\n  ncol = 3\n)\ncolnames(data) <- c(\"A\", \"B\", \"C\")\ndata##              A           B          C\n##  [1,] 18.87905  3.44816359 -1.3391185\n##  [2,] 19.53965  1.71962765  2.9101254\n##  [3,] 23.11742  1.80154290 -1.1300222\n##  [4,] 20.14102  1.22136543  0.3555439\n##  [5,] 20.25858 -0.11168227  0.8748037\n##  [6,] 23.43013  4.57382627 -4.4334666\n##  [7,] 20.92183  1.99570096  8.1889352\n##  [8,] 17.46988 -2.93323431  4.7668656\n##  [9,] 18.62629  2.40271180 -1.6906847\n## [10,] 19.10868  0.05441718 10.2690746\nscaled_data <- apply(data,2, \\(x) (x-mean(x))/sd(x))\nscaled_data##                  A           B          C\n##  [1,] -0.665875352  0.97821584 -0.6910813\n##  [2,] -0.319572479  0.14564661  0.2219402\n##  [3,]  1.555994430  0.18510203 -0.6461534\n##  [4,] -0.004316756 -0.09434713 -0.3269546\n##  [5,]  0.057310762 -0.73642490 -0.2153829\n##  [6,]  1.719927421  1.52040423 -1.3559540\n##  [7,]  0.405008410  0.27862049  1.3561812\n##  [8,] -1.404601888 -2.09545793  0.6208920\n##  [9,] -0.798376211  0.47466195 -0.7666212\n## [10,] -0.545498338 -0.65642119  1.8031341\nscaled_data <- scale(data, center = TRUE, scale = TRUE) # Same\nscaled_data##                  A           B          C\n##  [1,] -0.665875352  0.97821584 -0.6910813\n##  [2,] -0.319572479  0.14564661  0.2219402\n##  [3,]  1.555994430  0.18510203 -0.6461534\n##  [4,] -0.004316756 -0.09434713 -0.3269546\n##  [5,]  0.057310762 -0.73642490 -0.2153829\n##  [6,]  1.719927421  1.52040423 -1.3559540\n##  [7,]  0.405008410  0.27862049  1.3561812\n##  [8,] -1.404601888 -2.09545793  0.6208920\n##  [9,] -0.798376211  0.47466195 -0.7666212\n## [10,] -0.545498338 -0.65642119  1.8031341\n## attr(,\"scaled:center\")\n##         A         B         C \n## 20.149251  1.417244  1.877206 \n## attr(,\"scaled:scale\")\n##        A        B        C \n## 1.907568 2.076147 4.654046\ncov_matrix <- t(scaled_data) %*% scaled_data / (nrow(scaled_data) - 1) # matrix multiplication\ncov_matrix##            A          B          C\n## A  1.0000000  0.5776151 -0.4059593\n## B  0.5776151  1.0000000 -0.5673487\n## C -0.4059593 -0.5673487  1.0000000\ncov_matrix <- cov(scaled_data)\ncov_matrix # same##            A          B          C\n## A  1.0000000  0.5776151 -0.4059593\n## B  0.5776151  1.0000000 -0.5673487\n## C -0.4059593 -0.5673487  1.0000000\neigen_info <- eigen(cov_matrix)\neigen_info## eigen() decomposition\n## $values\n## [1] 2.0376621 0.5941719 0.3681659\n## \n## $vectors\n##            [,1]       [,2]       [,3]\n## [1,] -0.5596713 0.69413342 -0.4527105\n## [2,] -0.6151534 0.01806984  0.7882003\n## [3,]  0.5552966 0.71961954  0.4168854\neigen_back <- \n  eigen_info$vectors %*% diag(eigen_info$values) %*% solve(eigen_info$vectors)\n\nround(sum(eigen_back - cov_matrix), 10) # Diff nearly 0 due to algorithm## [1] 0\nsorted_eigenvalues <- eigen_info$values\nsorted_eigenvectors <- eigen_info$vectors[, order(-sorted_eigenvalues)] # Decreasing order\nnum_components <- 3  # Adjust this as needed\nselected_eigenvectors <- sorted_eigenvectors[, 1:num_components]\npca_result <- scaled_data %*% selected_eigenvectors\ncolnames(pca_result) <- paste0(\"PC\", 1:ncol(selected_eigenvectors))\npca_result##              PC1         PC2        PC3\n##  [1,] -0.6128366 -0.94184573  0.7843772\n##  [2,]  0.2125032 -0.05948164  0.3519961\n##  [3,] -1.3435184  0.61842786 -0.8278895\n##  [4,] -0.1211029 -0.23998416 -0.2087128\n##  [5,]  0.3013377 -0.12851951 -0.6961855\n##  [6,] -2.6508325  0.24556160 -0.1455235\n##  [7,]  0.3550169  1.26209896  0.6016293\n##  [8,]  2.4199227 -0.56603968 -0.7569218\n##  [9,] -0.2708638 -1.09727813  0.4159689\n## [10,]  1.7103737  0.90706045  0.4812617"},{"path":"principal-component-analysis.html","id":"pca-by-r-function","chapter":"15 Principal component analysis","heading":"15.3 PCA by R function","text":"ท่านไม่จำเป็นต้องคำนวณ PCA ตามทั้งหมดที่กล่าวมาขั้นต้น เนื่องจาก R ได้มีฟังก์ชันสำหรับวิเคราะห์ PCA ไว้ให้แล้ว ในที่นี้จะลองสร้างสถานการณ์ที่ Features > Observationsrotation คือ Eigenvectors ที่คำนวณจาก eigenrotation คือ Eigenvectors ที่คำนวณจาก eigensdev คือ รากที่สองของ Eigenvalues ที่คำนวณจาก eigensdev คือ รากที่สองของ Eigenvalues ที่คำนวณจาก eigenx คือ ข้อมูลสุดท้ายที่ถูกลดมิติแล้วx คือ ข้อมูลสุดท้ายที่ถูกลดมิติแล้วสังเกตว่าสุดท้าย PC จะไม่เกินจำนวน Observations = 5","code":"\nset.seed(123)\n\ndata <- matrix(\n  sapply(1:10, \\(x) rnorm(5, mean = sample(1:20, 1), sd = sample(1:10, 1))),\n  ncol = 10\n)\ncolnames(data) <- LETTERS[1:10]\n\nas.data.frame(data)\npc <- prcomp(data, scale. = TRUE, center = TRUE)\nstr(pc)## List of 5\n##  $ sdev    : num [1:5] 2.03 1.88 1.45 4.94e-01 2.83e-16\n##  $ rotation: num [1:10, 1:5] -0.196 -0.424 -0.336 -0.274 0.267 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:10] \"A\" \"B\" \"C\" \"D\" ...\n##   .. ..$ : chr [1:5] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n##  $ center  : Named num [1:10] 15.31 8.31 10.98 8.02 11.69 ...\n##   ..- attr(*, \"names\")= chr [1:10] \"A\" \"B\" \"C\" \"D\" ...\n##  $ scale   : Named num [1:10] 3.61 2.92 8.36 6.37 5.36 ...\n##   ..- attr(*, \"names\")= chr [1:10] \"A\" \"B\" \"C\" \"D\" ...\n##  $ x       : num [1:5, 1:5] -1.242 -0.599 -2.375 1.895 2.322 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr [1:5] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n##  - attr(*, \"class\")= chr \"prcomp\"\npc$x##             PC1          PC2        PC3          PC4           PC5\n## [1,] -1.2422037 -2.971150049 -0.8164442 -0.009920526  3.701466e-16\n## [2,] -0.5994732  2.075884300 -1.9908362 -0.039263516  4.440892e-16\n## [3,] -2.3753407  0.963616858  1.7962551  0.097225061 -2.220446e-16\n## [4,]  1.8951608 -0.003007043  0.6758462 -0.718606015  3.330669e-16\n## [5,]  2.3218568 -0.065344065  0.3351791  0.670564995  2.220446e-16"},{"path":"principal-component-analysis.html","id":"biplot","chapter":"15 Principal component analysis","heading":"15.3.1 Biplot","text":"คือการพล็อตกราฟเพื่อบ่งบอก ว่า แต่ละ Features ดั้งเดิมนั้น มีผลต่อ PC มากน้อยเพียงใดโดยทิศทางที่ชี้ไปนั้นคืออิทธิพลของ Features ดั้งเดิมที่ประกอบเป็น PC","code":"\nbiplot(pc)"},{"path":"principal-component-analysis.html","id":"screeplot","chapter":"15 Principal component analysis","heading":"15.3.2 Screeplot","text":"เป็นการพล็อตเพื่อสำรวจว่าแต่ละ PC นั้นมีอิทธิผลจากข้อมูลมากเท่าใด ซึ่งก็คือการดูขนาดของ Eigenvalues หรือ ความแปรปรวนของ PC นั้นๆหรือท่านอาจจะเรียก screeplot()สังเกตว่า PC จะเรียงจากมากไปน้อยเสมอ ตามขนาดของ Eigenvalue","code":"\nggplot(data = NULL, aes(x = 1:5)) + \n  geom_col(aes(y = pc$sdev^2),fill = \"skyblue\", col = \"black\") +\n  geom_line(aes(y = cumsum(pc$sdev^2))) +\n  geom_text(aes(y = pc$sdev^2, \n                label = paste0(round(pc$sdev^2,2),\"%\"), vjust = -2)) +\n  labs(x = \"PC\", y = \"Variance (%)\") +\n  theme_bw()\nscreeplot(pc)"},{"path":"bioconductor.html","id":"bioconductor","chapter":"16 Bioconductor","heading":"16 Bioconductor","text":"Bioconductor คือกลุ่มของ Open-source package ที่ใช้ในการวิเคราะห์ข้อมูลประเภท Bioinformatics ที่ครอบคลุมหลากหลายมากที่สุดใน R ซึ่งจุดเด่นหลักของ Bioconductor นั้นคือการวิเคราะห์ข้อมูลประเภท High-throughout technology เช่นSummarizedExperiment, Biobase สำหรับการเก็บข้อมูลSummarizedExperiment, Biobase สำหรับการเก็บข้อมูลlimma สำหรับการวิเคราะห์ข้อมูล RNA microarray, RNA-seq, Proteomicslimma สำหรับการวิเคราะห์ข้อมูล RNA microarray, RNA-seq, ProteomicsedgeR, DESeq2 สำหรับการวิเคราะห์ข้อมูล RNA-seqedgeR, DESeq2 สำหรับการวิเคราะห์ข้อมูล RNA-seqmaftools สำหรับ สำหรับการวิเคราะห์ข้อมูล Genomicsmaftools สำหรับ สำหรับการวิเคราะห์ข้อมูล GenomicsclusterProfiler สำหรับ Functional analysisclusterProfiler สำหรับ Functional analysisComplexHeatmap, EnhancedVolcano, PCAtools สำหรับ VisualizationComplexHeatmap, EnhancedVolcano, PCAtools สำหรับ Visualizationและอื่นๆ อีกมากสามารถดูได้ที่ https://www.bioconductor.org/packages/release/bioc/การติดตั้ง package ต่างๆ จาก Bioconductor ลงใน R นั้น จำเป็นเรียกจากชุดติดตั้งของ Bioconductor โดยเฉพาะ ชื่อว่า BiocManagerหลังจากนั้น จะสามารถติดตั้ง package ได้โดยการเรียกชุดติดตั้ง BiocManager เช่นจะทำการติดตั้ง package limma ลงใน R ซึ่งหลังจากนั้นสามารถใช้คำสั่ง library เรียกได้ตามปกติ","code":"\nif (!require(\"BiocManager\", quietly = TRUE)) # ตรวจสอบว่ามีติดตั้งไว้แล้วหรือไม่\n    install.packages(\"BiocManager\")\nBiocManager::install(\"limma\")"},{"path":"limma.html","id":"limma","chapter":"17 Limma","heading":"17 Limma","text":"","code":""},{"path":"limma.html","id":"principle-2","chapter":"17 Limma","heading":"17.1 Principle","text":"Limma (Linear Model Microarray Data) (Smyth et al., 2023) เป็นหนึ่งใน bioconductor package ที่นิยมใช้อย่างแพร่หลายในการศึกษา ความแตกต่างของการแสดงออกของยีนระหว่างกลุ่มสองกลุ่ม (Differential gene expression; DGE) ในงานทดลอง Microarrayหลักการสำคัญของ limma คือการใช้สมการถดถอยเชิงเส้นในการหา DGE โดยมีสมมติฐานเบื้องต้นว่าค่าความเข้มพื้นหลังที่อ่านได้จาก Probe ของ Microarray นั้นมีการกระจายตัวแบบ Normal distribution ซึ่งหลักการนี้สามารถนำไปปรับใช้ในการหา Differential expression ในงานประเภทอื่นๆ ได้ด้วย เช่น RNA-seq, Proteomics\\(H_{0}\\): ไม่มีความแตกต่างกันในการแสดงออกของยีน\\(H_{0}\\): ไม่มีความแตกต่างกันในการแสดงออกของยีน\\(H_{}\\): มีความแตกต่างกันในการแสดงออกของยีน\\(H_{}\\): มีความแตกต่างกันในการแสดงออกของยีนจากกราฟ เส้นประคือเส้นของ \\(H_{0}\\) หมายถึงการแสดงออกของยีนทั้งหมดสามารถหาได้โดยใช้ค่าเฉลี่ยโดยรวมของทั้งสองกลุ่ม \\(\\text{Intensity} = \\theta_{0} + \\epsilon\\) และ เส้นทึบคือเส้นของ \\(H_{}\\) หมายคือเส้นสมการถดถอยที่ลากผ่านค่าเฉลี่ยของแต่ละกลุ่ม \\(\\text{Intensity }= \\theta_{0} + \\theta_{1}*\\text{Group} + \\epsilon\\) ซึ่งทั้งหมดนี้ ก็คือเส้นสมการถดถอยเชิงเส้นตามปกติโดยทั่วไปนั่นเอง โดย limma ซึ่งความแตกต่างตามสมมติฐานนั้น จะถูกทดสอบ โดย \\(t\\)-test (\\(F\\)-test ถ้ามากกว่าสองกลุ่ม)คำถามคือ วิธีการของ limma แตกต่างอย่างไรกับการสร้างสมการถดถอยโดยทั่วไป? เนื่องจากงานประเภท Bioinformatics นั้นมักมีค่าใช้จ่ายในแต่ละการทดลองสูง จำนวนตัวอย่างที่นำมาใช้ในการทดลองนั้นมีไม่มากนัก เช่น 3-4 ตัวอย่างต่อกลุ่ม\\[\nt = \\frac{\\theta_{1}-0}{SE} = \\frac{\\theta_{1}}{S/\\sqrt{n}}\n\\]ซึ่งเมื่อพิจารณาจากสูตรแล้ว จะเห็นว่าจำนวนตัวอย่างที่ต่ำ ส่งผลให้ \\(SE\\) สูง มีค่า \\(t\\) ที่แกว่ง (ตาม \\(S\\)) และมี Power ที่ต่ำด้วย เนื่องจากมีกลุ่มตัวอย่างน้อยผู้นิพนธ์ limma นั้นได้แก้ปัญหานี้โดยใช้หลักการสถิติแบบ Empirical Bayes ชื่อว่า Moderated \\(t\\)-test โดยหลักการคือ เนื่องจากหลักการของการอ่านข้อมูลใน Microarray ในแต่ละยีนนั้นเป็นไปโดยพร้อมกัน อัตราส่วนความแปรปรวนของยีนแต่ละตัวที่อ่านค่าได้นั้นจะเท่าๆ กันตลอดทั้ง array ดังนั้นจึงสามารถยืมข้อมูลของความแปรปรวนมาจากทุกยีนทั้งหมด มาสร้างเป็น ความน่าจะเป็นก่อนหน้า (Prior probability) เพื่อนำมาถ่วงน้ำหนักกับยีนแต่ละตัว (Posterior probability) ภายหลังค่า mean variance นี้จะถูกนำไปถ่วงน้ำหนักความแปรปรวนเดิมของแต่ละยีน และจะถูกนำไปใช้ในการคำนวณใหม่ เรียกว่า moderated \\(t\\)\\[\n\\tilde{t_{gj}} = \\sqrt\\frac{d_{0} + d_{g}}{d_{g}}\\times\\frac{\\hat{\\beta_{gj}}}{\\sqrt{s^{2}_{*,g}/n_{gj}}}\n\\]\\[\ns^{2}_{*,g} = s^{2}_{g}+(\\frac{d_{0}}{d_{g}})s_{0}^{2}\n\\]สังเกตว่า ค่า \\(\\tilde{t}\\) นั้นจะถูก Moderated ให้เข้าสู่ \\(d_{0}\\) (Prior degree freedom) และ \\(s_{0}\\) (Prior variance) ส่งผลให้ถ้า \\(d_{g}\\) (Degree freedom แต่ละยีน) > \\(d_{0}\\) และ \\(s_{g}\\) (ความแปรปรวนแต่ละยีน) > \\(s_{0}\\): \\(d\\) และ \\(s^{2}\\) โดยรวมจะถูกดึงให้เพิ่มขึ้นถ้า \\(d_{g}\\) (Degree freedom แต่ละยีน) > \\(d_{0}\\) และ \\(s_{g}\\) (ความแปรปรวนแต่ละยีน) > \\(s_{0}\\): \\(d\\) และ \\(s^{2}\\) โดยรวมจะถูกดึงให้เพิ่มขึ้นถ้า \\(d_{g}\\) < \\(d_{0}\\) และ \\(s_{g}\\) < \\(s_{0}\\): \\(d\\) และ \\(s^{2}\\) โดยรวมจะถูกดึงให้ลดลงถ้า \\(d_{g}\\) < \\(d_{0}\\) และ \\(s_{g}\\) < \\(s_{0}\\): \\(d\\) และ \\(s^{2}\\) โดยรวมจะถูกดึงให้ลดลงถ้า \\(d_{g}\\) = \\(d_{0}\\) และ \\(s_{g}\\) = \\(s_{0}\\): \\(d\\) จะเพิ่มขึ้นในอัตราส่วนคงที่ (\\(\\sqrt{2}\\)) และ \\(s^{2}\\) โดยรวมจะเท่าเดิมถ้า \\(d_{g}\\) = \\(d_{0}\\) และ \\(s_{g}\\) = \\(s_{0}\\): \\(d\\) จะเพิ่มขึ้นในอัตราส่วนคงที่ (\\(\\sqrt{2}\\)) และ \\(s^{2}\\) โดยรวมจะเท่าเดิมทั้งหมดนี้จะส่งผลให้ มี Moderate effect ของการคำนวณค่า \\(\\tilde{t}\\) และมี Degree freedom ที่เพิ่มขึ้นส่งผลให้ Power สูงขึ้น (เพราะการกระจายตัวแคบลง)","code":"\ncx_nc <- GSE63514 |> \n  as.data.frame() |> \n  dplyr::select(contains(c(\"Normal\", \"Cancer\")))\ncx_var <- apply(cx_nc, 1, var)\n\nggplot(data = NULL, aes(x = cx_var)) + \n  geom_density(fill = \"skyblue\") +\n  scale_x_continuous(breaks = seq(0,3,0.25), limits = c(0,3)) +\n  annotate(\"text\", x = mean(cx_var)+0.35, y = 4, label= \"Mean variance\") +\n  labs(x = \"Variance\") +\n  geom_vline(xintercept = mean(cx_var), linetype = \"dashed\") +\n  theme_bw() "},{"path":"limma.html","id":"example","chapter":"17 Limma","heading":"17.2 Example","text":"ต่อจากนี้จะยกตัวอย่างการคำนวณ DGE ระหว่างกลุ่มโดยใช้ limma ต่อจาก GSE63514 ซึ่งครั้งนี้จะเทียบทุกกลุ่ม และเทียบทุกยีนก่อนอื่นจะต้องสร้างสมการเชิงเส้นของกลุ่มแต่ละกลุ่มขึ้นมาก่อนแมทริกซ์ที่เห็นนี้ ข้อมูลของ Sample แต่ละคน โดยแต่ละคอลัมน์จะเป็นตัวแทนของกลุ่มของ Sample นั้น เช่น คนที่ 1 จะมีตัวเลข [0 0 0 0 1] บ่งบอกว่าเป็นชิ้นเนื้อประเภท Cancer และคนสุดท้ายเป็น [1 0 0 0 0] บ่งบอกว่าชิ้นเนื้อประเภท Normalต่อจากนั้นเราจะสร้าง contrast matrix ขึ้นมาเพื่อนำไปใช้อ้างอิงในการเทียบ DEGสังเกตว่าแต่ละคอลัมน์ผลรวมจะเท่ากับ 0 เนื่องจาก \\(H_{0}\\) ว่าไม่มีความแตกต่างกันในการแสดงออกของยีน (= 0) นั่นเองเมื่อได้ design matrix ต่อไปคือการ fit linear regression ตาม model โดยข้อมูลจะต้องอยู่ในรูปตัวเลขทั้งหมด ดังนั้นจะต้องปรับแต่ง ข้อมูล expression เริ่มต้นเล็กน้อยต่อจากนั้นจะ fit model ด้วย lmFit() และเทียบแต่ละกลุ่มด้วย contrasts.fit()สุดท้ายเราจะทำการคำนวณ Moderated t-test โดยใช้ eBayes()สุดท้ายคือการแสดงผล DGE จากการคำนวณทั้งหมดโดยใช้ topTable() ซึ่งจะแสดง Moderated \\(F\\)-test (ANOVA) และ \\(p\\)-value, Adjusted \\(p\\)-valueและสามารถเรียกดู \\(t\\)-test แต่ละกลุ่มได้โดยการระบุ coefโดย logFC คือ Log2 fold-change ของ Gene expression แต่ละตัวท่านสามารถสรุปข้อมูล DGE ได้โดยใช้ decideTests()","code":"\npData <- GSE63514_meta |> \n  select(title) |> \n  mutate(group = gsub(\"-\\\\d+\", \"\", title))\nunique(pData$group)## [1] \"Normal\" \"CIN1\"   \"CIN2\"   \"CIN3\"   \"Cancer\"\ncx_mod <- model.matrix(~ 0 + group, data = pData)\nas.data.frame(cx_mod) # Display as dataframe\nlibrary(limma)\ncontrasts <- apply(combn(colnames(cx_mod),2)[2:1,], 2, paste, collapse = \"-\")\ncontrasts##  [1] \"groupCIN1-groupCancer\"   \"groupCIN2-groupCancer\"   \"groupCIN3-groupCancer\"  \n##  [4] \"groupNormal-groupCancer\" \"groupCIN2-groupCIN1\"     \"groupCIN3-groupCIN1\"    \n##  [7] \"groupNormal-groupCIN1\"   \"groupCIN3-groupCIN2\"     \"groupNormal-groupCIN2\"  \n## [10] \"groupNormal-groupCIN3\"\ncontrasts.matrix <- makeContrasts(contrasts = contrasts, levels = colnames(cx_mod)) \ncontrasts.matrix##              Contrasts\n## Levels        groupCIN1-groupCancer groupCIN2-groupCancer groupCIN3-groupCancer\n##   groupCancer                    -1                    -1                    -1\n##   groupCIN1                       1                     0                     0\n##   groupCIN2                       0                     1                     0\n##   groupCIN3                       0                     0                     1\n##   groupNormal                     0                     0                     0\n##              Contrasts\n## Levels        groupNormal-groupCancer groupCIN2-groupCIN1 groupCIN3-groupCIN1 groupNormal-groupCIN1\n##   groupCancer                      -1                   0                   0                     0\n##   groupCIN1                         0                  -1                  -1                    -1\n##   groupCIN2                         0                   1                   0                     0\n##   groupCIN3                         0                   0                   1                     0\n##   groupNormal                       1                   0                   0                     1\n##              Contrasts\n## Levels        groupCIN3-groupCIN2 groupNormal-groupCIN2 groupNormal-groupCIN3\n##   groupCancer                   0                     0                     0\n##   groupCIN1                     0                     0                     0\n##   groupCIN2                    -1                    -1                     0\n##   groupCIN3                     1                     0                    -1\n##   groupNormal                   0                     1                     1\nGSE63514_fixed <- GSE63514 |> \n                    column_to_rownames(\"prob\")\ncx_fit <- lmFit(GSE63514_fixed, design = cx_mod)\ncx_fit_contrasts <- contrasts.fit(cx_fit, contrasts.matrix)\ncx_fit_contrasts_eB <- eBayes(cx_fit_contrasts)\ntopTable(cx_fit_contrasts_eB, n = 100) # First 100 genes\ntopTable(cx_fit_contrasts_eB, coef = 1, n = 100) # เรียงตาม contrasts.matrix\nmap(1:length(colnames(contrasts.matrix)), \n    ~topTable(cx_fit_contrasts_eB, coef = .x)) |> \n  set_names(colnames(contrasts.matrix))## $`groupCIN1-groupCancer`\n##                  logFC   AveExpr         t      P.Value    adj.P.Val        B\n## 206025_s_at -1.2599869  6.600127 -7.954562 8.906110e-13 4.869415e-08 18.36296\n## 222835_at    2.7037467 11.390512  7.656871 4.369597e-12 9.910994e-08 16.87732\n## 232855_at    1.9511837  7.464893  7.581694 6.508984e-12 9.910994e-08 16.50502\n## 220090_at    5.1219007 11.675811  7.561290 7.250842e-12 9.910994e-08 16.40418\n## 205185_at    3.6701214 12.344023  7.312213 2.685207e-11 2.682614e-07 15.18098\n## 235651_at    2.5105300 11.627430  7.294601 2.943884e-11 2.682614e-07 15.09504\n## 203857_s_at -1.3379967  8.865472 -7.135465 6.732460e-11 5.258532e-07 14.32218\n## 226506_at    2.2236684 10.612391  6.929967 1.938536e-10 1.324868e-06 13.33417\n## 207821_s_at -0.8531452  7.215208 -6.723744 5.531008e-10 3.360087e-06 12.35491\n## 40016_g_at   1.6956215 11.063432  6.637076 8.558517e-10 4.077549e-06 11.94725\n## \n## $`groupCIN2-groupCancer`\n##                 logFC   AveExpr          t      P.Value    adj.P.Val        B\n## 206025_s_at -1.380196  6.600127 -10.011010 1.030733e-17 5.635535e-13 29.24779\n## 210495_x_at -2.715234 10.074149  -8.851861 6.668495e-15 1.617849e-10 23.13848\n## 216442_x_at -2.764813  9.747865  -8.800039 8.877086e-15 1.617849e-10 22.86809\n## 205464_at    2.458379  9.030410   8.665389 1.863583e-14 2.435352e-10 22.16703\n## 218468_s_at -2.659120  5.349137  -8.625147 2.324822e-14 2.435352e-10 21.95795\n## 219597_s_at  1.847612 10.636695   8.599759 2.672540e-14 2.435352e-10 21.82616\n## 211719_x_at -2.978875  9.677495  -8.468114 5.497177e-14 3.537199e-10 21.14420\n## 227140_at   -3.349588  6.795510  -8.460792 5.721706e-14 3.537199e-10 21.10635\n## 212464_s_at -2.941674  9.149103  -8.457596 5.822549e-14 3.537199e-10 21.08983\n## 220090_at    4.972371 11.675811   8.433644 6.636766e-14 3.628652e-10 20.96605\n## \n## $`groupCIN3-groupCancer`\n##                  logFC   AveExpr          t      P.Value    adj.P.Val        B\n## 206025_s_at -1.3149204  6.600127 -11.027724 3.311442e-20 1.810531e-15 34.44112\n## 218468_s_at -2.5804213  5.349137  -9.677614 6.709207e-17 1.834129e-12 27.33557\n## 227566_at   -1.2024603  6.615340  -8.972807 3.415686e-15 5.539439e-11 23.65417\n## 206026_s_at -1.3312089  6.219060  -8.941927 4.052630e-15 5.539439e-11 23.49384\n## 219597_s_at  1.6246901 10.636695   8.743700 1.211057e-14 1.324291e-10 22.46704\n## 210495_x_at -2.2247812 10.074149  -8.386172 8.599897e-14 7.836656e-10 20.62733\n## 205464_at    2.0342073  9.030410   8.290557 1.447645e-13 1.130714e-09 20.13838\n## 216442_x_at -2.2269918  9.747865  -8.195711 2.422825e-13 1.655849e-09 19.65479\n## 212464_s_at -2.4481475  9.149103  -8.138402 3.304613e-13 1.950668e-09 19.36330\n## 213434_at   -0.7553377  8.747262  -8.124238 3.567751e-13 1.950668e-09 19.29135\n## \n## $`groupNormal-groupCancer`\n##                 logFC   AveExpr          t      P.Value    adj.P.Val        B\n## 232855_at    2.353804  7.464893  10.762249 1.485735e-19 8.123254e-15 33.65727\n## 206025_s_at -1.407391  6.600127 -10.455133 8.425180e-19 2.303233e-14 31.99032\n## 207802_at    6.508949  7.727577  10.008609 1.044759e-17 1.904073e-13 29.57022\n## 205464_at    2.730049  9.030410   9.855695 2.468951e-17 2.847392e-13 28.74320\n## 212621_at   -1.668245  9.041825  -9.816828 3.071485e-17 2.847392e-13 28.53318\n## 226506_at    2.672522 10.612391   9.800471 3.367040e-17 2.847392e-13 28.44482\n## 210262_at    3.001186  6.730272   9.786321 3.645495e-17 2.847392e-13 28.36840\n## 214431_at   -1.402298 10.718458  -9.638181 8.369057e-17 5.086653e-13 27.56901\n## 202055_at   -1.184061  9.912045  -9.638095 8.373092e-17 5.086653e-13 27.56855\n## 222835_at    2.873757 11.390512   9.576350 1.183340e-16 6.469909e-13 27.23578\n## \n## $`groupCIN2-groupCIN1`\n##                  logFC  AveExpr         t      P.Value adj.P.Val         B\n## 217575_s_at -0.2603589 4.108856 -4.119514 6.827838e-05 0.9999659 -1.555995\n## 219450_at    1.3974047 8.021937  4.108238 7.128229e-05 0.9999659 -1.572421\n## 204779_s_at  1.0783374 9.084288  3.979722 1.157851e-04 0.9999659 -1.757629\n## 214611_at   -0.3799563 5.371863 -3.974482 1.180717e-04 0.9999659 -1.765101\n## 219358_s_at -0.5104374 7.798451 -3.972613 1.188977e-04 0.9999659 -1.767765\n## 216973_s_at  0.9522317 9.119675  3.794046 2.290469e-04 0.9999659 -2.018449\n## 242979_at   -1.3010652 7.799901 -3.791582 2.310954e-04 0.9999659 -2.021855\n## 228152_s_at -1.1207177 8.651740 -3.777430 2.431998e-04 0.9999659 -2.041386\n## 235350_at    1.5458201 7.528341  3.714830 3.043333e-04 0.9999659 -2.127184\n## 238175_at   -0.3896401 4.538269 -3.696149 3.252317e-04 0.9999659 -2.152598\n## \n## $`groupCIN3-groupCIN1`\n##                  logFC   AveExpr         t      P.Value  adj.P.Val        B\n## 204603_at    0.7024178  8.164955  5.356714 3.887100e-07 0.01455424 5.999781\n## 204775_at    0.6352418  7.355724  5.222561 7.081541e-07 0.01455424 5.472794\n## 202954_at    0.8500242 10.181699  5.147294 9.876396e-07 0.01455424 5.180686\n## 209054_s_at  0.7012696  9.439562  5.130188 1.064782e-06 0.01455424 5.114663\n## 225784_s_at -0.5082652  7.521747 -5.022461 1.703992e-06 0.01765170 4.702034\n## 226308_at    0.8984545  7.797713  4.964062 2.193210e-06 0.01765170 4.480667\n## 209053_s_at  0.7712176  8.902046  4.936377 2.470436e-06 0.01765170 4.376302\n## 219490_s_at  0.5818629  8.117920  4.926008 2.582782e-06 0.01765170 4.337314\n## 202183_s_at  0.8829252  8.037612  4.885240 3.074616e-06 0.01867829 4.184526\n## 226456_at    1.0098740 10.334927  4.802377 4.369883e-06 0.02274771 3.876535\n## \n## $`groupNormal-groupCIN1`\n##                  logFC   AveExpr         t      P.Value  adj.P.Val          B\n## 210262_at    1.8942513  6.730272  5.109272 1.167105e-06 0.06381149  0.1414762\n## 226702_at   -1.8232496  9.133420 -4.447203 1.888683e-05 0.51631880 -0.9456764\n## 219258_at   -0.9936006  8.488497 -4.236185 4.353151e-05 0.59048057 -1.2746881\n## 217905_at   -0.5415615  8.950872 -4.227458 4.503515e-05 0.59048057 -1.2880906\n## 229450_at   -1.6129526  9.183332 -4.170784 5.608285e-05 0.59048057 -1.3747156\n## 209969_s_at -1.4138293 10.354201 -4.133184 6.479897e-05 0.59048057 -1.4317897\n## 204510_at   -1.0598884  9.627055 -4.069143 8.270786e-05 0.64600748 -1.5282574\n## 227609_at   -1.1735956  9.637705 -4.001474 1.067367e-04 0.70559370 -1.6291527\n## 210766_s_at -0.6423773 11.534705 -3.978886 1.161471e-04 0.70559370 -1.6625908\n## 204994_at   -1.1295418 10.139366 -3.914339 1.475994e-04 0.80699981 -1.7574641\n## \n## $`groupCIN3-groupCIN2`\n##                  logFC   AveExpr         t      P.Value adj.P.Val           B\n## 31845_at     0.5943656  9.673283  4.745089 5.560039e-06 0.1145089  1.57530720\n## 214858_at   -0.8133322  5.396801 -4.732526 5.860234e-06 0.1145089  1.54352542\n## 232222_at    0.3363060  6.001814  4.715848 6.283067e-06 0.1145089  1.50141268\n## 241014_at   -1.1644315  6.254372 -4.243388 4.232694e-05 0.5785564  0.34663636\n## 205968_at    0.8210918  9.169976  4.159667 5.853537e-05 0.6236596  0.15019705\n## 220040_x_at -0.3462818  7.659149 -4.061818 8.503499e-05 0.6236596 -0.07605645\n## 229782_at   -0.7239349  6.901729 -3.981172 1.151596e-04 0.6236596 -0.25975588\n## 1563881_at  -0.7137692  4.397457 -3.971422 1.194271e-04 0.6236596 -0.28179416\n## 200060_s_at  0.3245884 11.538188  3.970706 1.197462e-04 0.6236596 -0.28341066\n## 236010_at   -0.5131600  5.533651 -3.936483 1.359910e-04 0.6236596 -0.36045186\n## \n## $`groupNormal-groupCIN2`\n##                 logFC   AveExpr         t      P.Value    adj.P.Val        B\n## 223556_at   -1.445308  9.156569 -7.093800 8.350796e-11 4.565797e-06 13.83546\n## 235609_at   -1.432506  9.066440 -6.867548 2.666254e-10 7.288873e-06 12.78063\n## 201930_at   -1.280896 11.792626 -6.667073 7.360367e-10 1.262858e-05 11.85781\n## 218039_at   -1.450594 11.879667 -6.597757 1.042446e-09 1.262858e-05 11.54149\n## 218585_s_at -1.642935 10.431612 -6.577297 1.154877e-09 1.262858e-05 11.44841\n## 205909_at   -1.310771  9.479550 -6.532028 1.447888e-09 1.319388e-05 11.24291\n## 225655_at   -1.473806 10.563237 -6.484394 1.835405e-09 1.366560e-05 11.02737\n## 226456_at   -1.292656 10.334927 -6.467152 1.999539e-09 1.366560e-05 10.94953\n## 204026_s_at -1.136500 11.876375 -6.437869 2.312056e-09 1.404574e-05 10.81756\n## 228273_at   -1.497697  9.925345 -6.403594 2.739372e-09 1.497751e-05 10.66344\n## \n## $`groupNormal-groupCIN3`\n##                  logFC   AveExpr         t      P.Value    adj.P.Val        B\n## 218585_s_at -2.1455228 10.431612 -9.819011 3.034047e-17 9.812847e-13 28.38863\n## 204510_at   -1.9576326  9.627055 -9.789077 3.589519e-17 9.812847e-13 28.22861\n## 221521_s_at -1.7711864 10.294773 -9.687331 6.353396e-17 1.157906e-12 27.68508\n## 222680_s_at -1.8101779  8.533391 -9.265312 6.725115e-16 6.884628e-12 25.43791\n## 225655_at   -1.8399722 10.563237 -9.254393 7.146915e-16 6.884628e-12 25.37995\n## 226456_at   -1.6163749 10.334927 -9.244420 7.555147e-16 6.884628e-12 25.32702\n## 235609_at   -1.6711061  9.066440 -9.158340 1.219785e-15 9.527392e-12 24.87058\n## 205339_at   -1.8494642  9.227739 -9.117800 1.528068e-15 1.044339e-11 24.65586\n## 204603_at   -0.9813232  8.164955 -9.000435 2.930854e-15 1.649719e-11 24.03517\n## 218039_at   -1.7300249 11.879667 -8.995189 3.017319e-15 1.649719e-11 24.00746\nresults <- decideTests(cx_fit_contrasts_eB, fc=log2(1.5)) \nsummary(results)##        groupCIN1-groupCancer groupCIN2-groupCancer groupCIN3-groupCancer groupNormal-groupCancer\n## Down                    2221                  3818                  2402                    6317\n## NotSig                 50408                 47570                 50026                   40811\n## Up                      2046                  3287                  2247                    7547\n##        groupCIN2-groupCIN1 groupCIN3-groupCIN1 groupNormal-groupCIN1 groupCIN3-groupCIN2\n## Down                     0                  34                     0                   0\n## NotSig               54675               54548                 54675               54675\n## Up                       0                  93                     0                   0\n##        groupNormal-groupCIN2 groupNormal-groupCIN3\n## Down                     311                  2214\n## NotSig                 54333                 50612\n## Up                        31                  1849\nvennDiagram(results[,1:5]) # max 5 groups"},{"path":"limma.html","id":"mean-variance-relationship","chapter":"17 Limma","heading":"17.3 Mean-variance relationship","text":"ในบางข้อมูลนั้นมีความสัมพันธ์ระหว่างค่าเฉลี่ยและความแปรปรวน ซึ่งสามารถตรวจสอบได้โดยใช้ plotSA()จากกราฟจะพบความสัมพันธ์ระหว่าง Average log-expression และ sqrt(sigma) แปลว่ามีความสัมพันธ์ระหว่างค่าเฉลี่ยและความแปรปรวน เนื่องจาก Linear model นั้นมีสมมติฐานว่า ความแปรปรวนนั้นคงที่ตลอด (Homoscedasticity) การสร้างสมการแบบธรรมดาอาจจะทำให้เกิด False positive มากกว่าปกติ ซึ่ง limma นั้นมีฟังก์ชันที่ปรับสมการตามแนวโน้มของความแปรปรวนด้วย eBayes(..., trend = TRUE)เมื่อทำการพล็อตความสัมพันธ์ระหว่างค่าเฉลี่ยและความแปรปรวนอีกครั้ง จะพบว่าโมเดลนั้นถูกปรับตามความแปรปรวนแล้วการใช้ trend = TRUE นี้มีความสำคัญมากในการประยุกต์ใช้ limma ในการวิเคราะห์ข้อมูลแบบอื่นๆ ที่ไม่ใช่ Microarray เช่น RNA-seq, Proteomics เนื่องจากข้อมูลมักจะมีความสัมพันธ์ระหว่างค่าเฉลี่ยกับตัวแปรสูง (Heteroscedasticity)","code":"\nplotSA(cx_fit_contrasts_eB) \ncx_fit_contrasts_eB_trend <- eBayes(cx_fit_contrasts, trend = TRUE)\nresults_trend <- decideTests(cx_fit_contrasts_eB, fc = log2(1.5)) \nsummary(results_trend)##        groupCIN1-groupCancer groupCIN2-groupCancer groupCIN3-groupCancer groupNormal-groupCancer\n## Down                    2221                  3818                  2402                    6317\n## NotSig                 50408                 47570                 50026                   40811\n## Up                      2046                  3287                  2247                    7547\n##        groupCIN2-groupCIN1 groupCIN3-groupCIN1 groupNormal-groupCIN1 groupCIN3-groupCIN2\n## Down                     0                  34                     0                   0\n## NotSig               54675               54548                 54675               54675\n## Up                       0                  93                     0                   0\n##        groupNormal-groupCIN2 groupNormal-groupCIN3\n## Down                     311                  2214\n## NotSig                 54333                 50612\n## Up                        31                  1849\nplotSA(cx_fit_contrasts_eB_trend)"},{"path":"edger.html","id":"edger","chapter":"18 edgeR","heading":"18 edgeR","text":"ผู้นิพนธ์ร่วม: นพ. พงศกร ชูชื่น","code":""},{"path":"edger.html","id":"principle-3","chapter":"18 edgeR","heading":"18.1 Principle","text":"edgeR (Empirical Analysis Gene Expression R) (Chen et al., 2023) เป็น bioconductor package ที่นิยมใช้ในการศึกษา DGE ในการทดลอง RNA-seq ซึ่งตั้งอยู่บนสมมติฐานที่แตกต่างจาก limma เนื่องจากข้อมูล RNA-seq นั้นมีคุณสมบัติที่แตกต่างจากข้อมูลประเภท Microarray ดังนี้log(intensity) ของ Microarray ซึ่งเป็นข้อมูล Continuous จึงสามารถใช้โมเดลแบบ Linear ได้ แต่ ข้อมูล RNA-seq นั้นเป็นข้อมูล read count ที่ได้จากการนับ Sequencing จึงเหมาะกับการใช้โมเดลแบบ Poisson มากกว่าlog(intensity) ของ Microarray ซึ่งเป็นข้อมูล Continuous จึงสามารถใช้โมเดลแบบ Linear ได้ แต่ ข้อมูล RNA-seq นั้นเป็นข้อมูล read count ที่ได้จากการนับ Sequencing จึงเหมาะกับการใช้โมเดลแบบ Poisson มากกว่าHeteroscedasticity สูง จึงไม่สามารถใช้สมการถดถอยแบบ Poisson โดยปกติได้Heteroscedasticity สูง จึงไม่สามารถใช้สมการถดถอยแบบ Poisson โดยปกติได้กราฟซ้าย คือความสัมพันธ์ของค่าเฉลี่ยและความแปรปรวนของการกระจายตัวแบบ Poisson โดยทั่วไป ส่วนกราฟขวา เป็นการกระจายตัวของข้อมูล RNA-seq จะเห็นว่าเมื่อค่าเฉลี่ยเพิ่มมากขึ้นนั้น ความแปรปรวนจะเพิ่มในอัตราส่วนที่มากกว่า ทั้งนี้ เนื่องจาก มีปัจจัยของความแปรปรวนที่ไม่ใช่ความแปรปรวนของ Technical variability จากกระบวนการ RNA sequencing (ซึ่งเป็น การกระจายตัวแบบ Poisson) เพียงอย่างเดียว แต่มีความแปรปรวนของ Biological properties ในตัวอย่างมาเกี่ยวข้องด้วย","code":""},{"path":"edger.html","id":"negative-binomial-model","chapter":"18 edgeR","heading":"18.1.1 Negative binomial model","text":"","code":""},{"path":"edger.html","id":"handling-biological-variation","chapter":"18 edgeR","heading":"18.1.1.1 Handling biological variation","text":"จากข้อมูลขั้นต้น สมมติฐานของ edgeR คือ RNA-seq มี Technical variation ซึ่งมีการกระจายตัวแบบ Poisson โดยในสถาณการณ์ไม่มี Biological variation ณ ตัวอย่าง \\(\\) และ ยีน \\(g\\) จะมี จำนวนยีน \\(y_{gi}\\) เท่ากับผลคูณจำนวนยีนทั้งหมดในตัวอย่างนั้น (Total library size; \\(N_{}\\)) กับ อัตราส่วนของยีนนั้นๆ ต่อยีนทั้งหมดในตัวอย่าง (\\(\\pi_{gi}\\)) ทั้งหมด \\(G\\) ยีน ซึ่ง \\(\\sum_{g=1}^{G}{\\pi_{gi}} = 1\\) จะได้ว่า\\[\nE(y_{gi}) = \\mu_{gi} = N_{}\\pi_{gi}\n\\]อย่างไรก็ตามในข้อมูล RNA-seq นั้นมีส่วนของ Biological variation ในแต่ละตัวอย่างอยู่ ซึ่งทำให้เกิดความแปรปรวนที่นอกเหนือจาก Poisson (Extra-poisson variability) ส่งผลให้เกิดข้อมูลแบบ Overdispersion ซึ่งสมการความสัมพันธ์ระหว่างความแปรปรวนกับค่าเฉลี่ยของ Mixed-poisson distribution คือ\\[\nvar(y_{gi}) = E_{\\pi}[var(y|\\pi)] + var_{\\pi}[E(y|\\pi)] = \\mu_{gi} + \\phi_{g}\\mu^{2}_{gi}\n\\]\\[\nCV^{2} = 1/\\mu_{gi} + \\phi\n\\]โดยที่ \\(\\sqrt{1/\\mu_{gi}}\\) คือ Coefficient variation (\\(S/\\mu\\); CV) จากการกระจายตัวแบบ Poisson ส่วน \\(\\sqrt{\\phi}\\) คือ รากที่สองของ CV จากการกระบวนการอื่น ซึ่งในที่นี้คือ Biological properties ของตัว Sample ดังนั้นจะได้ว่า\\[\n\\text{Total CV}^{2} = \\text{Technical CV}^{2} + \\text{Biological CV}^{2}\n\\]สมมติฐานของ edgeR นั้นตั้งอยู่บนพื้นฐานที่ว่า Technical CV นั้นจะลดลงเมื่อจำนวน Sequencing depth นั้นเพิ่มมากขึ้น (\\(1/\\mu_{gi}\\)) แต่ Biological CV นั้นจะยังคงอยู่ไม่ว่า Sequencing depth จะเพิ่มมากขึ้นเท่าไร ดังนั้นสิ่งสำคัญที่เป็นตัวแปรก่อกวนการวัด DGE (ซึ่ง Systemic variability) คือ Biological CV (\\(\\sqrt{\\phi}\\)) นั่นเอง","code":""},{"path":"edger.html","id":"estimation-of-phi-and-dge","chapter":"18 edgeR","heading":"18.1.1.2 Estimation of \\(\\phi\\) and DGE","text":"edgeR ทำการสร้างสมการถดถอย Negative binomial จากการประเมินค่า \\(\\phi\\) โดยวิธีที่เรียกว่า Quantile-adjusted maximum likelihood (qCML) ซึ่งใช้จำนวน Read ทั้งหมดในแต่ละยีนมาสร้างเป็นตาราง Pseudo-count\\[\ny_{gi} \\sim NB(\\mu_{gi}, \\phi_{})\n\\]หลังจากนั้น edgeR จะคำนวณ DGE โดยการใช้ Exact test ซึ่งเป็นการรวมโอกาสทั้งหมดที่จะสุ่มได้ Condition ที่ต้องการ ซึ่งวิธีคล้าย Fisher’s exact test ที่มีการคำนึงถึง Effective library size และใช้ Negative binomial distribution แทน Hypergeometric distribution ซึ่งเป็นวิธี Classical method ของ edgeR อย่างไรก็ตาม วิธีนี้มีข้อจำกัดคือสามารถเปรียบเทียบ DGE ได้ระหว่างสองสภาวะเท่านั้น และไม่สามารถจำลองความแปรปรวนอื่นๆ นอกเหนือจากความแปรปรวนของยีนได้","code":""},{"path":"edger.html","id":"negative-binomial-extension","chapter":"18 edgeR","heading":"18.1.2 Negative binomial extension","text":"นอกจาก global BCV dispersion แล้ว edgeR ยังสามารถคำนวณ BCV ของแต่ละยีนได้โดยใช้ Quasi-likelihood ของสมการถดถอย Negative binomial\\[\nvar(y_{gi}) = \\sigma^{2}_{g}(\\mu_{gi} + \\phi\\mu_{gi}^{2})\n\\]ซึ่งหมายความว่า ทุกๆ ความแปรปรวนของ \\(var(y_{gi})\\) ที่เพิ่มขึ้นจะถูกทำนายด้วยค่า \\(\\phi\\) (Negative binomial parameter; Global BCV) และ \\(\\sigma^{2}\\) (Quasi-likelihood parameter; Gene-specific BCV) ซึ่งปัญหาของสมการนี้คือจำนวณ Replicate ที่ไม่มากพอในแต่ละยีน edgeR แก้จึงปัญหานี้โดยประยุกต์เทคนิกของ limma มาใช้ ซึ่งก็คือ Empirical Bayes ซึ่งยืมความแปรปรวนมาจากทุกยีนนั่นเอง","code":""},{"path":"edger.html","id":"log-linear-model","chapter":"18 edgeR","heading":"18.1.3 Log-linear model","text":"สำหรับข้อมูลที่มีตัวแปรมากกว่าแค่การแสดงออกของยีน หรือมีมากกว่า 2 กลุ่มขึ้นไป จะไม่สามารถประเมิน \\(\\phi\\) โดยใช้วิธี qCML ได้ edgeR จึงทำการประเมิน \\(\\mu_{gi}\\) ด้วยสมการถดถอยเชิงเส้น ซึ่งทำให้สามารถประเมิน \\(\\mu_{gi}\\) ร่วมกับตัวแปรอื่นๆ ได้\\[\nlog(\\mu_{gi}) = \\theta_{g_{0}} + \\theta_{g_{1}}x_{i_{1}} + \\ ... \\ + \\theta_{g_{n}}x_{i_{n}} + log(N_{}) \\rightarrow \\text{offset}\n\\]\\[\ny_{gi} \\sim NB(\\mu_{gi}, \\phi_{})\n\\]โดยในสมการนี้จะประเมิน \\(\\phi\\) ด้วยวิธี Cox-Reid profile-adjusted likelihood (cPAL) ซึ่งสามารถจัดการกับข้อมูลแบบ Multivariate ได้ ซึ่งหลังจากนั้นจะสามารถทดสอบ DGE ได้โดย \\(F\\)-test หรือ Likelihood-ratio test","code":""},{"path":"edger.html","id":"workflow","chapter":"18 edgeR","heading":"18.2 Workflow","text":"","code":""},{"path":"edger.html","id":"edgeR-ex","chapter":"18 edgeR","heading":"18.3 Example","text":"ในตัวอย่างนี้จะใช้ Bladder gene expression ระหว่าง Tumor และ Control และใช้สมการ Generalized log-linear modelก่อนอื่นเราต้องสร้าง Condition matrix ซึ่งประกอบด้วย แต่ละ Sample ที่ต้องการศึกษา และ Condition ของตัวอย่างนั้น ซึ่งในที่นี้เราจะแบ่งเป็นสองกลุ่ม ก็คือ Normal และ Tumorหลังจากนั้นจะต้องกรองเอายีนที่แสดงออกน้อยออกไป เนื่องจาก Technical variation ของ RNA-seq นั้นมีการกระจายตัวแบบ Poisson นั่นหมายความว่า ยีนที่มีการแสดงออกต่ำนั้นย่อมมี CV ที่สูง อีกทั้ง ยีนที่มีการแสดงออกต่ำนั้นมีโอกาสที่จะถูก Translate ไปเป็นโปรตีนน้อย จึงถือว่าเป็นข้อมูลที่ไม่ค่อยเป็นประโยชน์และเพิ่ม \\(\\alpha\\) error โดยใช่เหตุ จึงสมควรที่จะตัดออกไป โดยค่า Default ของ edgeR นั้นอยู่ที่ 10 counts/sample (จาก Count-per million)ต่อมา เราจะต้องทำการ Normalize ค่าการแสดงออกของ RNA เนื่องจากการ run RNA seq ในแต่ละ Sample นั้น สภาวะของเครื่องอาจจะมีความแตกต่างกัน ส่งผลให้ค่า Total read count ในแต่ละ Sample มีไม่เท่ากัน โดย edgeR จะใช้วิธี Normalize ที่เรียกว่า Trimmed mean M-values (TMM) ซึ่งใช้ค่าเฉลี่ยถ่วงน้ำหนักของการแสดงออกยีนในตัวอย่างนั้นๆ หลังจากตัด Outlier ทั้งสูงและต่ำออก (30% ของค่า Log และ 5% ของค่า Absolute)หลังจากนั้นจะทำการคำนวณ \\(\\phi\\) ถ้ามี edgeR จะใช้วิธี qCML ถ้าไม่มี Design matrix และใช้วิธี cPAL ถ้ามี Design matrixต่อไปจะทำการ ทดสอบ DGE ด้วยสมการถดถอย ql-nbglmตอนนี้จะเห็นว่า BCV ของข้อมูลนั้นถูกบีบมาให้อยู่ใน Trend line จาก Empirical Bayes (Note: edgeR เรียก Function นี้จาก package limma)สุดท้าย เราจะใช้ glmQLFTest() เพื่อวิเคราะห์ \\(p\\)-value จาก \\(F\\)-test (สามารถใช้ Likelihood-ratio test ได้ด้วย glmLRT()) และเรียก topTags() เพื่อทำการดึงตารางผลของ DGE ออกมาซึงการแปลผลท้ายสุดจะคล้ายการแปลผลของ limma","code":"\nas.data.frame(bladder_mat)\ndesign <- data.frame(\n  ID =  colnames(bladder_mat),\n  condition = gsub(\".{1}$\", \"\", colnames(bladder_mat)) # Remove number\n)\n\ndesign\nbladder_dge <- DGEList(bladder_mat, group = design$condition) \nkeep <- filterByExpr(bladder_dge, min.count = 10)\nbladder_dge <- bladder_dge[keep,]\nbladder_dge <- calcNormFactors(bladder_dge)\nbladder_dge$samples\nmodel <- model.matrix(~0+condition, data = design)\nmodel##   conditionN conditionT\n## 1          1          0\n## 2          1          0\n## 3          1          0\n## 4          0          1\n## 5          0          1\n## 6          0          1\n## attr(,\"assign\")\n## [1] 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"\nbladder_dge <- estimateDisp(bladder_dge, design = model)\nplotBCV(bladder_dge)\nfit <- glmQLFit(bladder_dge, model)\nplotQLDisp(fit)\ngenediff <- glmQLFTest(fit, contrast=c(-1,1))\ntopTags(genediff, n = 100) |> as.data.frame() # First 100 genes"},{"path":"pathway-analysis.html","id":"pathway-analysis","chapter":"19 Pathway analysis","heading":"19 Pathway analysis","text":"","code":""},{"path":"pathway-analysis.html","id":"principle-4","chapter":"19 Pathway analysis","heading":"19.1 Principle","text":"Pathway analysis คือการวิเคราะห์ข้อมูล High-throughput ให้มีความหมาย โดยการนำข้อมูลของ DNA/RNA ทั้งหมดที่ได้มาทำจัดกลุ่มร่วมกับฐานข้อมูลของยีนนั้นๆ ว่าเกี่ยวข้องกับกลไกการทำงานในระดับโมเลกุล (Molecular pathway) แบบใด","code":""},{"path":"pathway-analysis.html","id":"database-annotation","chapter":"19 Pathway analysis","heading":"19.1.1 Database annotation","text":"คือ การจัดกลุ่มของยีนให้ตรงกับกลุ่มของข้อมูล โดยปัจจุบันมีฐานข้อมูลดังนี้Gene ontology (Thomas et al., 2022) คือ ฐานข้อมูลที่รวบรวมหน้าที่ของยีนแต่ละยีนและจัดเป็นหมวดหมู่ โดยแต่ละยีนสามารถอยู่หลายหมวดหมู่ได้ ซึ่งหมวดหมู่หลักโดยหลักนั้นแบ่งเป็น\nCellular component (CC) คือยีนนั้นมักอยู่ในเซลล์ หรือสภาวะแวดล้อมรอบข้างแบบใด\nMolecular function (MF) คือ ยีนนั้นทำหน้าที่ใดบ้างในระดับโมเลกุล มักต่อท้ายด้วยคำว่า “activity”\nBiological process (BP) คือ ยีนนั้นเป็นส่วนประกอบหลักของการทำงานในระบบชีววิทยาอย่างไร ซึ่งเป็นภาพใหญ่ของหลายโมเลกุลที่มารวมตัวกันทำหน้าที่เป็นระบบ\nGene ontology (Thomas et al., 2022) คือ ฐานข้อมูลที่รวบรวมหน้าที่ของยีนแต่ละยีนและจัดเป็นหมวดหมู่ โดยแต่ละยีนสามารถอยู่หลายหมวดหมู่ได้ ซึ่งหมวดหมู่หลักโดยหลักนั้นแบ่งเป็นCellular component (CC) คือยีนนั้นมักอยู่ในเซลล์ หรือสภาวะแวดล้อมรอบข้างแบบใดCellular component (CC) คือยีนนั้นมักอยู่ในเซลล์ หรือสภาวะแวดล้อมรอบข้างแบบใดMolecular function (MF) คือ ยีนนั้นทำหน้าที่ใดบ้างในระดับโมเลกุล มักต่อท้ายด้วยคำว่า “activity”Molecular function (MF) คือ ยีนนั้นทำหน้าที่ใดบ้างในระดับโมเลกุล มักต่อท้ายด้วยคำว่า “activity”Biological process (BP) คือ ยีนนั้นเป็นส่วนประกอบหลักของการทำงานในระบบชีววิทยาอย่างไร ซึ่งเป็นภาพใหญ่ของหลายโมเลกุลที่มารวมตัวกันทำหน้าที่เป็นระบบBiological process (BP) คือ ยีนนั้นเป็นส่วนประกอบหลักของการทำงานในระบบชีววิทยาอย่างไร ซึ่งเป็นภาพใหญ่ของหลายโมเลกุลที่มารวมตัวกันทำหน้าที่เป็นระบบKEGG (Kyoto Encyclopedia Genes Genomes) (Kanehisa, Sato, Kawashima, Furumichi, & Tanabe, 2016) คือ ฐานข้อมูลของข้อมูล High-throughput ที่เชื่อมโยงข้อมูลของยีนเข้ากับระบบทางอื่นๆ ทางชีววิทยา โดยหมวดหมู่นั้นแบ่งเป็น\nSystem information คือ ข้อมูลของความเชื่อมโยงจากข้อมูลในระบบชนิดอื่นๆ\nGenomic information คือ ข้อมูลของยีนแล้วโปรตีนจากสิ่งมีชีวิตหลากชนิด\nChemical information คือ ข้อมูลของทางเคมี เช่น เอนไซม์ ปฏิกริยาทางชีวเคมี เป็นต้น\nHealth information คือ ข้อมูลของโรคและยาต่างๆ\nKEGG (Kyoto Encyclopedia Genes Genomes) (Kanehisa, Sato, Kawashima, Furumichi, & Tanabe, 2016) คือ ฐานข้อมูลของข้อมูล High-throughput ที่เชื่อมโยงข้อมูลของยีนเข้ากับระบบทางอื่นๆ ทางชีววิทยา โดยหมวดหมู่นั้นแบ่งเป็นSystem information คือ ข้อมูลของความเชื่อมโยงจากข้อมูลในระบบชนิดอื่นๆSystem information คือ ข้อมูลของความเชื่อมโยงจากข้อมูลในระบบชนิดอื่นๆGenomic information คือ ข้อมูลของยีนแล้วโปรตีนจากสิ่งมีชีวิตหลากชนิดGenomic information คือ ข้อมูลของยีนแล้วโปรตีนจากสิ่งมีชีวิตหลากชนิดChemical information คือ ข้อมูลของทางเคมี เช่น เอนไซม์ ปฏิกริยาทางชีวเคมี เป็นต้นChemical information คือ ข้อมูลของทางเคมี เช่น เอนไซม์ ปฏิกริยาทางชีวเคมี เป็นต้นHealth information คือ ข้อมูลของโรคและยาต่างๆHealth information คือ ข้อมูลของโรคและยาต่างๆReactome (Gillespie et al., 2022) คือ ฐานข้อมูลที่รวบรวมหน้าที่การทำงานของยีนแต่ละโมเลกุล (Biological pathway) จากฐานข้อมูลและงานวิจัยต่างๆ ซึ่งมีการปรับแต่งให้เข้ากับการทำงานระดับโปรตีนด้วยReactome (Gillespie et al., 2022) คือ ฐานข้อมูลที่รวบรวมหน้าที่การทำงานของยีนแต่ละโมเลกุล (Biological pathway) จากฐานข้อมูลและงานวิจัยต่างๆ ซึ่งมีการปรับแต่งให้เข้ากับการทำงานระดับโปรตีนด้วยMSigDB (Liberzon et al., 2011) คือ ฐานข้อมูลที่รวบรวมจากฐานข้อมูลขั้นต้นทั้งหมดและนำมาจัดหมวดหมู่ใหม่ โดยทำการกำจัดข้อมูลที่ซ้ำกันออกMSigDB (Liberzon et al., 2011) คือ ฐานข้อมูลที่รวบรวมจากฐานข้อมูลขั้นต้นทั้งหมดและนำมาจัดหมวดหมู่ใหม่ โดยทำการกำจัดข้อมูลที่ซ้ำกันออก","code":""},{"path":"pathway-analysis.html","id":"statistical-analysis","chapter":"19 Pathway analysis","heading":"19.1.2 Statistical analysis","text":"หลังจาก Annotation แล้วผู้วิจัยมักจะทำการเปรียบเทียบว่าจากกลุ่มยีนทั้งหมด มียีนไหนที่แสดงออกมากกว่ากลุ่มอื่นอย่างมีนัยสำคัญ โดยการทดสอบนั้นมีหลายแบบ","code":""},{"path":"pathway-analysis.html","id":"over-representation-analysis-ora","chapter":"19 Pathway analysis","heading":"19.1.2.1 Over-representation analysis (ORA)","text":"คือ การเปรียบเทียบวจำนวนยีนที่อยู่ใน Pathway นั้นๆ กับกลุ่มยีนทั้งหมดที่มีอยู่ เรียกว่ายีนพื้นหลัง (Background) โดยยีนพื้นหลังนี้หมายถึงยีนทีมีอยู่ในกลุ่มประชากรโดยทั่วไป ซึ่งการคำนวณทางสถิตินั้นใช้วิธี Fisher’s exact testยกตัวอย่างเช่นในฐานข้อมูลที่ท่านเลือกมา Annotate นั้น มี Pathway \\(\\) อยู่ที่ท่านสนใจในฐานข้อมูลที่ท่านเลือกมา Annotate นั้น มี Pathway \\(\\) อยู่ที่ท่านสนใจข้อมูลของของตัวท่านเองพบว่า มี DGE ทั้งหมด 5,000 ยีน และมี 500 ยีน ที่อยู่ใน Pathway \\(\\) (500/5,000 = 10%)ข้อมูลของของตัวท่านเองพบว่า มี DGE ทั้งหมด 5,000 ยีน และมี 500 ยีน ที่อยู่ใน Pathway \\(\\) (500/5,000 = 10%)ส่วนข้อมูลยีนพื้นหลังของท่านนั้นมีทั้งหมด 50,000 ยีน และมี 2,000 ยีน ที่อยู่ใน Pathway \\(\\) และไม่ได้อยู่ใน DGE ของท่าน (2,000/50,000 = 4%)ส่วนข้อมูลยีนพื้นหลังของท่านนั้นมีทั้งหมด 50,000 ยีน และมี 2,000 ยีน ที่อยู่ใน Pathway \\(\\) และไม่ได้อยู่ใน DGE ของท่าน (2,000/50,000 = 4%)เมื่อสร้างเป็นตาราง \\(2 \\times 2\\)หมายความว่า ยีนกลุ่มนี้มีการแสดงออกมากกว่าพื้นหลังอย่างมีนัยสำคัญNote:ควรใช้ DGE ที่มีนัยสำคัญเท่านั้น (ทั้ง logFC และ \\(p\\)) เนื่องจากท่านต้องการยีนที่แสดงออกมากกว่าพื้นหลัง (ซึ่งในกรณีของ GSEA ควรใช้ยีนทั้งหมด)ควรใช้ DGE ที่มีนัยสำคัญเท่านั้น (ทั้ง logFC และ \\(p\\)) เนื่องจากท่านต้องการยีนที่แสดงออกมากกว่าพื้นหลัง (ซึ่งในกรณีของ GSEA ควรใช้ยีนทั้งหมด)ในการหา กลุ่มที่แสดงออกอย่างมีนัยสำคัญ จากข้อมูลยีนที่วิเคราะห์มาจาก DGE ของโปรตีนอีกครั้งหนึ่ง มักจะพบว่าหนึ่งโปรตีนมักเกิดจากหลายยีนได้ โดย ORA จะนับรวมเพิ่มเข้าไปทำให้ในกลุ่มนั้นเกิดผลบวกลวงได้เนื่องจากนับยีนที่ซ้ำกันในการหา กลุ่มที่แสดงออกอย่างมีนัยสำคัญ จากข้อมูลยีนที่วิเคราะห์มาจาก DGE ของโปรตีนอีกครั้งหนึ่ง มักจะพบว่าหนึ่งโปรตีนมักเกิดจากหลายยีนได้ โดย ORA จะนับรวมเพิ่มเข้าไปทำให้ในกลุ่มนั้นเกิดผลบวกลวงได้เนื่องจากนับยีนที่ซ้ำกัน","code":"\nfisher.test(ora_df, alternative = \"greater\")## \n##  Fisher's Exact Test for Count Data\n## \n## data:  ora_df\n## p-value < 2.2e-16\n## alternative hypothesis: true odds ratio is greater than 1\n## 95 percent confidence interval:\n##  2.442091      Inf\n## sample estimates:\n## odds ratio \n##   2.666606"},{"path":"pathway-analysis.html","id":"gene-set-enrichment-analysis-gsea","chapter":"19 Pathway analysis","heading":"19.1.2.2 Gene set enrichment analysis (GSEA)","text":"รูปภาพจาก: Subramanian et al. (2005)คือ การหายีนที่มีการแสดงออกมากกว่ากลุ่มอื่น ซึ่งใช้วิธีที่แตกต่างจาก ORA โดยวิธีของ GSEA ((Subramanian et al., 2005)) คือเรียงยีนทั้งหมดที่มีอยู่ตามความแตกต่างระหว่างกลุ่ม จากมากไปน้อย (Ranked gene) ซึ่งมักจะใช้ \\(-log_{10}(p) \\times sign(FC)\\) หรือ \\(-log_{10}p \\times log_{x}(FC)\\) (\\(x\\) เป็นอะไรก็ได้ไม่แตกต่างกัน)ให้คะแนนของยีนเรียงตามข้อ 1. แบบบวกเพิ่มไปเรื่อยๆ (Cumulative) ถ้ามียีนไหนที่อยู่ในกลุ่มที่ต้องการ (กลุ่ม \\(S\\) ดังรูป) จะให้คะแนนเป็น + แต่ถ้ายีนไม่อยู่ในกลุ่ม จะให้คะแนนเป็น - โดยขนาดของคะแนนนั้นจะขึ้นอยู่กับความแตกต่างด้วยคำนวณ Enrichment score (ES) จากระยะทางของคะแนนที่มากที่สุด (รูปขวาล่าง) และเปรียบเทียบทางสถิติว่ามีการเปลี่ยนแปลงของคะแนนเป็นแบบ Normal distribution หรือไม่ โดยใช้วิธี Kolmogorov-Smirnov (หลักการคล้าย Chi-square goodness fit สำหรับ Continuous data) ซึ่งถ้าไม่เป็น Normal distribution แสดงว่าการแสดงออกของยีนกลุ่มนี้มีทิศทางอย่างชัดเจน ไม่ใช่แบบสุ่ม (Random walk)คำนวณ \\(p\\)-value โดยการใช้ Permutation test ซึ่งคือการสุ่มสลับ Phenotype หลายๆ ครั้งและคำนวณ จำนวนครั้งที่ ES นั้นให้ผลไม่แตกต่างจาก Normal distribution (เช่น ไม่แตกต่าง 5/1000 ครั้ง \\(p\\)-value = 0.005)Note:ควรใช้ DGE ทั้งหมด เนื่องจากข้อมูลทั้งหมดจะถูกนำมาให้คำนวณ ES ดูจากข้อ 1.ควรใช้ DGE ทั้งหมด เนื่องจากข้อมูลทั้งหมดจะถูกนำมาให้คำนวณ ES ดูจากข้อ 1.ใน GSEA นั้น ถ้าพบยีนซ้ำจะทำการเฉลี่ยข้อมูล ซึ่งทำให้โอกาสเกิด False positive จาก DGE ระดับโปรตีนน้อยลงใน GSEA นั้น ถ้าพบยีนซ้ำจะทำการเฉลี่ยข้อมูล ซึ่งทำให้โอกาสเกิด False positive จาก DGE ระดับโปรตีนน้อยลง","code":""},{"path":"pathway-analysis.html","id":"example-1","chapter":"19 Pathway analysis","heading":"19.2 Example","text":"ในตัวอย่างนี้ จะนำ DGE จาก Bladder cancer มาทำการวิเคราะห์ Pathway analysis","code":"\nlibrary(clusterProfiler)\nlibrary(org.Hs.eg.db) # Homo Sapiens database\ngene_result <- topTags(genediff, n = Inf) |> as.data.frame()\ngene_result"},{"path":"pathway-analysis.html","id":"ora","chapter":"19 Pathway analysis","heading":"19.2.1 ORA","text":"GeneRatio คือ อัตราส่วนยีนที่พบใน GO ต่อ ยีนทั้งหมด (มีบางส่วนถูกตัดเนื่องจากไม่มีข้อมูลในฐาน)GeneRatio คือ อัตราส่วนยีนที่พบใน GO ต่อ ยีนทั้งหมด (มีบางส่วนถูกตัดเนื่องจากไม่มีข้อมูลในฐาน)BgRatio คือ อัตราส่วนยีนพื้นหลังที่พบใน GO ต่อ ยีนทั้งหมดBgRatio คือ อัตราส่วนยีนพื้นหลังที่พบใน GO ต่อ ยีนทั้งหมดpvalue, p.adjust คือ \\(p\\)-value จาก FIsher’s exact testpvalue, p.adjust คือ \\(p\\)-value จาก FIsher’s exact testqvalue คือ Adjusted \\(p\\)-value จาก Package qvalue ซึ่งใช้วิธีที่แตกต่างกันqvalue คือ Adjusted \\(p\\)-value จาก Package qvalue ซึ่งใช้วิธีที่แตกต่างกันท่านสามารถสร้างกราฟจากผลนี้ได้โดย dotplot(), barplot()หรือ สามารถพล็อต GO pathway โดยรวมได้นอกจากนี้ยังมีฐานข้อมูลอื่นให้วิเคราะห์ได้ เช่น enrichKEGG(), enrichPathway()","code":"\ngene_sig <- gene_result |> filter(FDR <= 0.05) # significant gene with FDR <= 0.01\n\nora_entries <- mapIds(org.Hs.eg.db, rownames(gene_sig), \"ENTREZID\", \"ALIAS\") |> na.omit() |> \n  as.character()\nhead(ora_entries, 20)##  [1] \"5753\"      \"54958\"     \"100302237\" \"9051\"      \"1118\"      \"116138\"    \"25843\"     \"3738\"     \n##  [9] \"864\"       \"3013\"      \"4145\"      \"126961\"    \"3008\"      \"6503\"      \"80342\"     \"256158\"   \n## [17] \"30009\"     \"1536\"      \"57194\"     \"11017\"\nunique(ora_entries) |> length()## [1] 2103\nora_cc <- enrichGO(ora_entries, org.Hs.eg.db, ont = \"CC\")\nas.data.frame(ora_cc)\ndotplot(ora_cc, showCategory = 20)\nbarplot(ora_cc, showCategory = 10, color = \"qvalue\")\ngoplot(ora_cc)"},{"path":"pathway-analysis.html","id":"gsea","chapter":"19 Pathway analysis","heading":"19.2.2 GSEA","text":"ในส่วน GSEA จะใช้ยีนทั้งหมดเรียงจากมากไปน้อยต่อจากนั้นจะนำข้อมูลนี้ไปใช้ต่อ ซึ่งต้องเปลี่ยนเป็น Entries ID เหมือน ORAจะเห็นว่าผลลัพธ์ที่ได้เป็น ES แทน Ratioท่านสามารถพล๋อตผลลัพธ์ได้เหมือน ORAสำหรับ GSEA ท่านสามารถพล็อตการคำนวณคะแนนในแต่ละยีนได้ด้วย gseaplot()โดยตำแหน่งขีดสีดำคล้ายบาร์โค้ด คือตำแหน่งของยีนที่อยู่ใน Pathway กลุ่มนั้น โดยความยาวของขีดคือ Signal--noise ratio","code":"\ngene_result_ranked <- gene_result |> \n  mutate(Score = -log10(FDR)*logFC) |> \n  arrange(desc(Score)) # arrange score max -> min\ngene_result_ranked |> dplyr::select(logFC, FDR, Score)\ndge_ranked <- gene_result_ranked$Score\nnames(dge_ranked) <- rownames(gene_result_ranked)\nhead(dge_ranked, 20) # gene names##      MOB4    KLHDC3      GKN1  RPL37AP1      PTK6      TFF1   TMEM160   MT-ATP8   TGFB1I1     ZBTB9 \n##  25.45660  25.37892  24.07057  23.29780  22.97158  20.83893  20.20938  19.99603  18.58928  18.52797 \n##   SNRNP27     CRIP2   NDUFA13    MIR675       MDK RNA5SP392    MRPL12   PLA2G2A  C19orf33     MTCP1 \n##  18.45924  17.69500  17.69159  17.63663  16.37928  16.15418  16.03922  15.85577  15.11291  15.03821\ngene_entries <- mapIds(org.Hs.eg.db, names(dge_ranked), \"ENTREZID\", \"ALIAS\") \nnames(dge_ranked) <- gene_entries\nhead(dge_ranked, 20) # entriesID sorted by rank##     25843    116138     56287    140717      5753      7031     54958      <NA>      7041    221504 \n##  25.45660  25.37892  24.07057  23.29780  22.97158  20.83893  20.20938  19.99603  18.58928  18.52797 \n##     11017      1397     51079 100033819      4192 100873645      6182      5320     64073      4068 \n##  18.45924  17.69500  17.69159  17.63663  16.37928  16.15418  16.03922  15.85577  15.11291  15.03821\ngsea_bp <- gseGO(na.omit(dge_ranked), \n                 ont = \"MF\", \n                 OrgDb = org.Hs.eg.db, \n                 keyType = \"ENTREZID\")\n\nas.data.frame(gsea_bp)\ndotplot(gsea_bp, showCategory = 20, font.size = 10, label_format = 50)\ngseaplot(gsea_bp, geneSetID = 1, title = gsea_bp$ID[1], font.size = 5)"},{"path":"distribution.html","id":"distribution","chapter":"20 Distribution","heading":"20 Distribution","text":"","code":""},{"path":"distribution.html","id":"norm-dist","chapter":"20 Distribution","heading":"20.1 Normal distribution","text":"คือ ลักษณะการกระจายตัวของข้อมูลที่เป็นรูประฆังคว่ำ ซึ่งพบมากสุดในธรรมชาติ\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^{2}}\n\\]\\(Z\\)-distribution = Standard normal distribution ของประชากร ที่มี \\(\\mu = 0\\), \\(\\sigma = 1\\)สามารถสร้าง \\(Z\\) score ได้จาก\\[\nZ = \\frac{x-\\mu}{\\sigma}\n\\]ค่าของ \\(Z\\)-score ที่พบบ่อย","code":""},{"path":"distribution.html","id":"t-dist","chapter":"20 Distribution","heading":"20.2 \\(t\\)-distribution","text":"คือ การกระจายตัวของอัตราส่วนของความต่างของค่าเฉลี่ยระหว่างกลุ่มตัวอย่างกับความผิดพลาดของการวัดที่สุ่มจากประชากร ใช้ใน t-test\\[\nf(t) = \\frac{\\Gamma(\\frac{v+1}{2})}{\\sqrt{v\\pi}\\Gamma(\\frac{v}{2})}(1+\\frac{t^{2}}{v})^{-\\frac{v+1}{2}}\n\\]สามารถสร้าง \\(t\\)-score (\\(t\\) -value) ได้จาก\\[\nt = \\frac{\\bar{x}-\\mu}{SE} = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}}\n\\]Note: \\(t\\)-score บางครั้งเป็นคำศัพท์เฉพาะทางEducation assessment \\(\\mu = 50\\), \\(\\sigma = 10\\)Education assessment \\(\\mu = 50\\), \\(\\sigma = 10\\)Bone density เทียบกับผู้ป่วยอายุ 30 ปี \\(\\mu = 0\\), \\(\\sigma = 1\\)Bone density เทียบกับผู้ป่วยอายุ 30 ปี \\(\\mu = 0\\), \\(\\sigma = 1\\)","code":"\nt_dist <- data.frame(x = x, \n                        `DF 1` = dt(x, df = 1),\n                        `DF 2` = dt(x, df = 2),\n                        `DF 4` = dt(x, df = 4),\n                        `DF 8` = dt(x, df = 8)) |> \n  pivot_longer(-x,names_to = \"Param\", values_to  = \"T.distribution\" ) \n\nggplot(t_dist, aes(x = x, y = T.distribution, col = Param)) + \n  geom_line(alpha = 0.8, linewidth = 0.5) +\n  xlim(-4,4) +\n  labs(title = \"t-distribution\", x = \"Values\", y = \"Rate\")"},{"path":"distribution.html","id":"unif-dist","chapter":"20 Distribution","heading":"20.3 Uniform distribution","text":"คือ การกระจายตัวของข้อมูลที่อัตราการเกิดเท่าๆ กัน เช่น ทอยลูกเต๋าไม่ถ่วงน้ำหนัก 1 ลูก ดึงไพ่จากสำรับ เป็นต้น\\[\nf(x) = \\begin{cases}\n\\frac{1}{b-} \\ \\text{} \\ \\leq x \\leq b \\\\\n0 \\ \\text{} \\ x<\\ \\text{} \\ x>b\n\\end{cases}\n\\]","code":"\nx <- seq(0,10,1)\n\nunif_dist <- data.frame(x = x, \n                     dice = dunif(x, min = 1 ,max = 6)) |> \n  pivot_longer(-x,names_to = \"Param\", values_to  = \"Uniform.distribution\" ) \n\nggplot(unif_dist, aes(x = x, y = Uniform.distribution)) + \n  geom_point() +\n  geom_segment(col = \"darkblue\", xend = x, yend = 0) +\n  scale_x_continuous(breaks = 0:10, limits = c(0,10)) +\n  labs(title = \"Dice rolls\", x = \"Face\", y = \"Rate\")"},{"path":"distribution.html","id":"binom-dist","chapter":"20 Distribution","heading":"20.4 Binomial distribution","text":"คือ การกระจายตัวของโอกาสสำเร็จในการทดลองที่มีผลลัพธ์สองรูปแบบ เช่น หัว/ก้อย ชนะ/แพ้\\[\nf(x,n,p) = P(X = x) = {n\\choose x}p^{x}(1-p)^{n-k}  \n\\]","code":"\n# 20 trials, prob of success = 10% to 90%\nbinom_dist <- map_dfc(c(0.1, 0.3, 0.5, 0.7, 0.9), ~dbinom(1:20, 20, .x)) |> \n  set_names(paste0(\"Prob = \", c(0.1, 0.3, 0.5, 0.7, 0.9) )) |> \n  mutate(numb_success = 1:20) |> \n  pivot_longer(!numb_success, names_to = \"Prob\", values_to = \"rate\") \n\nggplot(binom_dist, aes(x = numb_success, y = rate, col = Prob)) + \n  geom_point(size = 0.8) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0,21,1), limits = c(0,21)) +\n  labs(x = \"Number of success\", y = \"Rate\")"},{"path":"distribution.html","id":"negative-binomial-distribution","chapter":"20 Distribution","heading":"20.5 Negative binomial distribution","text":"คือ การกระจายของจำนวนครั้งที่ไม่สำเร็จก่อนที่จะได้จำนวนครั้งของการสำเร็จที่ต้องการ\\[\np(k) = {r-1+k \\choose r-1}p^{r-1}(1-p)^{k}p = {r-1+k \\choose k}p^{r}(1-p)^{k}\n\\]","code":"\nnbprob_dist <- map_dfc(c(0.1, 0.3, 0.5, 0.7, 0.9), \\(x) dnbinom(1:20, 20,x)) |> \n  set_names(paste0(\"Prob = \", c(0.1, 0.3,0.5,0.7,0.9) )) |> \n  mutate(numb_failure = 1:20) |> \n  pivot_longer(!numb_failure, names_to = \"Prob\", values_to = \"rate\") \n\nggplot(nbprob_dist, aes(x = numb_failure, y = rate, col = Prob)) + \n  geom_point(alpha = 0.9, size =2) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0,20,2)) +\n  theme_bw() +\n  labs(x = \"Number of failures\", y = \"Rate\")"},{"path":"distribution.html","id":"hypergeometric-distribution","chapter":"20 Distribution","heading":"20.6 Hypergeometric distribution","text":"คือ การกระจายตัวของโอกาสที่จะสุ่มได้เป้าหมายที่ต้องการจากการหยิบสุ่มแบบใส่คืน (Sampling replacement) ใช้ใน Fisher’s exact test\\[\nf(x,n,M,N) = p(X=x) = \\frac{{M \\choose x}{N-M \\choose n-x}}{N \\choose n}\n\\]","code":"\nhyper_dist <- map_dfc(c(10,20,30), ~dhyper(1:30, .x, 30, 30)) |>  \n  set_names(paste0(\"Black = \", c(10,20,30))) |> \n  mutate(numb_success = 1:30) |> \n  pivot_longer(!numb_success, names_to = \"Pop\", values_to = \"rate\") \n\nggplot(hyper_dist, aes(x = numb_success, y = rate, col = Pop)) + \n  geom_point(size = 0.8) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0,30,2), limits = c(0,30)) +\n  labs(title = \"Probablity of getting black balls from 30 picks\", \n       x = \"Number of black balls picked\", y = \"Rate\", col = \"Total black balls\\n in the bag\")"},{"path":"distribution.html","id":"poisson-dist","chapter":"20 Distribution","heading":"20.7 Poisson distribution","text":"คือ การกระจายตัวของโอกาสที่จะเกิดเหตุการณ์เท่ากับจำนวนครั้งที่ต้องการภายใต้ช่วงเวลาใดเวลาหนึ่ง\\[\nf(k) = P(X = k) = \\frac{\\lambda^{k}}{k!}e^{-\\lambda}\n\\]\\[\n\\lambda = \\frac{k}{t}\n\\]การกระจายตัวแบบ Poisson เป็นการกระจายตัวแบบพื้นฐานในการนับจำนวนต่างๆ รวมถึงจำนวนยีนที่เกิดขึ้นจาก RNA sequencing profile","code":"\npois_dist <- map_dfc(c(1, 2, 4, 10), ~dpois(1:20, .x)) |>  \n  set_names(paste0(\"Mean = \", c(1, 2, 4, 10))) |> \n  mutate(numb_event = 1:20) |> \n  pivot_longer(!numb_event, names_to = \"Mean\", values_to = \"value\") |> \n  mutate(Mean = fct_relevel(Mean, paste0(\"Mean = \", c(1, 2, 4, 10))))\n\nggplot(pois_dist, aes(x = numb_event, y = value, col = Mean)) + \n  geom_point(size = 0.8) +\n  geom_line() + \n  scale_x_continuous(breaks = seq(0,20,2), limits = c(0,21)) +\n  labs(title = \"Probablity that certain number of events will occur\", \n       x = \"Number of events\", y = \"Rate\", col = \"Average number of events\")"},{"path":"session-info.html","id":"session-info","chapter":"21 Session info","heading":"21 Session info","text":"","code":"\nsessionInfo()## R version 4.3.1 (2023-06-16 ucrt)\n## Platform: x86_64-w64-mingw32/x64 (64-bit)\n## Running under: Windows 11 x64 (build 22621)\n## \n## Matrix products: default\n## \n## \n## locale:\n## [1] LC_COLLATE=English_United Kingdom.utf8  LC_CTYPE=English_United Kingdom.utf8   \n## [3] LC_MONETARY=English_United Kingdom.utf8 LC_NUMERIC=C                           \n## [5] LC_TIME=English_United Kingdom.utf8    \n## \n## time zone: Asia/Bangkok\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats4    stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] org.Hs.eg.db_3.17.0   AnnotationDbi_1.64.0  IRanges_2.34.1        S4Vectors_0.38.2     \n##  [5] Biobase_2.60.0        BiocGenerics_0.46.0   clusterProfiler_4.8.3 survminer_0.4.9      \n##  [9] ggpubr_0.6.0          lmtest_0.9-40         zoo_1.8-12            cowplot_1.1.1        \n## [13] broom_1.0.5           survival_3.5-7        corrplot_0.92         granova_2.2          \n## [17] car_3.1-2             carData_3.0-5         pwrss_0.3.1           ggsci_3.0.0          \n## [21] lubridate_1.9.3       forcats_1.0.0         stringr_1.5.0         dplyr_1.1.3          \n## [25] purrr_1.0.2           readr_2.1.4           tidyr_1.3.0           tibble_3.2.1         \n## [29] ggplot2_3.4.4         tidyverse_2.0.0       edgeR_4.0.0           limma_3.56.2         \n## \n## loaded via a namespace (and not attached):\n##   [1] splines_4.3.1           bitops_1.0-7            ggplotify_0.1.2        \n##   [4] polyclip_1.10-6         lifecycle_1.0.3         Rdpack_2.5             \n##   [7] rstatix_0.7.2           lattice_0.21-8          vroom_1.6.4            \n##  [10] MASS_7.3-60             backports_1.4.1         magrittr_2.0.3         \n##  [13] sass_0.4.7              rmarkdown_2.25          jquerylib_0.1.4        \n##  [16] yaml_2.3.7              DBI_1.1.3               RColorBrewer_1.1-3     \n##  [19] abind_1.4-5             zlibbioc_1.46.0         downlit_0.4.3          \n##  [22] ggraph_2.1.0            RCurl_1.98-1.12         yulab.utils_0.1.0      \n##  [25] tweenr_2.0.2            GenomeInfoDbData_1.2.11 KMsurv_0.1-5           \n##  [28] enrichplot_1.20.3       ggrepel_0.9.4           tidytree_0.4.5         \n##  [31] testthat_3.2.0          commonmark_1.9.0        codetools_0.2-19       \n##  [34] ggtext_0.1.2            DOSE_3.26.2             xml2_1.3.5             \n##  [37] ggforce_0.4.1           tidyselect_1.2.0        aplot_0.2.2            \n##  [40] farver_2.1.1            viridis_0.6.4           jsonlite_1.8.7         \n##  [43] tidygraph_1.2.3         tools_4.3.1             treeio_1.24.3          \n##  [46] snow_0.4-4              Rcpp_1.0.11             glue_1.6.2             \n##  [49] mnormt_2.1.1            gridExtra_2.3           xfun_0.40              \n##  [52] mgcv_1.8-42             qvalue_2.32.0           GenomeInfoDb_1.38.0    \n##  [55] withr_2.5.1             fastmap_1.1.1           fansi_1.0.5            \n##  [58] truncnorm_1.0-9         digest_0.6.33           timechange_0.2.0       \n##  [61] R6_2.5.1                gridGraphics_0.5-1      colorspace_2.1-0       \n##  [64] GO.db_3.17.0            markdown_1.11           jpeg_0.1-10            \n##  [67] RSQLite_2.3.1           utf8_1.2.4              generics_0.1.3         \n##  [70] data.table_1.14.8       graphlayouts_1.0.1      httr_1.4.7             \n##  [73] scatterpie_0.2.1        pkgconfig_2.0.3         gtable_0.3.4           \n##  [76] blob_1.2.4              XVector_0.40.0          survMisc_0.5.6         \n##  [79] brio_1.1.3              shadowtext_0.1.2        htmltools_0.5.6.1      \n##  [82] bookdown_0.36           fgsea_1.26.0            scales_1.2.1           \n##  [85] png_0.1-8               ggfun_0.1.3             knitr_1.44             \n##  [88] km.ci_0.5-6             rstudioapi_0.15.0       tzdb_0.4.0             \n##  [91] reshape2_1.4.4          nlme_3.1-162            randomNames_1.5-0.0    \n##  [94] cachem_1.0.8            parallel_4.3.1          HDO.db_0.99.1          \n##  [97] pillar_1.9.0            grid_4.3.1              vctrs_0.6.4            \n## [100] toOrdinal_1.3-0.0       xtable_1.8-4            evaluate_0.22          \n## [103] cli_3.6.1               locfit_1.5-9.8          compiler_4.3.1         \n## [106] rlang_1.1.1             crayon_1.5.2            ggsignif_0.6.4         \n## [109] labeling_0.4.3          plyr_1.8.9              fs_1.6.3               \n## [112] psych_2.3.9             stringi_1.7.12          viridisLite_0.4.2      \n## [115] BiocParallel_1.34.2     nleqslv_3.3.4           munsell_0.5.0          \n## [118] Biostrings_2.68.1       lazyeval_0.2.2          detectnorm_1.0.0       \n## [121] GOSemSim_2.26.1         Matrix_1.6-1.1          hms_1.1.3              \n## [124] patchwork_1.1.3         bit64_4.0.5             KEGGREST_1.42.0        \n## [127] rbibutils_2.2.16        gridtext_0.1.5          igraph_1.5.1           \n## [130] memoise_2.0.1           bslib_0.5.1             ggtree_3.8.2           \n## [133] fastmatch_1.1-4         bit_4.0.5               downloader_0.4         \n## [136] ape_5.7-1               gson_0.1.0"},{"path":"references.html","id":"references","chapter":"22 References","heading":"22 References","text":"","code":""}]
